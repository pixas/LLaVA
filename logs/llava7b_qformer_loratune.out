Node IP: 10.0.1.4 nodes_array: x090-5
x090-5-0,1,2,3
x090-8-0,1,2,3
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : llava/train/train_mem.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 4
  run_id           : 29566
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.0.1.4:29566
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : llava/train/train_mem.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 4
  run_id           : 29566
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.0.1.4:29566
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_qdwe5f78/29566_27kywtzk
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_zek83pr_/29566_il4q_2js
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=x090-5
  master_port=49057
  group_rank=0
  group_world_size=2
  local_ranks=[0, 1, 2, 3]
  role_ranks=[0, 1, 2, 3]
  global_ranks=[0, 1, 2, 3]
  role_world_sizes=[8, 8, 8, 8]
  global_world_sizes=[8, 8, 8, 8]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=x090-5
  master_port=49057
  group_rank=1
  group_world_size=2
  local_ranks=[0, 1, 2, 3]
  role_ranks=[4, 5, 6, 7]
  global_ranks=[4, 5, 6, 7]
  role_world_sizes=[8, 8, 8, 8]
  global_world_sizes=[8, 8, 8, 8]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_qdwe5f78/29566_27kywtzk/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_qdwe5f78/29566_27kywtzk/attempt_0/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_qdwe5f78/29566_27kywtzk/attempt_0/2/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_qdwe5f78/29566_27kywtzk/attempt_0/3/error.json
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zek83pr_/29566_il4q_2js/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zek83pr_/29566_il4q_2js/attempt_0/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_zek83pr_/29566_il4q_2js/attempt_0/2/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_zek83pr_/29566_il4q_2js/attempt_0/3/error.json
[2023-11-22 09:17:16,247] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 09:17:16,248] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 09:17:16,248] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 09:17:16,255] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 09:17:25,030] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-22 09:17:25,030] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-22 09:17:25,030] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-22 09:17:25,030] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-22 09:17:25,030] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-22 09:17:25,030] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-22 09:17:25,030] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-22 09:17:25,031] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-22 09:17:31,749] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 09:17:31,749] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 09:17:31,750] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 09:17:31,750] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 09:17:58,293] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-22 09:17:58,293] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-22 09:17:58,294] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-22 09:17:58,294] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-22 09:17:58,294] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-22 09:17:58,294] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-22 09:17:58,295] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-22 09:17:58,295] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-22 09:17:58,295] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
[2023-11-22 09:18:17,692] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 6.74B parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:36<00:36, 36.35s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:36<00:36, 36.39s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:36<00:36, 36.40s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:36<00:36, 36.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:49<00:00, 22.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:49<00:00, 24.61s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:49<00:00, 22.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:49<00:00, 24.61s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:49<00:00, 22.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:49<00:00, 24.61s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:49<00:00, 22.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:49<00:00, 24.63s/it]
Adding LoRA adapters...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:41<00:41, 41.60s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:51<00:51, 51.40s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:51<00:51, 51.74s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:54<00:54, 54.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 24.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 28.56s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:04<00:00, 30.40s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:04<00:00, 32.08s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:09<00:00, 31.67s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:09<00:00, 34.68s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:10<00:00, 31.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:10<00:00, 35.22s/it]
Adding LoRA adapters...
[2023-11-22 09:21:25,648] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0
[2023-11-22 09:21:25,882] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 7.04B parameters
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Parameter Offload: Total persistent parameters: 833792 in 519 params
wandb: Currently logged in as: jiangshuyang0. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /remote-home/syjiang/repo/LLaVA/wandb/run-20231122_092237-uxswszkb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-field-47
wandb: â­ï¸ View project at https://wandb.ai/jiangshuyang0/huggingface
wandb: ðŸš€ View run at https://wandb.ai/jiangshuyang0/huggingface/runs/uxswszkb
{'loss': 1.5763, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
{'loss': 1.5763, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
{'loss': 1.4675, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
{'loss': 1.4675, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
{'loss': 1.467, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.0}
{'loss': 1.467, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.0}
{'loss': 1.6379, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.0}
{'loss': 1.6379, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.0}
{'loss': 1.4904, 'learning_rate': 6.41025641025641e-06, 'epoch': 0.0}
{'loss': 1.4904, 'learning_rate': 6.41025641025641e-06, 'epoch': 0.0}
{'loss': 1.4114, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.0}
{'loss': 1.4114, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.0}
  0%|          | 0/5197 [00:00<?, ?it/s]  0%|          | 1/5197 [06:31<564:39:30, 391.22s/it]                                                       0%|          | 1/5197 [06:31<564:39:30, 391.22s/it]  0%|          | 2/5197 [11:23<480:56:06, 333.28s/it]                                                       0%|          | 2/5197 [11:23<480:56:06, 333.28s/it]  0%|          | 3/5197 [15:55<440:24:24, 305.25s/it]                                                       0%|          | 3/5197 [15:55<440:24:24, 305.25s/it]  0%|          | 4/5197 [20:46<431:44:07, 299.30s/it]                                                       0%|          | 4/5197 [20:46<431:44:07, 299.30s/it]  0%|          | 5/5197 [25:56<437:24:06, 303.28s/it]                                                       0%|          | 5/5197 [25:56<437:24:06, 303.28s/it]  0%|          | 6/5197 [30:58<436:45:16, 302.89s/it]                                                       0%|          | 6/5197 [30:58<436:45:16, 302.89s/it]  0%  0%|          | 0/5197 [00:00<?, ?it/s]  0%|          | 1/5197 [06:03<525:05:28, 363.80s/it]                                                       0%|          | 1/5197 [06:03<525:05:28, 363.80s/it]  0%|          | 2/5197 [10:56<464:38:52, 321.99s/it]                                                       0%|          | 2/5197 [10:56<464:38:52, 321.99s/it]  0%|          | 3/5197 [15:28<431:33:26, 299.12s/it]                                                       0%|          | 3/5197 [15:28<431:33:26, 299.12s/it]  0%|          | 4/5197 [20:18<426:22:29, 295.58s/it]                                                       0%|          | 4/5197 [20:18<426:22:29, 295.58s/it]  0%|          | 5/5197 [25:28<433:58:48, 300.91s/it]                                                       0%|          | 5/5197 [25:28<433:58:48, 300.91s/it]  0%|          | 6/5197 [30:31<434:29:50, 301.33s/it]                                                       0%|          | 6/5197 [30:31<434:29:50, 301.33s/it]  0%{'loss': 1.4476, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.0}
{'loss': 1.4476, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.0}
{'loss': 1.3998, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.0}
{'loss': 1.3998, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.0}
{'loss': 1.3021, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.0}
{'loss': 1.3021, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.0}
{'loss': 1.4465, 'learning_rate': 1.282051282051282e-05, 'epoch': 0.0}
{'loss': 1.4465, 'learning_rate': 1.282051282051282e-05, 'epoch': 0.0}
{'loss': 1.3327, 'learning_rate': 1.4102564102564104e-05, 'epoch': 0.0}
{'loss': 1.3327, 'learning_rate': 1.4102564102564104e-05, 'epoch': 0.0}
[2023-11-22 10:24:46,362] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.2376, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.0}
{'loss': 1.2376, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.0}
|          | 7/5197 [36:20<445:51:54, 309.27s/it]                                                       0%|          | 7/5197 [36:20<445:51:54, 309.27s/it]  0%|          | 8/5197 [41:41<450:46:34, 312.74s/it]                                                       0%|          | 8/5197 [41:41<450:46:34, 312.74s/it]  0%|          | 9/5197 [46:59<453:11:18, 314.47s/it]                                                       0%|          | 9/5197 [46:59<453:11:18, 314.47s/it]  0%|          | 10/5197 [52:19<455:49:38, 316.36s/it]                                                        0%|          | 10/5197 [52:19<455:49:38, 316.36s/it]  0%|          | 11/5197 [57:22<449:33:52, 312.08s/it]                                                        0%|          | 11/5197 [57:22<449:33:52, 312.08s/it]  0%|          | 12/5197 [1:02:19<442:58:21, 307.56s/it]                                                          0%|          | 12/5197 [1:02:19<442:58:21, 307.56s/it]  0%|          | 13/5197 [1:07:24<4|          | 7/5197 [35:53<444:20:29, 308.21s/it]                                                       0%|          | 7/5197 [35:53<444:20:29, 308.21s/it]  0%|          | 8/5197 [41:13<449:44:16, 312.02s/it]                                                       0%|          | 8/5197 [41:13<449:44:16, 312.02s/it]  0%|          | 9/5197 [46:31<452:28:23, 313.98s/it]                                                       0%|          | 9/5197 [46:31<452:28:23, 313.98s/it]  0%|          | 10/5197 [51:52<455:19:43, 316.02s/it]                                                        0%|          | 10/5197 [51:52<455:19:43, 316.02s/it]  0%|          | 11/5197 [56:54<449:12:59, 311.84s/it]                                                        0%|          | 11/5197 [56:54<449:12:59, 311.84s/it]  0%|          | 12/5197 [1:01:52<442:43:59, 307.39s/it]                                                          0%|          | 12/5197 [1:01:52<442:43:59, 307.39s/it]  0%|          | 13/5197 [1:06:56<4{'loss': 1.3192, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.0}
{'loss': 1.3192, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.0}
{'loss': 1.2878, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.0}
{'loss': 1.2878, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.0}
{'loss': 1.251, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.0}
{'loss': 1.251, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.0}
{'loss': 1.2433, 'learning_rate': 2.0512820512820512e-05, 'epoch': 0.0}
{'loss': 1.2433, 'learning_rate': 2.0512820512820512e-05, 'epoch': 0.0}
{'loss': 1.3391, 'learning_rate': 2.1794871794871795e-05, 'epoch': 0.0}
{'loss': 1.3391, 'learning_rate': 2.1794871794871795e-05, 'epoch': 0.0}
{'loss': 1.1414, 'learning_rate': 2.307692307692308e-05, 'epoch': 0.0}
{'loss': 1.1414, 'learning_rate': 2.307692307692308e-05, 'epoch': 0.0}
41:34:25, 306.65s/it]                                                          0%|          | 13/5197 [1:07:24<441:34:25, 306.65s/it]  0%|          | 14/5197 [1:11:59<427:52:51, 297.20s/it]                                                          0%|          | 14/5197 [1:11:59<427:52:51, 297.20s/it]  0%|          | 15/5197 [1:17:03<430:59:07, 299.41s/it]                                                          0%|          | 15/5197 [1:17:03<430:59:07, 299.41s/it]  0%|          | 16/5197 [1:22:01<430:03:10, 298.82s/it]                                                          0%|          | 16/5197 [1:22:01<430:03:10, 298.82s/it]  0%|          | 17/5197 [1:27:11<434:50:04, 302.20s/it]                                                          0%|          | 17/5197 [1:27:11<434:50:04, 302.20s/it]  0%|          | 18/5197 [1:32:11<434:00:43, 301.69s/it]                                                          0%|          | 18/5197 [1:32:12<434:00:43, 301.69s/it]  0%|          | 19/5197 [1{'loss': 1.1683, 'learning_rate': 2.435897435897436e-05, 'epoch': 0.0}
41:24:36, 306.53s/it]                                                          0%|          | 13/5197 [1:06:56<441:24:36, 306.53s/it]  0%|          | 14/5197 [1:11:32<427:46:09, 297.12s/it]                                                          0%|          | 14/5197 [1:11:32<427:46:09, 297.12s/it]  0%|          | 15/5197 [1:16:36<430:54:39, 299.36s/it]                                                          0%|          | 15/5197 [1:16:36<430:54:39, 299.36s/it]  0%|          | 16/5197 [1:21:34<430:00:07, 298.79s/it]                                                          0%|          | 16/5197 [1:21:34<430:00:07, 298.79s/it]  0%|          | 17/5197 [1:26:44<434:48:10, 302.18s/it]                                                          0%|          | 17/5197 [1:26:44<434:48:10, 302.18s/it]  0%|          | 18/5197 [1:31:44<433:59:17, 301.67s/it]                                                          0%|          | 18/5197 [1:31:44<433:59:17, 301.67s/it]  0%|          | 19/5197 [1{'loss': 1.1683, 'learning_rate': 2.435897435897436e-05, 'epoch': 0.0}
{'loss': 1.0838, 'learning_rate': 2.564102564102564e-05, 'epoch': 0.0}
{'loss': 1.0838, 'learning_rate': 2.564102564102564e-05, 'epoch': 0.0}
{'loss': 1.0973, 'learning_rate': 2.6923076923076923e-05, 'epoch': 0.0}
{'loss': 1.0973, 'learning_rate': 2.6923076923076923e-05, 'epoch': 0.0}
{'loss': 1.1494, 'learning_rate': 2.8205128205128207e-05, 'epoch': 0.0}
{'loss': 1.1494, 'learning_rate': 2.8205128205128207e-05, 'epoch': 0.0}
{'loss': 1.1644, 'learning_rate': 2.948717948717949e-05, 'epoch': 0.0}
{'loss': 1.1644, 'learning_rate': 2.948717948717949e-05, 'epoch': 0.0}
{'loss': 1.1099, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.0}
{'loss': 1.1099, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.0}
:37:16<435:13:33, 302.59s/it]                                                          0%|          | 19/5197 [1:37:16<435:13:33, 302.59s/it]  0%|          | 20/5197 [1:42:34<441:49:14, 307.23s/it]                                                          0%|          | 20/5197 [1:42:34<441:49:14, 307.23s/it]  0%|          | 21/5197 [1:47:26<435:09:43, 302.66s/it]                                                          0%|          | 21/5197 [1:47:26<435:09:43, 302.66s/it]  0%|          | 22/5197 [1:52:42<440:36:28, 306.51s/it]                                                          0%|          | 22/5197 [1:52:42<440:36:28, 306.51s/it]  0%|          | 23/5197 [1:58:03<447:00:51, 311.03s/it]                                                          0%|          | 23/5197 [1:58:03<447:00:51, 311.03s/it]  0%|          | 24/5197 [2:03:24<451:06:13, 313.93s/it]                                                          0%|          | 24/5197 [2:03:24<451:06:13, 313.93s/it]  0%|          | 25:36:49<435:12:25, 302.58s/it]                                                          0%|          | 19/5197 [1:36:49<435:12:25, 302.58s/it]  0%|          | 20/5197 [1:42:07<441:48:33, 307.23s/it]                                                          0%|          | 20/5197 [1:42:07<441:48:33, 307.23s/it]  0%|          | 21/5197 [1:46:59<435:09:27, 302.66s/it]                                                          0%|          | 21/5197 [1:46:59<435:09:27, 302.66s/it]  0%|          | 22/5197 [1:52:14<440:36:14, 306.51s/it]                                                          0%|          | 22/5197 [1:52:14<440:36:14, 306.51s/it]  0%|          | 23/5197 [1:57:36<447:00:43, 311.03s/it]                                                          0%|          | 23/5197 [1:57:36<447:00:43, 311.03s/it]  0%|          | 24/5197 [2:02:57<451:06:05, 313.93s/it]                                                          0%|          | 24/5197 [2:02:57<451:06:05, 313.93s/it]  0%|          | 25{'loss': 1.2124, 'learning_rate': 3.205128205128206e-05, 'epoch': 0.0}
{'loss': 1.2124, 'learning_rate': 3.205128205128206e-05, 'epoch': 0.0}
{'loss': 1.2261, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}
{'loss': 1.2261, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}
{'loss': 1.2349, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.01}
{'loss': 1.2349, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.01}
/5197 [2:07:41<438:13:54, 305.03s/it]                                                          0%|          | 25/5197 [2:07:41<438:13:54, 305.03s/it]  1%|          | 26/5197 [2:12:21<427:20:45, 297.51s/it]                                                          1%|          | 26/5197 [2:12:21<427:20:45, 297.51s/it]  1%|          | 27/5197 [2:17:41<437:13:31, 304.45s/it]                                                          1%|          | 27/5197 [2:17:41<437:13:31, 304.45s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2378 > 2048). Running this sequence through the model will result in indexing errors
{'loss': 1.139, 'learning_rate': 3.58974358974359e-05, 'epoch': 0.01}
{'loss': 1.139, 'learning_rate': 3.58974358974359e-05, 'epoch': 0.01}
{'loss': 1.1475, 'learning_rate': 3.717948717948718e-05, 'epoch': 0.01}
{'loss': 1.1475, 'learning_rate': 3.717948717948718e-05, 'epoch': 0.01}
[2023-11-22 11:55:36,009] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.1902, 'learning_rate': 3.846153846153846e-05, 'epoch': 0.01}
{'loss': 1.1902, 'learning_rate': 3.846153846153846e-05, 'epoch': 0.01}
/5197 [2:08:08<438:13:58, 305.03s/it]                                                          0%|          | 25/5197 [2:08:08<438:13:58, 305.03s/it]  1%|          | 26/5197 [2:12:48<427:20:48, 297.51s/it]                                                          1%|          | 26/5197 [2:12:48<427:20:48, 297.51s/it]  1%|          | 27/5197 [2:18:09<437:13:51, 304.45s/it]                                                          1%|          | 27/5197 [2:18:09<437:13:51, 304.45s/it]  1%|          | 28/5197 [2:23:33<445:46:03, 310.46s/it]                                                          1%|          | 28/5197 [2:23:33<445:46:03, 310.46s/it]  1%|          | 29/5197 [2:28:12<431:57:04, 300.89s/it]                                                          1%|          | 29/5197 [2:28:12<431:57:04, 300.89s/it]  1%|          | 30/5197 [2:33:09<430:04:22, 299.64s/it]                                                          1%|          | 30/5197 [2:33:09<430:04:22, 299.64s/it]  1%|      {'loss': 1.1124, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.01}
{'loss': 1.1124, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.01}
{'loss': 1.1098, 'learning_rate': 4.1025641025641023e-05, 'epoch': 0.01}
{'loss': 1.1098, 'learning_rate': 4.1025641025641023e-05, 'epoch': 0.01}
{'loss': 1.0811, 'learning_rate': 4.230769230769231e-05, 'epoch': 0.01}
{'loss': 1.0811, 'learning_rate': 4.230769230769231e-05, 'epoch': 0.01}
  1%|          | 28/5197 [2:23:06<445:45:45, 310.46s/it]                                                          1%|          | 28/5197 [2:23:06<445:45:45, 310.46s/it]  1%|          | 29/5197 [2:27:45<431:56:42, 300.89s/it]                                                          1%|          | 29/5197 [2:27:45<431:56:42, 300.89s/it]  1%|          | 30/5197 [2:32:41<430:03:59, 299.64s/it]                                                          1%|          | 30/5197 [2:32:41<430:03:59, 299.64s/it]  1%|          | 31/5197 [2:37:40<429:46:01, 299.49s/it]                                                          1%|          | 31/5197 [2:37:40<429:46:01, 299.49s/it]  1%|          | 32/5197 [2:42:35<427:31:52, 297.99s/it]                                                          1%|          | 32/5197 [2:42:35<427:31:52, 297.99s/it]  1%|          | 33/5197 [2:47:31<426:38:36, 297.43s/it]                                                          1%|          | 33/5197 [2:47:31<426:38:36, 297{'loss': 1.1016, 'learning_rate': 4.358974358974359e-05, 'epoch': 0.01}
{'loss': 1.1016, 'learning_rate': 4.358974358974359e-05, 'epoch': 0.01}
{'loss': 1.0912, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.01}
{'loss': 1.0912, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.01}
{'loss': 1.1009, 'learning_rate': 4.615384615384616e-05, 'epoch': 0.01}
{'loss': 1.1009, 'learning_rate': 4.615384615384616e-05, 'epoch': 0.01}
[2023-11-22 12:31:08,085] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
    | 31/5197 [2:38:08<429:46:35, 299.50s/it]                                                          1%|          | 31/5197 [2:38:08<429:46:35, 299.50s/it]  1%|          | 32/5197 [2:43:02<427:32:06, 297.99s/it]                                                          1%|          | 32/5197 [2:43:02<427:32:06, 297.99s/it]  1%|          | 33/5197 [2:47:58<426:38:47, 297.43s/it]                                                          1%|          | 33/5197 [2:47:58<426:38:47, 297.43s/it]  1%|          | 34/5197 [2:53:03<429:33:33, 299.52s/it]                                                          1%|          | 34/5197 [2:53:03<429:33:33, 299.52s/it]  1%|          | 35/5197 [2:58:02<429:23:44, 299.46s/it]                                                          1%|          | 35/5197 [2:58:02<429:23:44, 299.46s/it]  1%|          | 36/5197 [3:03:20<437:04:30, 304.88s/it]                                                          1%|          | 36/5197 [3:03:20<437:04:30, 304.88s/it]  1{'loss': 0.3395, 'learning_rate': 4.7435897435897435e-05, 'epoch': 0.01}
{'loss': 0.3395, 'learning_rate': 4.7435897435897435e-05, 'epoch': 0.01}
[2023-11-22 12:36:29,667] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.1563, 'learning_rate': 4.871794871794872e-05, 'epoch': 0.01}
{'loss': 1.1563, 'learning_rate': 4.871794871794872e-05, 'epoch': 0.01}
{'loss': 1.0782, 'learning_rate': 5e-05, 'epoch': 0.01}
{'loss': 1.0782, 'learning_rate': 5e-05, 'epoch': 0.01}
.43s/it]  1%|          | 34/5197 [2:52:35<429:33:08, 299.51s/it]                                                          1%|          | 34/5197 [2:52:35<429:33:08, 299.51s/it]  1%|          | 35/5197 [2:57:35<429:23:27, 299.46s/it]                                                          1%|          | 35/5197 [2:57:35<429:23:27, 299.46s/it]  1%|          | 36/5197 [3:02:52<437:04:23, 304.88s/it]                                                          1%|          | 36/5197 [3:02:52<437:04:23, 304.88s/it]  1%|          | 37/5197 [3:08:13<443:58:09, 309.75s/it]                                                          1%|          | 37/5197 [3:08:13<443:58:09, 309.75s/it]  1%|          | 38/5197 [3:13:35<448:58:23, 313.30s/it]                                                          1%|          | 38/5197 [3:13:35<448:58:23, 313.30s/it]  1%|          | 39/5197 [3:18:47<448:27:58, 313.00s/it]                                                          1%|          | 39/5197 [3:18:47<448:27{'loss': 1.0355, 'learning_rate': 5.128205128205128e-05, 'epoch': 0.01}
{'loss': 1.0355, 'learning_rate': 5.128205128205128e-05, 'epoch': 0.01}
{'loss': 1.0838, 'learning_rate': 5.256410256410257e-05, 'epoch': 0.01}
{'loss': 1.0838, 'learning_rate': 5.256410256410257e-05, 'epoch': 0.01}
{'loss': 1.1057, 'learning_rate': 5.384615384615385e-05, 'epoch': 0.01}
%|          | 37/5197 [3:08:41<443:58:02, 309.74s/it]                                                          1%|          | 37/5197 [3:08:41<443:58:02, 309.74s/it]  1%|          | 38/5197 [3:14:02<448:58:13, 313.30s/it]                                                          1%|          | 38/5197 [3:14:02<448:58:13, 313.30s/it]  1%|          | 39/5197 [3:19:15<448:27:47, 313.00s/it]                                                          1%|          | 39/5197 [3:19:15<448:27:47, 313.00s/it]  1%|          | 40/5197 [3:24:25<447:16:20, 312.23s/it]                                                          1%|          | 40/5197 [3:24:25<447:16:20, 312.23s/it]  1%|          | 41/5197 [3:29:14<437:22:14, 305.38s/it]                                                          1%|          | 41/5197 [3:29:14<437:22:14, 305.38s/it]  1%|          | 42/5197 [3:34:26<439:59:57, 307.27s/it]                                                          1%|          | 42/5197 [3:34:26<439:59:57, 307.27s{'loss': 1.1057, 'learning_rate': 5.384615384615385e-05, 'epoch': 0.01}
{'loss': 1.1028, 'learning_rate': 5.512820512820514e-05, 'epoch': 0.01}
{'loss': 1.1028, 'learning_rate': 5.512820512820514e-05, 'epoch': 0.01}
{'loss': 1.1244, 'learning_rate': 5.6410256410256414e-05, 'epoch': 0.01}
{'loss': 1.1244, 'learning_rate': 5.6410256410256414e-05, 'epoch': 0.01}
{'loss': 1.1439, 'learning_rate': 5.769230769230769e-05, 'epoch': 0.01}
:58, 313.00s/it]  1%|          | 40/5197 [3:23:58<447:16:24, 312.23s/it]                                                          1%|          | 40/5197 [3:23:58<447:16:24, 312.23s/it]  1%|          | 41/5197 [3:28:47<437:22:08, 305.38s/it]                                                          1%|          | 41/5197 [3:28:47<437:22:08, 305.38s/it]  1%|          | 42/5197 [3:33:59<440:00:27, 307.28s/it]                                                          1%|          | 42/5197 [3:33:59<440:00:27, 307.28s/it]  1%|          | 43/5197 [3:39:01<437:41:00, 305.72s/it]                                                          1%|          | 43/5197 [3:39:01<437:41:00, 305.72s/it]  1%|          | 44/5197 [3:44:05<436:52:39, 305.21s/it]                                                          1%|          | 44/5197 [3:44:05<436:52:39, 305.21s/it]  1%|          | 45/5197 [3:48:59<432:06:39, 301.94s/it]                                                          1%|          | 45/5197 [3:48:5{'loss': 1.1439, 'learning_rate': 5.769230769230769e-05, 'epoch': 0.01}
{'loss': 1.0734, 'learning_rate': 5.897435897435898e-05, 'epoch': 0.01}
{'loss': 1.0734, 'learning_rate': 5.897435897435898e-05, 'epoch': 0.01}
{'loss': 1.0722, 'learning_rate': 6.025641025641026e-05, 'epoch': 0.01}
{'loss': 1.0722, 'learning_rate': 6.025641025641026e-05, 'epoch': 0.01}
{'loss': 1.0115, 'learning_rate': 6.153846153846155e-05, 'epoch': 0.01}
{'loss': 1.0115, 'learning_rate': 6.153846153846155e-05, 'epoch': 0.01}
/it]  1%|          | 43/5197 [3:39:28<437:40:15, 305.71s/it]                                                          1%|          | 43/5197 [3:39:28<437:40:15, 305.71s/it]  1%|          | 44/5197 [3:44:32<436:51:56, 305.20s/it]                                                          1%|          | 44/5197 [3:44:32<436:51:56, 305.20s/it]  1%|          | 45/5197 [3:49:27<432:05:55, 301.93s/it]                                                          1%|          | 45/5197 [3:49:27<432:05:55, 301.93s/it]  1%|          | 46/5197 [3:54:32<433:39:00, 303.08s/it]                                                          1%|          | 46/5197 [3:54:32<433:39:00, 303.08s/it]  1%|          | 47/5197 [3:59:39<435:03:07, 304.11s/it]                                                          1%|          | 47/5197 [3:59:39<435:03:07, 304.11s/it]  1%|          | 48/5197 [4:04:30<429:17:38, 300.15s/it]                                                          1%|          | 48/5197 [4:04:30<429:17:38,{'loss': 1.09, 'learning_rate': 6.282051282051282e-05, 'epoch': 0.01}
{'loss': 1.09, 'learning_rate': 6.282051282051282e-05, 'epoch': 0.01}
{'loss': 1.0877, 'learning_rate': 6.410256410256412e-05, 'epoch': 0.01}
{'loss': 1.0877, 'learning_rate': 6.410256410256412e-05, 'epoch': 0.01}
{'loss': 1.0635, 'learning_rate': 6.538461538461539e-05, 'epoch': 0.01}
9<432:06:39, 301.94s/it]  1%|          | 46/5197 [3:54:05<433:39:13, 303.08s/it]                                                          1%|          | 46/5197 [3:54:05<433:39:13, 303.08s/it]  1%|          | 47/5197 [3:59:11<435:03:29, 304.12s/it]                                                          1%|          | 47/5197 [3:59:11<435:03:29, 304.12s/it]  1%|          | 48/5197 [4:04:02<429:17:54, 300.15s/it]                                                          1%|          | 48/5197 [4:04:02<429:17:54, 300.15s/it]  1%|          | 49/5197 [4:09:08<431:45:15, 301.93s/it]                                                          1%|          | 49/5197 [4:09:08<431:45:15, 301.93s/it]  1%|          | 50/5197 [4:14:30<439:57:25, 307.72s/it]                                                          1%|          | 50/5197 [4:14:30<439:57:25, 307.72s/it]  1%|          | 51/5197 [4:19:53<446:26:37, 312.32s/it]                                                          1%|          | 51/5197{'loss': 1.0635, 'learning_rate': 6.538461538461539e-05, 'epoch': 0.01}
{'loss': 1.0897, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.01}
{'loss': 1.0897, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.01}
{'loss': 1.0451, 'learning_rate': 6.794871794871795e-05, 'epoch': 0.01}
{'loss': 1.0451, 'learning_rate': 6.794871794871795e-05, 'epoch': 0.01}
{'loss': 1.0553, 'learning_rate': 6.923076923076924e-05, 'epoch': 0.01}
{'loss': 1.0553, 'learning_rate': 6.923076923076924e-05, 'epoch': 0.01}
 300.15s/it]  1%|          | 49/5197 [4:09:36<431:45:27, 301.93s/it]                                                          1%|          | 49/5197 [4:09:36<431:45:27, 301.93s/it]  1%|          | 50/5197 [4:14:57<439:57:54, 307.73s/it]                                                          1%|          | 50/5197 [4:14:57<439:57:54, 307.73s/it]  1%|          | 51/5197 [4:20:20<446:26:52, 312.32s/it]                                                          1%|          | 51/5197 [4:20:20<446:26:52, 312.32s/it]  1%|          | 52/5197 [4:25:25<443:09:47, 310.09s/it]                                                          1%|          | 52/5197 [4:25:25<443:09:47, 310.09s/it]  1%|          | 53/5197 [4:30:50<449:24:36, 314.52s/it]                                                          1%|          | 53/5197 [4:30:50<449:24:36, 314.52s/it]  1%|          | 54/5197 [4:36:25<458:09:11, 320.70s/it]                                                          1%|          | 54/5197 [4:36:25<45{'loss': 1.0068, 'learning_rate': 7.051282051282052e-05, 'epoch': 0.01}
{'loss': 1.0068, 'learning_rate': 7.051282051282052e-05, 'epoch': 0.01}
[2023-11-22 14:11:00,422] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.282, 'learning_rate': 7.17948717948718e-05, 'epoch': 0.01}
{'loss': 0.282, 'learning_rate': 7.17948717948718e-05, 'epoch': 0.01}
{'loss': 1.0689, 'learning_rate': 7.307692307692307e-05, 'epoch': 0.01}
{'loss': 1.0689, 'learning_rate': 7.307692307692307e-05, 'epoch': 0.01}
 [4:19:53<446:26:37, 312.32s/it]  1%|          | 52/5197 [4:24:58<443:09:31, 310.08s/it]                                                          1%|          | 52/5197 [4:24:58<443:09:31, 310.08s/it]  1%|          | 53/5197 [4:30:22<449:24:30, 314.52s/it]                                                          1%|          | 53/5197 [4:30:22<449:24:30, 314.52s/it]  1%|          | 54/5197 [4:35:58<458:09:15, 320.70s/it]                                                          1%|          | 54/5197 [4:35:58<458:09:15, 320.70s/it]  1%|          | 55/5197 [4:41:58<475:18:04, 332.77s/it]                                                          1%|          | 55/5197 [4:41:58<475:18:04, 332.77s/it]  1%|          | 56/5197 [4:48:06<489:57:16, 343.09s/it]                                                          1%|          | 56/5197 [4:48:06<489:57:16, 343.09s/it]  1%|          | 57/5197 [4:54:13<500:03:00, 350.23s/it]                                                          1%|          |{'loss': 1.0108, 'learning_rate': 7.435897435897436e-05, 'epoch': 0.01}
{'loss': 1.0108, 'learning_rate': 7.435897435897436e-05, 'epoch': 0.01}
{'loss': 1.1209, 'learning_rate': 7.564102564102564e-05, 'epoch': 0.01}
{'loss': 1.1209, 'learning_rate': 7.564102564102564e-05, 'epoch': 0.01}
{'loss': 1.0869, 'learning_rate': 7.692307692307693e-05, 'epoch': 0.01}
{'loss': 1.0869, 'learning_rate': 7.692307692307693e-05, 'epoch': 0.01}
8:09:11, 320.70s/it]  1%|          | 55/5197 [4:42:26<475:18:09, 332.77s/it]                                                          1%|          | 55/5197 [4:42:26<475:18:09, 332.77s/it]  1%|          | 56/5197 [4:48:33<489:57:11, 343.09s/it]                                                          1%|          | 56/5197 [4:48:33<489:57:11, 343.09s/it]  1%|          | 57/5197 [4:54:40<500:03:08, 350.23s/it]                                                          1%|          | 57/5197 [4:54:40<500:03:08, 350.23s/it]  1%|          | 58/5197 [5:00:21<495:51:46, 347.36s/it]                                                          1%|          | 58/5197 [5:00:21<495:51:46, 347.36s/it]  1%|          | 59/5197 [5:06:21<501:17:41, 351.24s/it]                                                          1%|          | 59/5197 [5:06:21<501:17:41, 351.24s/it]  1%|          | 60/5197 [5:12:27<507:27:46, 355.63s/it]                                                          1%|          | 60/5197 [5:{'loss': 1.0025, 'learning_rate': 7.820512820512821e-05, 'epoch': 0.01}
{'loss': 1.0025, 'learning_rate': 7.820512820512821e-05, 'epoch': 0.01}
{'loss': 1.0981, 'learning_rate': 7.948717948717948e-05, 'epoch': 0.01}
{'loss': 1.0981, 'learning_rate': 7.948717948717948e-05, 'epoch': 0.01}
{'loss': 1.0807, 'learning_rate': 8.076923076923078e-05, 'epoch': 0.01}
{'loss': 1.0807, 'learning_rate': 8.076923076923078e-05, 'epoch': 0.01}
 57/5197 [4:54:13<500:03:00, 350.23s/it]  1%|          | 58/5197 [4:59:53<495:51:50, 347.37s/it]                                                          1%|          | 58/5197 [4:59:53<495:51:50, 347.37s/it]  1%|          | 59/5197 [5:05:54<501:17:55, 351.24s/it]                                                          1%|          | 59/5197 [5:05:54<501:17:55, 351.24s/it]  1%|          | 60/5197 [5:11:59<507:27:33, 355.63s/it]                                                          1%|          | 60/5197 [5:11:59<507:27:33, 355.63s/it]  1%|          | 61/5197 [5:18:11<513:59:53, 360.28s/it]                                                          1%|          | 61/5197 [5:18:11<513:59:53, 360.28s/it]  1%|          | 62/5197 [5:24:25<520:03:37, 364.60s/it]                                                          1%|          | 62/5197 [5:24:25<520:03:37, 364.60s/it]  1%|          | 63/5197 [5:30:06<509:51:46, 357.52s/it]                                                          1%|   {'loss': 1.1192, 'learning_rate': 8.205128205128205e-05, 'epoch': 0.01}
{'loss': 1.1192, 'learning_rate': 8.205128205128205e-05, 'epoch': 0.01}
{'loss': 1.0742, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.01}
{'loss': 1.0742, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.01}
[2023-11-22 15:11:23,986] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0356, 'learning_rate': 8.461538461538461e-05, 'epoch': 0.01}
12:27<507:27:46, 355.63s/it]  1%|          | 61/5197 [5:18:38<513:59:29, 360.27s/it]                                                          1%|          | 61/5197 [5:18:38<513:59:29, 360.27s/it]  1%|          | 62/5197 [5:24:53<520:03:09, 364.59s/it]                                                          1%|          | 62/5197 [5:24:53<520:03:09, 364.59s/it]  1%|          | 63/5197 [5:30:34<509:50:56, 357.51s/it]                                                          1%|          | 63/5197 [5:30:34<509:50:56, 357.51s/it]  1%|          | 64/5197 [5:36:50<518:01:12, 363.31s/it]                                                          1%|          | 64/5197 [5:36:50<518:01:12, 363.31s/it]  1%|â–         | 65/5197 [5:42:53<517:42:59, 363.17s/it]                                                          1%|â–         | 65/5197 [5:42:53<517:42:59, 363.17s/it]  1%|â–         | 66/5197 [5:48:57<517:42:05, 363.23s/it]                                                          1%|â–      {'loss': 1.0356, 'learning_rate': 8.461538461538461e-05, 'epoch': 0.01}
{'loss': 1.0456, 'learning_rate': 8.58974358974359e-05, 'epoch': 0.01}
{'loss': 1.0456, 'learning_rate': 8.58974358974359e-05, 'epoch': 0.01}
{'loss': 1.0897, 'learning_rate': 8.717948717948718e-05, 'epoch': 0.01}
{'loss': 1.0897, 'learning_rate': 8.717948717948718e-05, 'epoch': 0.01}
{'loss': 1.0145, 'learning_rate': 8.846153846153847e-05, 'epoch': 0.01}
       | 63/5197 [5:30:06<509:51:46, 357.52s/it]  1%|          | 64/5197 [5:36:23<518:01:29, 363.31s/it]                                                          1%|          | 64/5197 [5:36:23<518:01:29, 363.31s/it]  1%|â–         | 65/5197 [5:42:26<517:42:56, 363.17s/it]                                                          1%|â–         | 65/5197 [5:42:26<517:42:56, 363.17s/it]  1%|â–         | 66/5197 [5:48:29<517:42:38, 363.23s/it]                                                          1%|â–         | 66/5197 [5:48:29<517:42:38, 363.23s/it]  1%|â–         | 67/5197 [5:54:39<520:23:52, 365.19s/it]                                                          1%|â–         | 67/5197 [5:54:39<520:23:52, 365.19s/it]  1%|â–         | 68/5197 [6:00:55<525:02:24, 368.52s/it]                                                          1%|â–         | 68/5197 [6:00:55<525:02:24, 368.52s/it]  1%|â–         | 69/5197 [6:06:47<517:34:42, 363.35s/it]                                        {'loss': 1.0145, 'learning_rate': 8.846153846153847e-05, 'epoch': 0.01}
{'loss': 1.0501, 'learning_rate': 8.974358974358975e-05, 'epoch': 0.01}
{'loss': 1.0501, 'learning_rate': 8.974358974358975e-05, 'epoch': 0.01}
{'loss': 0.9967, 'learning_rate': 9.102564102564103e-05, 'epoch': 0.01}
{'loss': 0.9967, 'learning_rate': 9.102564102564103e-05, 'epoch': 0.01}
   | 66/5197 [5:48:57<517:42:05, 363.23s/it]  1%|â–         | 67/5197 [5:55:06<520:24:08, 365.19s/it]                                                          1%|â–         | 67/5197 [5:55:06<520:24:08, 365.19s/it]  1%|â–         | 68/5197 [6:01:23<525:02:53, 368.53s/it]                                                          1%|â–         | 68/5197 [6:01:23<525:02:53, 368.53s/it]  1%|â–         | 69/5197 [6:07:14<517:34:44, 363.35s/it]                                                          1%|â–         | 69/5197 [6:07:14<517:34:44, 363.35s/it]  1%|â–         | 70/5197 [6:13:20<518:32:20, 364.10s/it]                                                          1%|â–         | 70/5197 [6:13:20<518:32:20, 364.10s/it]  1%|â–         | 71/5197 [6:19:25<518:46:36, 364.34s/it]                                                          1%|â–         | 71/5197 [6:19:25<518:46:36, 364.34s/it]  1%|â–         | 72/5197 [6:25:33<520:15:48, 365.45s/it]                                        {'loss': 0.9753, 'learning_rate': 9.230769230769232e-05, 'epoch': 0.01}
{'loss': 0.9753, 'learning_rate': 9.230769230769232e-05, 'epoch': 0.01}
{'loss': 0.9917, 'learning_rate': 9.35897435897436e-05, 'epoch': 0.01}
{'loss': 0.9917, 'learning_rate': 9.35897435897436e-05, 'epoch': 0.01}
{'loss': 0.9888, 'learning_rate': 9.487179487179487e-05, 'epoch': 0.01}
{'loss': 0.9888, 'learning_rate': 9.487179487179487e-05, 'epoch': 0.01}
[2023-11-22 16:05:43,682] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0446, 'learning_rate': 9.615384615384617e-05, 'epoch': 0.01}
                  1%|â–         | 69/5197 [6:06:47<517:34:42, 363.35s/it]  1%|â–         | 70/5197 [6:12:52<518:32:10, 364.10s/it]                                                          1%|â–         | 70/5197 [6:12:52<518:32:10, 364.10s/it]  1%|â–         | 71/5197 [6:18:57<518:46:50, 364.34s/it]                                                          1%|â–         | 71/5197 [6:18:57<518:46:50, 364.34s/it]  1%|â–         | 72/5197 [6:25:05<520:15:26, 365.45s/it]                                                          1%|â–         | 72/5197 [6:25:05<520:15:26, 365.45s/it]  1%|â–         | 73/5197 [6:31:10<519:41:26, 365.12s/it]                                                          1%|â–         | 73/5197 [6:31:10<519:41:26, 365.12s/it]  1%|â–         | 74/5197 [6:37:03<514:34:18, 361.60s/it]                                                          1%|â–         | 74/5197 [6:37:03<514:34:18, 361.60s/it]  1%|â–         | 75/5197 [6:42:49<507:44:30, 356.87s/it]        {'loss': 1.0446, 'learning_rate': 9.615384615384617e-05, 'epoch': 0.01}
{'loss': 1.0311, 'learning_rate': 9.743589743589744e-05, 'epoch': 0.01}
{'loss': 1.0311, 'learning_rate': 9.743589743589744e-05, 'epoch': 0.01}
{'loss': 0.9888, 'learning_rate': 9.871794871794872e-05, 'epoch': 0.01}
{'loss': 0.9888, 'learning_rate': 9.871794871794872e-05, 'epoch': 0.01}
                  1%|â–         | 72/5197 [6:25:33<520:15:48, 365.45s/it]  1%|â–         | 73/5197 [6:31:37<519:41:46, 365.13s/it]                                                          1%|â–         | 73/5197 [6:31:37<519:41:46, 365.13s/it]  1%|â–         | 74/5197 [6:37:30<514:34:35, 361.60s/it]                                                          1%|â–         | 74/5197 [6:37:31<514:34:35, 361.60s/it]  1%|â–         | 75/5197 [6:43:16<507:44:42, 356.87s/it]                                                          1%|â–         | 75/5197 [6:43:16<507:44:42, 356.87s/it]  1%|â–         | 76/5197 [6:49:15<508:35:54, 357.54s/it]                                                          1%|â–         | 76/5197 [6:49:15<508:35:54, 357.54s/it]  1%|â–         | 77/5197 [6:55:17<510:01:24, 358.61s/it]                                                          1%|â–         | 77/5197 [6:55:17<510:01:24, 358.61s/it]  2%|â–         | 78/5197 [7:01:03<504:34:40, 354.85s/it]        {'loss': 1.0814, 'learning_rate': 0.0001, 'epoch': 0.02}
{'loss': 1.0814, 'learning_rate': 0.0001, 'epoch': 0.02}
{'loss': 0.9943, 'learning_rate': 0.00010128205128205129, 'epoch': 0.02}
{'loss': 0.9943, 'learning_rate': 0.00010128205128205129, 'epoch': 0.02}
{'loss': 1.0382, 'learning_rate': 0.00010256410256410256, 'epoch': 0.02}
{'loss': 1.0382, 'learning_rate': 0.00010256410256410256, 'epoch': 0.02}
                                                  1%|â–         | 75/5197 [6:42:49<507:44:30, 356.87s/it]  1%|â–         | 76/5197 [6:48:48<508:36:17, 357.54s/it]                                                          1%|â–         | 76/5197 [6:48:48<508:36:17, 357.54s/it]  1%|â–         | 77/5197 [6:54:49<510:01:08, 358.61s/it]                                                          1%|â–         | 77/5197 [6:54:49<510:01:08, 358.61s/it]  2%|â–         | 78/5197 [7:00:35<504:34:26, 354.85s/it]                                                          2%|â–         | 78/5197 [7:00:35<504:34:26, 354.85s/it]  2%|â–         | 79/5197 [7:06:37<507:22:56, 356.89s/it]                                                          2%|â–         | 79/5197 [7:06:37<507:22:56, 356.89s/it]  2%|â–         | 80/5197 [7:12:54<516:06:05, 363.10s/it]                                                          2%|â–         | 80/5197 [7:12:54<516:06:05, 363.10s/it]  2%|â–         | 81/5197 [7:19:13{'loss': 1.0443, 'learning_rate': 0.00010384615384615386, 'epoch': 0.02}
{'loss': 1.0443, 'learning_rate': 0.00010384615384615386, 'epoch': 0.02}
{'loss': 1.0461, 'learning_rate': 0.00010512820512820514, 'epoch': 0.02}
{'loss': 1.0461, 'learning_rate': 0.00010512820512820514, 'epoch': 0.02}
{'loss': 1.0129, 'learning_rate': 0.00010641025641025641, 'epoch': 0.02}
{'loss': 1.0129, 'learning_rate': 0.00010641025641025641, 'epoch': 0.02}
                                                  2%|â–         | 78/5197 [7:01:03<504:34:40, 354.85s/it]  2%|â–         | 79/5197 [7:07:04<507:22:44, 356.89s/it]                                                          2%|â–         | 79/5197 [7:07:04<507:22:44, 356.89s/it]  2%|â–         | 80/5197 [7:13:22<516:06:07, 363.10s/it]                                                          2%|â–         | 80/5197 [7:13:22<516:06:07, 363.10s/it]  2%|â–         | 81/5197 [7:19:41<522:39:57, 367.79s/it]                                                          2%|â–         | 81/5197 [7:19:41<522:39:57, 367.79s/it]  2%|â–         | 82/5197 [7:26:02<528:24:34, 371.90s/it]                                                          2%|â–         | 82/5197 [7:26:02<528:24:34, 371.90s/it]  2%|â–         | 83/5197 [7:32:19<530:23:15, 373.37s/it]                                                          2%|â–         | 83/5197 [7:32:19<530:23:15, 373.37s/it]  2%|â–         | 84/5197 [7:38:03{'loss': 1.0012, 'learning_rate': 0.0001076923076923077, 'epoch': 0.02}
{'loss': 1.0012, 'learning_rate': 0.0001076923076923077, 'epoch': 0.02}
{'loss': 1.0799, 'learning_rate': 0.00010897435897435896, 'epoch': 0.02}
{'loss': 1.0799, 'learning_rate': 0.00010897435897435896, 'epoch': 0.02}
{'loss': 1.0626, 'learning_rate': 0.00011025641025641027, 'epoch': 0.02}
{'loss': 1.0626, 'learning_rate': 0.00011025641025641027, 'epoch': 0.02}
<522:40:08, 367.79s/it]                                                          2%|â–         | 81/5197 [7:19:13<522:40:08, 367.79s/it]  2%|â–         | 82/5197 [7:25:35<528:24:17, 371.90s/it]                                                          2%|â–         | 82/5197 [7:25:35<528:24:17, 371.90s/it]  2%|â–         | 83/5197 [7:31:51<530:23:41, 373.37s/it]                                                          2%|â–         | 83/5197 [7:31:51<530:23:41, 373.37s/it]  2%|â–         | 84/5197 [7:37:36<517:48:05, 364.58s/it]                                                          2%|â–         | 84/5197 [7:37:36<517:48:05, 364.58s/it]  2%|â–         | 85/5197 [7:43:37<516:16:25, 363.57s/it]                                                          2%|â–         | 85/5197 [7:43:37<516:16:25, 363.57s/it]  2%|â–         | 86/5197 [7:49:23<508:45:32, 358.35s/it]                                                          2%|â–         | 86/5197 [7:49:23<508:45:32, 358.35s/it]  2{'loss': 1.1306, 'learning_rate': 0.00011153846153846154, 'epoch': 0.02}
{'loss': 1.1306, 'learning_rate': 0.00011153846153846154, 'epoch': 0.02}
{'loss': 1.0224, 'learning_rate': 0.00011282051282051283, 'epoch': 0.02}
{'loss': 1.0224, 'learning_rate': 0.00011282051282051283, 'epoch': 0.02}
[2023-11-22 17:30:32,221] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2827, 'learning_rate': 0.0001141025641025641, 'epoch': 0.02}
{'loss': 0.2827, 'learning_rate': 0.0001141025641025641, 'epoch': 0.02}
<517:48:10, 364.58s/it]                                                          2%|â–         | 84/5197 [7:38:03<517:48:10, 364.58s/it]  2%|â–         | 85/5197 [7:44:04<516:16:31, 363.57s/it]                                                          2%|â–         | 85/5197 [7:44:04<516:16:31, 363.57s/it]  2%|â–         | 86/5197 [7:49:50<508:45:20, 358.35s/it]                                                          2%|â–         | 86/5197 [7:49:50<508:45:20, 358.35s/it]  2%|â–         | 87/5197 [7:56:04<515:03:17, 362.86s/it]                                                          2%|â–         | 87/5197 [7:56:04<515:03:17, 362.86s/it]  2%|â–         | 88/5197 [8:01:59<511:54:34, 360.71s/it]                                                          2%|â–         | 88/5197 [8:01:59<511:54:34, 360.71s/it]  2%|â–         | 89/5197 [8:08:05<513:49:29, 362.13s/it]                                                          2%|â–         | 89/5197 [8:08:05<513:49:29, 362.13s/it]  2{'loss': 1.0677, 'learning_rate': 0.00011538461538461538, 'epoch': 0.02}
{'loss': 1.0677, 'learning_rate': 0.00011538461538461538, 'epoch': 0.02}
{'loss': 1.0085, 'learning_rate': 0.00011666666666666668, 'epoch': 0.02}
{'loss': 1.0085, 'learning_rate': 0.00011666666666666668, 'epoch': 0.02}
{'loss': 1.0335, 'learning_rate': 0.00011794871794871796, 'epoch': 0.02}
{'loss': 1.0335, 'learning_rate': 0.00011794871794871796, 'epoch': 0.02}
%|â–         | 87/5197 [7:55:36<515:03:24, 362.86s/it]                                                          2%|â–         | 87/5197 [7:55:36<515:03:24, 362.86s/it]  2%|â–         | 88/5197 [8:01:32<511:54:55, 360.72s/it]                                                          2%|â–         | 88/5197 [8:01:32<511:54:55, 360.72s/it]  2%|â–         | 89/5197 [8:07:37<513:49:47, 362.14s/it]                                                          2%|â–         | 89/5197 [8:07:37<513:49:47, 362.14s/it]  2%|â–         | 90/5197 [8:13:33<510:45:22, 360.04s/it]                                                          2%|â–         | 90/5197 [8:13:33<510:45:22, 360.04s/it]  2%|â–         | 91/5197 [8:19:52<518:59:24, 365.92s/it]                                                          2%|â–         | 91/5197 [8:19:52<518:59:24, 365.92s/it]  2%|â–         | 92/5197 [8:25:47<514:00:02, 362.47s/it]                                                          2%|â–         | 92/5197 [8:{'loss': 1.1017, 'learning_rate': 0.00011923076923076923, 'epoch': 0.02}
{'loss': 1.1017, 'learning_rate': 0.00011923076923076923, 'epoch': 0.02}
[2023-11-22 18:01:01,687] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0528, 'learning_rate': 0.00012051282051282052, 'epoch': 0.02}
{'loss': 1.0528, 'learning_rate': 0.00012051282051282052, 'epoch': 0.02}
[2023-11-22 18:06:47,330] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2372, 'learning_rate': 0.00012179487179487179, 'epoch': 0.02}
%|â–         | 90/5197 [8:14:00<510:45:47, 360.04s/it]                                                          2%|â–         | 90/5197 [8:14:00<510:45:47, 360.04s/it]  2%|â–         | 91/5197 [8:20:20<518:59:30, 365.92s/it]                                                          2%|â–         | 91/5197 [8:20:20<518:59:30, 365.92s/it]  2%|â–         | 92/5197 [8:26:14<514:00:14, 362.47s/it]                                                          2%|â–         | 92/5197 [8:26:14<514:00:14, 362.47s/it]  2%|â–         | 93/5197 [8:32:18<514:40:55, 363.02s/it]                                                          2%|â–         | 93/5197 [8:32:18<514:40:55, 363.02s/it]  2%|â–         | 94/5197 [8:38:34<520:05:13, 366.90s/it]                                                          2%|â–         | 94/5197 [8:38:34<520:05:13, 366.90s/it]  2%|â–         | 95/5197 [8:44:20<510:56:36, 360.52s/it]                                                          2%|â–         | 95/5197 [8:{'loss': 0.2372, 'learning_rate': 0.00012179487179487179, 'epoch': 0.02}
{'loss': 0.9521, 'learning_rate': 0.0001230769230769231, 'epoch': 0.02}
{'loss': 0.9521, 'learning_rate': 0.0001230769230769231, 'epoch': 0.02}
[2023-11-22 18:18:48,347] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0547, 'learning_rate': 0.00012435897435897437, 'epoch': 0.02}
{'loss': 1.0547, 'learning_rate': 0.00012435897435897437, 'epoch': 0.02}
{'loss': 0.9516, 'learning_rate': 0.00012564102564102564, 'epoch': 0.02}
25:47<514:00:02, 362.47s/it]  2%|â–         | 93/5197 [8:31:51<514:40:21, 363.01s/it]                                                          2%|â–         | 93/5197 [8:31:51<514:40:21, 363.01s/it]  2%|â–         | 94/5197 [8:38:07<520:04:51, 366.90s/it]                                                          2%|â–         | 94/5197 [8:38:07<520:04:51, 366.90s/it]  2%|â–         | 95/5197 [8:43:53<510:56:27, 360.52s/it]                                                          2%|â–         | 95/5197 [8:43:53<510:56:27, 360.52s/it]  2%|â–         | 96/5197 [8:49:45<507:30:30, 358.17s/it]                                                          2%|â–         | 96/5197 [8:49:45<507:30:30, 358.17s/it]  2%|â–         | 97/5197 [8:55:54<511:43:44, 361.22s/it]                                                          2%|â–         | 97/5197 [8:55:54<511:43:44, 361.22s/it]  2%|â–         | 98/5197 [9:01:43<506:25:55, 357.55s/it]                                                        {'loss': 0.9516, 'learning_rate': 0.00012564102564102564, 'epoch': 0.02}
{'loss': 1.0428, 'learning_rate': 0.00012692307692307693, 'epoch': 0.02}
{'loss': 1.0428, 'learning_rate': 0.00012692307692307693, 'epoch': 0.02}
[2023-11-22 18:36:14,299] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2807, 'learning_rate': 0.00012820512820512823, 'epoch': 0.02}
{'loss': 0.2807, 'learning_rate': 0.00012820512820512823, 'epoch': 0.02}
{'loss': 1.0289, 'learning_rate': 0.0001294871794871795, 'epoch': 0.02}
44:20<510:56:36, 360.52s/it]  2%|â–         | 96/5197 [8:50:13<507:30:37, 358.17s/it]                                                          2%|â–         | 96/5197 [8:50:13<507:30:37, 358.17s/it]  2%|â–         | 97/5197 [8:56:21<511:43:46, 361.22s/it]                                                          2%|â–         | 97/5197 [8:56:21<511:43:46, 361.22s/it]  2%|â–         | 98/5197 [9:02:10<506:25:49, 357.55s/it]                                                          2%|â–         | 98/5197 [9:02:10<506:25:49, 357.55s/it]  2%|â–         | 99/5197 [9:08:00<503:11:38, 355.34s/it]                                                          2%|â–         | 99/5197 [9:08:00<503:11:38, 355.34s/it]  2%|â–         | 100/5197 [9:13:47<499:28:02, 352.77s/it]                                                           2%|â–         | 100/5197 [9:13:47<499:28:02, 352.77s/it]  2%|â–         | 101/5197 [9:19:57<506:41:35, 357.95s/it]                                                    {'loss': 1.0289, 'learning_rate': 0.0001294871794871795, 'epoch': 0.02}
{'loss': 1.0462, 'learning_rate': 0.00013076923076923077, 'epoch': 0.02}
{'loss': 1.0462, 'learning_rate': 0.00013076923076923077, 'epoch': 0.02}
{'loss': 0.9738, 'learning_rate': 0.00013205128205128204, 'epoch': 0.02}
{'loss': 0.9738, 'learning_rate': 0.00013205128205128204, 'epoch': 0.02}
{'loss': 1.0496, 'learning_rate': 0.00013333333333333334, 'epoch': 0.02}
  2%|â–         | 98/5197 [9:01:43<506:25:55, 357.55s/it]  2%|â–         | 99/5197 [9:07:33<503:11:39, 355.34s/it]                                                          2%|â–         | 99/5197 [9:07:33<503:11:39, 355.34s/it]  2%|â–         | 100/5197 [9:13:20<499:27:57, 352.77s/it]                                                           2%|â–         | 100/5197 [9:13:20<499:27:57, 352.77s/it]  2%|â–         | 101/5197 [9:19:30<506:41:22, 357.94s/it]                                                           2%|â–         | 101/5197 [9:19:30<506:41:22, 357.94s/it]  2%|â–         | 102/5197 [9:25:42<512:42:05, 362.26s/it]                                                           2%|â–         | 102/5197 [9:25:42<512:42:05, 362.26s/it]  2%|â–         | 103/5197 [9:31:09<497:36:22, 351.67s/it]                                                           2%|â–         | 103/5197 [9:31:09<497:36:22, 351.67s/it]  2%|â–         | 104/5197 [9:37:07<500:20:41, 353.67s/it]           {'loss': 1.0496, 'learning_rate': 0.00013333333333333334, 'epoch': 0.02}
[2023-11-22 19:06:20,804] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2622, 'learning_rate': 0.00013461538461538464, 'epoch': 0.02}
{'loss': 0.2622, 'learning_rate': 0.00013461538461538464, 'epoch': 0.02}
{'loss': 1.0064, 'learning_rate': 0.0001358974358974359, 'epoch': 0.02}
{'loss': 1.0064, 'learning_rate': 0.0001358974358974359, 'epoch': 0.02}
       2%|â–         | 101/5197 [9:19:57<506:41:35, 357.95s/it]  2%|â–         | 102/5197 [9:26:09<512:42:40, 362.27s/it]                                                           2%|â–         | 102/5197 [9:26:09<512:42:40, 362.27s/it]  2%|â–         | 103/5197 [9:31:36<497:36:35, 351.67s/it]                                                           2%|â–         | 103/5197 [9:31:36<497:36:35, 351.67s/it]  2%|â–         | 104/5197 [9:37:35<500:20:51, 353.67s/it]                                                           2%|â–         | 104/5197 [9:37:35<500:20:51, 353.67s/it]  2%|â–         | 105/5197 [9:43:53<510:56:35, 361.23s/it]                                                           2%|â–         | 105/5197 [9:43:54<510:56:35, 361.23s/it]  2%|â–         | 106/5197 [9:50:10<517:17:15, 365.79s/it]                                                           2%|â–         | 106/5197 [9:50:10<517:17:15, 365.79s/it]  2%|â–         | 107/5197 [9:56:17<517:57:30, 366.34s/it]  {'loss': 1.0356, 'learning_rate': 0.00013717948717948718, 'epoch': 0.02}
{'loss': 1.0356, 'learning_rate': 0.00013717948717948718, 'epoch': 0.02}
{'loss': 0.9944, 'learning_rate': 0.00013846153846153847, 'epoch': 0.02}
{'loss': 0.9944, 'learning_rate': 0.00013846153846153847, 'epoch': 0.02}
{'loss': 1.035, 'learning_rate': 0.00013974358974358974, 'epoch': 0.02}
{'loss': 1.035, 'learning_rate': 0.00013974358974358974, 'epoch': 0.02}
                                                2%|â–         | 104/5197 [9:37:07<500:20:41, 353.67s/it]  2%|â–         | 105/5197 [9:43:26<510:56:47, 361.23s/it]                                                           2%|â–         | 105/5197 [9:43:26<510:56:47, 361.23s/it]  2%|â–         | 106/5197 [9:49:42<517:16:46, 365.78s/it]                                                           2%|â–         | 106/5197 [9:49:42<517:16:46, 365.78s/it]  2%|â–         | 107/5197 [9:55:50<517:57:57, 366.34s/it]                                                           2%|â–         | 107/5197 [9:55:50<517:57:57, 366.34s/it]  2%|â–         | 108/5197 [10:01:57<518:10:23, 366.56s/it]                                                            2%|â–         | 108/5197 [10:01:57<518:10:23, 366.56s/it]  2%|â–         | 109/5197 [10:08:08<519:53:42, 367.85s/it]                                                            2%|â–         | 109/5197 [10:08:08<519:53:42, 367.85s/it]  2%|â–       {'loss': 1.0697, 'learning_rate': 0.00014102564102564104, 'epoch': 0.02}
{'loss': 1.0697, 'learning_rate': 0.00014102564102564104, 'epoch': 0.02}
{'loss': 1.0734, 'learning_rate': 0.0001423076923076923, 'epoch': 0.02}
{'loss': 1.0734, 'learning_rate': 0.0001423076923076923, 'epoch': 0.02}
[2023-11-22 19:48:54,312] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0031, 'learning_rate': 0.0001435897435897436, 'epoch': 0.02}
                                                         2%|â–         | 107/5197 [9:56:18<517:57:30, 366.34s/it]  2%|â–         | 108/5197 [10:02:25<518:10:07, 366.56s/it]                                                            2%|â–         | 108/5197 [10:02:25<518:10:07, 366.56s/it]  2%|â–         | 109/5197 [10:08:35<519:53:34, 367.85s/it]                                                            2%|â–         | 109/5197 [10:08:35<519:53:34, 367.85s/it]  2%|â–         | 110/5197 [10:14:41<518:42:53, 367.09s/it]                                                            2%|â–         | 110/5197 [10:14:41<518:42:53, 367.09s/it]  2%|â–         | 111/5197 [10:20:31<511:38:08, 362.15s/it]                                                            2%|â–         | 111/5197 [10:20:31<511:38:08, 362.15s/it]  2%|â–         | 112/5197 [10:26:27<508:45:29, 360.18s/it]                                                            2%|â–         | 112/5197 [10:26:27<508:45:29, 360.18s/i{'loss': 1.0031, 'learning_rate': 0.0001435897435897436, 'epoch': 0.02}
{'loss': 1.001, 'learning_rate': 0.00014487179487179488, 'epoch': 0.02}
{'loss': 1.001, 'learning_rate': 0.00014487179487179488, 'epoch': 0.02}
{'loss': 1.0175, 'learning_rate': 0.00014615384615384615, 'epoch': 0.02}
{'loss': 1.0175, 'learning_rate': 0.00014615384615384615, 'epoch': 0.02}
{'loss': 0.9634, 'learning_rate': 0.00014743589743589745, 'epoch': 0.02}
{'loss': 0.9634, 'learning_rate': 0.00014743589743589745, 'epoch': 0.02}
  | 110/5197 [10:14:13<518:43:05, 367.09s/it]                                                            2%|â–         | 110/5197 [10:14:13<518:43:05, 367.09s/it]  2%|â–         | 111/5197 [10:20:04<511:37:51, 362.15s/it]                                                            2%|â–         | 111/5197 [10:20:04<511:37:51, 362.15s/it]  2%|â–         | 112/5197 [10:26:00<508:45:45, 360.19s/it]                                                            2%|â–         | 112/5197 [10:26:00<508:45:45, 360.19s/it]  2%|â–         | 113/5197 [10:32:09<512:37:16, 362.99s/it]                                                            2%|â–         | 113/5197 [10:32:09<512:37:16, 362.99s/it]  2%|â–         | 114/5197 [10:38:20<516:01:04, 365.47s/it]                                                            2%|â–         | 114/5197 [10:38:20<516:01:04, 365.47s/it]  2%|â–         | 115/5197 [10:44:28<517:00:58, 366.25s/it]                                                            2%|â–[2023-11-22 20:13:19,577] [WARNING] [stage3.py:1850:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2782, 'learning_rate': 0.00014871794871794872, 'epoch': 0.02}
{'loss': 0.2782, 'learning_rate': 0.00014871794871794872, 'epoch': 0.02}
{'loss': 0.9838, 'learning_rate': 0.00015000000000000001, 'epoch': 0.02}
{'loss': 0.9838, 'learning_rate': 0.00015000000000000001, 'epoch': 0.02}
[2023-11-22 20:25:18,249] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0028, 'learning_rate': 0.00015128205128205128, 'epoch': 0.02}
{'loss': 1.0028, 'learning_rate': 0.00015128205128205128, 'epoch': 0.02}
t]  2%|â–         | 113/5197 [10:32:37<512:37:31, 362.99s/it]                                                            2%|â–         | 113/5197 [10:32:37<512:37:31, 362.99s/it]  2%|â–         | 114/5197 [10:38:48<516:01:11, 365.47s/it]                                                            2%|â–         | 114/5197 [10:38:48<516:01:11, 365.47s/it]  2%|â–         | 115/5197 [10:44:56<517:01:13, 366.25s/it]                                                            2%|â–         | 115/5197 [10:44:56<517:01:13, 366.25s/it]  2%|â–         | 116/5197 [10:50:52<512:45:19, 363.30s/it]                                                            2%|â–         | 116/5197 [10:50:52<512:45:19, 363.30s/it]  2%|â–         | 117/5197 [10:57:06<517:11:38, 366.52s/it]                                                            2%|â–         | 117/5197 [10:57:06<517:11:38, 366.52s/it]  2%|â–         | 118/5197 [11:02:51<507:50:26, 359.96s/it]                                                  [2023-11-22 20:31:37,307] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0049, 'learning_rate': 0.00015256410256410255, 'epoch': 0.02}
{'loss': 1.0049, 'learning_rate': 0.00015256410256410255, 'epoch': 0.02}
{'loss': 0.9656, 'learning_rate': 0.00015384615384615385, 'epoch': 0.02}
{'loss': 0.9656, 'learning_rate': 0.00015384615384615385, 'epoch': 0.02}
         | 115/5197 [10:44:28<517:00:58, 366.25s/it]  2%|â–         | 116/5197 [10:50:25<512:45:04, 363.30s/it]                                                            2%|â–         | 116/5197 [10:50:25<512:45:04, 363.30s/it]  2%|â–         | 117/5197 [10:56:39<517:11:30, 366.51s/it]                                                            2%|â–         | 117/5197 [10:56:39<517:11:30, 366.51s/it]  2%|â–         | 118/5197 [11:02:23<507:50:07, 359.95s/it]                                                            2%|â–         | 118/5197 [11:02:23<507:50:07, 359.95s/it]  2%|â–         | 119/5197 [11:08:43<515:49:01, 365.68s/it]                                                            2%|â–         | 119/5197 [11:08:43<515:49:01, 365.68s/it]  2%|â–         | 120/5197 [11:14:58<519:53:29, 368.64s/it]                                                            2%|â–         | 120/5197 [11:14:58<519:53:29, 368.64s/it]  2%|â–         | 121/5197 [11:20:39<508:11:24, 360.42s/it]{'loss': 1.0535, 'learning_rate': 0.00015512820512820515, 'epoch': 0.02}
{'loss': 1.0535, 'learning_rate': 0.00015512820512820515, 'epoch': 0.02}
{'loss': 1.0403, 'learning_rate': 0.00015641025641025642, 'epoch': 0.02}
{'loss': 1.0403, 'learning_rate': 0.00015641025641025642, 'epoch': 0.02}
{'loss': 0.9757, 'learning_rate': 0.0001576923076923077, 'epoch': 0.02}
{'loss': 0.9757, 'learning_rate': 0.0001576923076923077, 'epoch': 0.02}
          2%|â–         | 118/5197 [11:02:51<507:50:26, 359.96s/it]  2%|â–         | 119/5197 [11:09:10<515:49:20, 365.69s/it]                                                            2%|â–         | 119/5197 [11:09:10<515:49:20, 365.69s/it]  2%|â–         | 120/5197 [11:15:26<519:53:19, 368.64s/it]                                                            2%|â–         | 120/5197 [11:15:26<519:53:19, 368.64s/it]  2%|â–         | 121/5197 [11:21:07<508:11:41, 360.42s/it]                                                            2%|â–         | 121/5197 [11:21:07<508:11:41, 360.42s/it]  2%|â–         | 122/5197 [11:27:18<512:29:44, 363.54s/it]                                                            2%|â–         | 122/5197 [11:27:18<512:29:44, 363.54s/it]  2%|â–         | 123/5197 [11:33:27<514:59:21, 365.38s/it]                                                            2%|â–         | 123/5197 [11:33:27<514:59:21, 365.38s/it]  2%|â–         | 124/5197 [11:39:42<518:4{'loss': 0.9961, 'learning_rate': 0.00015897435897435896, 'epoch': 0.02}
{'loss': 0.9961, 'learning_rate': 0.00015897435897435896, 'epoch': 0.02}
{'loss': 1.0645, 'learning_rate': 0.00016025641025641028, 'epoch': 0.02}
{'loss': 1.0645, 'learning_rate': 0.00016025641025641028, 'epoch': 0.02}
{'loss': 1.0633, 'learning_rate': 0.00016153846153846155, 'epoch': 0.02}
{'loss': 1.0633, 'learning_rate': 0.00016153846153846155, 'epoch': 0.02}
                                                            2%|â–         | 121/5197 [11:20:39<508:11:24, 360.42s/it]  2%|â–         | 122/5197 [11:26:50<512:29:41, 363.54s/it]                                                            2%|â–         | 122/5197 [11:26:50<512:29:41, 363.54s/it]  2%|â–         | 123/5197 [11:33:00<514:59:16, 365.38s/it]                                                            2%|â–         | 123/5197 [11:33:00<514:59:16, 365.38s/it]  2%|â–         | 124/5197 [11:39:14<518:45:15, 368.13s/it]                                                            2%|â–         | 124/5197 [11:39:14<518:45:15, 368.13s/it]  2%|â–         | 125/5197 [11:45:40<526:10:39, 373.47s/it]                                                            2%|â–         | 125/5197 [11:45:40<526:10:39, 373.47s/it]  2%|â–         | 126/5197 [11:51:56<527:11:29, 374.26s/it]                                                            2%|â–         | 126/5197 [11:51:56<527:11:29, 374.2{'loss': 1.0536, 'learning_rate': 0.00016282051282051282, 'epoch': 0.02}
{'loss': 1.0536, 'learning_rate': 0.00016282051282051282, 'epoch': 0.02}
{'loss': 1.0555, 'learning_rate': 0.0001641025641025641, 'epoch': 0.02}
{'loss': 1.0555, 'learning_rate': 0.0001641025641025641, 'epoch': 0.02}
{'loss': 1.0211, 'learning_rate': 0.0001653846153846154, 'epoch': 0.02}
{'loss': 1.0211, 'learning_rate': 0.0001653846153846154, 'epoch': 0.02}
5:23, 368.13s/it]                                                            2%|â–         | 124/5197 [11:39:42<518:45:23, 368.13s/it]  2%|â–         | 125/5197 [11:46:08<526:10:25, 373.47s/it]                                                            2%|â–         | 125/5197 [11:46:08<526:10:25, 373.47s/it]  2%|â–         | 126/5197 [11:52:24<527:11:26, 374.26s/it]                                                            2%|â–         | 126/5197 [11:52:24<527:11:26, 374.26s/it]  2%|â–         | 127/5197 [11:58:24<521:17:16, 370.15s/it]                                                            2%|â–         | 127/5197 [11:58:24<521:17:16, 370.15s/it]  2%|â–         | 128/5197 [12:04:32<519:58:09, 369.28s/it]                                                            2%|â–         | 128/5197 [12:04:32<519:58:09, 369.28s/it]  2%|â–         | 129/5197 [12:10:41<519:59:40, 369.37s/it]                                                            2%|â–         | 129/5197 [12:10:4{'loss': 1.0568, 'learning_rate': 0.0001666666666666667, 'epoch': 0.03}
{'loss': 1.0568, 'learning_rate': 0.0001666666666666667, 'epoch': 0.03}
{'loss': 0.988, 'learning_rate': 0.00016794871794871796, 'epoch': 0.03}
{'loss': 0.988, 'learning_rate': 0.00016794871794871796, 'epoch': 0.03}
{'loss': 0.9301, 'learning_rate': 0.00016923076923076923, 'epoch': 0.03}
6s/it]  2%|â–         | 127/5197 [11:57:57<521:17:59, 370.15s/it]                                                            2%|â–         | 127/5197 [11:57:57<521:17:59, 370.15s/it]  2%|â–         | 128/5197 [12:04:04<519:58:24, 369.28s/it]                                                            2%|â–         | 128/5197 [12:04:04<519:58:24, 369.28s/it]  2%|â–         | 129/5197 [12:10:14<520:00:05, 369.38s/it]                                                            2%|â–         | 129/5197 [12:10:14<520:00:05, 369.38s/it]  3%|â–Ž         | 130/5197 [12:16:08<513:22:34, 364.74s/it]                                                            3%|â–Ž         | 130/5197 [12:16:08<513:22:34, 364.74s/it]  3%|â–Ž         | 131/5197 [12:21:35<497:36:21, 353.61s/it]                                                            3%|â–Ž         | 131/5197 [12:21:35<497:36:21, 353.61s/it]  3%|â–Ž         | 132/5197 [12:27:49<506:09:13, 359.75s/it]                                              {'loss': 0.9301, 'learning_rate': 0.00016923076923076923, 'epoch': 0.03}
{'loss': 0.9774, 'learning_rate': 0.00017051282051282053, 'epoch': 0.03}
{'loss': 0.9774, 'learning_rate': 0.00017051282051282053, 'epoch': 0.03}
{'loss': 1.0147, 'learning_rate': 0.0001717948717948718, 'epoch': 0.03}
{'loss': 1.0147, 'learning_rate': 0.0001717948717948718, 'epoch': 0.03}
1<519:59:40, 369.37s/it]  3%|â–Ž         | 130/5197 [12:16:35<513:22:10, 364.74s/it]                                                            3%|â–Ž         | 130/5197 [12:16:35<513:22:10, 364.74s/it]  3%|â–Ž         | 131/5197 [12:22:03<497:35:59, 353.60s/it]                                                            3%|â–Ž         | 131/5197 [12:22:03<497:35:59, 353.60s/it]  3%|â–Ž         | 132/5197 [12:28:17<506:08:57, 359.75s/it]                                                            3%|â–Ž         | 132/5197 [12:28:17<506:08:57, 359.75s/it]  3%|â–Ž         | 133/5197 [12:34:31<512:04:38, 364.04s/it]                                                            3%|â–Ž         | 133/5197 [12:34:31<512:04:38, 364.04s/it]  3%|â–Ž         | 134/5197 [12:40:24<507:20:50, 360.74s/it]                                                            3%|â–Ž         | 134/5197 [12:40:24<507:20:50, 360.74s/it]  3%|â–Ž         | 135/5197 [12:46:32<510:27:19, 363.03s/it]                            {'loss': 1.0557, 'learning_rate': 0.0001730769230769231, 'epoch': 0.03}
{'loss': 1.0557, 'learning_rate': 0.0001730769230769231, 'epoch': 0.03}
{'loss': 1.034, 'learning_rate': 0.00017435897435897436, 'epoch': 0.03}
{'loss': 1.034, 'learning_rate': 0.00017435897435897436, 'epoch': 0.03}
[2023-11-22 22:21:02,604] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0472, 'learning_rate': 0.00017564102564102566, 'epoch': 0.03}
{'loss': 1.0472, 'learning_rate': 0.00017564102564102566, 'epoch': 0.03}
              3%|â–Ž         | 132/5197 [12:27:49<506:09:13, 359.75s/it]  3%|â–Ž         | 133/5197 [12:34:04<512:04:52, 364.04s/it]                                                            3%|â–Ž         | 133/5197 [12:34:04<512:04:52, 364.04s/it]  3%|â–Ž         | 134/5197 [12:39:57<507:21:02, 360.75s/it]                                                            3%|â–Ž         | 134/5197 [12:39:57<507:21:02, 360.75s/it]  3%|â–Ž         | 135/5197 [12:46:05<510:27:23, 363.03s/it]                                                            3%|â–Ž         | 135/5197 [12:46:05<510:27:23, 363.03s/it]  3%|â–Ž         | 136/5197 [12:51:51<502:59:53, 357.79s/it]                                                            3%|â–Ž         | 136/5197 [12:51:51<502:59:53, 357.79s/it]  3%|â–Ž         | 137/5197 [12:58:08<511:08:21, 363.66s/it]                                                            3%|â–Ž         | 137/5197 [12:58:08<511:08:21, 363.66s/it]  3%|â–Ž         | 138/5197 [13:03:57<5{'loss': 1.0001, 'learning_rate': 0.00017692307692307693, 'epoch': 0.03}
{'loss': 1.0001, 'learning_rate': 0.00017692307692307693, 'epoch': 0.03}
{'loss': 0.9953, 'learning_rate': 0.00017820512820512823, 'epoch': 0.03}
{'loss': 0.9953, 'learning_rate': 0.00017820512820512823, 'epoch': 0.03}
{'loss': 0.9841, 'learning_rate': 0.0001794871794871795, 'epoch': 0.03}
{'loss': 0.9841, 'learning_rate': 0.0001794871794871795, 'epoch': 0.03}
                                3%|â–Ž         | 135/5197 [12:46:32<510:27:19, 363.03s/it]  3%|â–Ž         | 136/5197 [12:52:18<502:59:54, 357.79s/it]                                                            3%|â–Ž         | 136/5197 [12:52:18<502:59:54, 357.79s/it]  3%|â–Ž         | 137/5197 [12:58:35<511:08:26, 363.66s/it]                                                            3%|â–Ž         | 137/5197 [12:58:35<511:08:26, 363.66s/it]  3%|â–Ž         | 138/5197 [13:04:25<505:01:41, 359.38s/it]                                                            3%|â–Ž         | 138/5197 [13:04:25<505:01:41, 359.38s/it]  3%|â–Ž         | 139/5197 [13:10:27<506:12:14, 360.29s/it]                                                            3%|â–Ž         | 139/5197 [13:10:27<506:12:14, 360.29s/it]  3%|â–Ž         | 140/5197 [13:16:56<518:12:40, 368.91s/it]                                                            3%|â–Ž         | 140/5197 [13:16:56<518:12:40, 368.91s/it]  3%|â–Ž         | 14{'loss': 0.9602, 'learning_rate': 0.00018076923076923077, 'epoch': 0.03}
{'loss': 0.9602, 'learning_rate': 0.00018076923076923077, 'epoch': 0.03}
{'loss': 0.918, 'learning_rate': 0.00018205128205128207, 'epoch': 0.03}
{'loss': 0.918, 'learning_rate': 0.00018205128205128207, 'epoch': 0.03}
{'loss': 1.0146, 'learning_rate': 0.00018333333333333334, 'epoch': 0.03}
{'loss': 1.0146, 'learning_rate': 0.00018333333333333334, 'epoch': 0.03}
05:01:19, 359.38s/it]                                                            3%|â–Ž         | 138/5197 [13:03:57<505:01:19, 359.38s/it]  3%|â–Ž         | 139/5197 [13:10:00<506:11:54, 360.28s/it]                                                            3%|â–Ž         | 139/5197 [13:10:00<506:11:54, 360.28s/it]  3%|â–Ž         | 140/5197 [13:16:29<518:12:18, 368.90s/it]                                                            3%|â–Ž         | 140/5197 [13:16:29<518:12:18, 368.90s/it]  3%|â–Ž         | 141/5197 [13:22:30<514:55:43, 366.64s/it]                                                            3%|â–Ž         | 141/5197 [13:22:30<514:55:43, 366.64s/it]  3%|â–Ž         | 142/5197 [13:28:43<517:41:20, 368.68s/it]                                                            3%|â–Ž         | 142/5197 [13:28:43<517:41:20, 368.68s/it]  3%|â–Ž         | 143/5197 [13:34:59<520:19:12, 370.63s/it]                                                            3%|â–Ž         | 143/5197 [13:{'loss': 1.0817, 'learning_rate': 0.00018461538461538463, 'epoch': 0.03}
{'loss': 1.0817, 'learning_rate': 0.00018461538461538463, 'epoch': 0.03}
{'loss': 0.9721, 'learning_rate': 0.0001858974358974359, 'epoch': 0.03}
{'loss': 0.9721, 'learning_rate': 0.0001858974358974359, 'epoch': 0.03}
{'loss': 1.0582, 'learning_rate': 0.0001871794871794872, 'epoch': 0.03}
{'loss': 1.0582, 'learning_rate': 0.0001871794871794872, 'epoch': 0.03}
1/5197 [13:22:57<514:56:03, 366.65s/it]                                                            3%|â–Ž         | 141/5197 [13:22:57<514:56:03, 366.65s/it]  3%|â–Ž         | 142/5197 [13:29:11<517:41:40, 368.68s/it]                                                            3%|â–Ž         | 142/5197 [13:29:11<517:41:40, 368.68s/it]  3%|â–Ž         | 143/5197 [13:35:26<520:18:56, 370.62s/it]                                                            3%|â–Ž         | 143/5197 [13:35:26<520:18:56, 370.62s/it]  3%|â–Ž         | 144/5197 [13:41:02<505:46:36, 360.34s/it]                                                            3%|â–Ž         | 144/5197 [13:41:02<505:46:36, 360.34s/it]  3%|â–Ž         | 145/5197 [13:46:31<492:20:06, 350.83s/it]                                                            3%|â–Ž         | 145/5197 [13:46:31<492:20:06, 350.83s/it]  3%|â–Ž         | 146/5197 [13:52:08<486:19:23, 346.62s/it]                                                            3%|â–Ž      {'loss': 1.0298, 'learning_rate': 0.00018846153846153847, 'epoch': 0.03}
{'loss': 1.0298, 'learning_rate': 0.00018846153846153847, 'epoch': 0.03}
{'loss': 0.9949, 'learning_rate': 0.00018974358974358974, 'epoch': 0.03}
{'loss': 0.9949, 'learning_rate': 0.00018974358974358974, 'epoch': 0.03}
{'loss': 1.0183, 'learning_rate': 0.00019102564102564104, 'epoch': 0.03}
34:59<520:19:12, 370.63s/it]  3%|â–Ž         | 144/5197 [13:40:35<505:47:10, 360.35s/it]                                                            3%|â–Ž         | 144/5197 [13:40:35<505:47:10, 360.35s/it]  3%|â–Ž         | 145/5197 [13:46:04<492:20:44, 350.84s/it]                                                            3%|â–Ž         | 145/5197 [13:46:04<492:20:44, 350.84s/it]  3%|â–Ž         | 146/5197 [13:51:40<486:19:30, 346.62s/it]                                                            3%|â–Ž         | 146/5197 [13:51:40<486:19:30, 346.62s/it]  3%|â–Ž         | 147/5197 [13:57:31<487:57:19, 347.85s/it]                                                            3%|â–Ž         | 147/5197 [13:57:31<487:57:19, 347.85s/it]  3%|â–Ž         | 148/5197 [14:03:15<486:17:56, 346.74s/it]                                                            3%|â–Ž         | 148/5197 [14:03:15<486:17:56, 346.74s/it]  3%|â–Ž         | 149/5197 [14:09:06<487:41:48, 347.80s/it]                        {'loss': 1.0183, 'learning_rate': 0.00019102564102564104, 'epoch': 0.03}
{'loss': 1.0399, 'learning_rate': 0.00019230769230769233, 'epoch': 0.03}
{'loss': 1.0399, 'learning_rate': 0.00019230769230769233, 'epoch': 0.03}
WARNING: tokenization mismatch: 1 vs. 789. (ignored)
{'loss': 0.9939, 'learning_rate': 0.0001935897435897436, 'epoch': 0.03}
{'loss': 0.9939, 'learning_rate': 0.0001935897435897436, 'epoch': 0.03}
[2023-11-22 23:49:01,425] [WARNING] [stage3.py:1850:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2971, 'learning_rate': 0.00019487179487179487, 'epoch': 0.03}
   | 146/5197 [13:52:08<486:19:23, 346.62s/it]  3%|â–Ž         | 147/5197 [13:57:59<487:57:32, 347.85s/it]                                                            3%|â–Ž         | 147/5197 [13:57:59<487:57:32, 347.85s/it]  3%|â–Ž         | 148/5197 [14:03:43<486:18:02, 346.74s/it]                                                            3%|â–Ž         | 148/5197 [14:03:43<486:18:02, 346.74s/it]  3%|â–Ž         | 149/5197 [14:09:33<487:42:27, 347.81s/it]                                                            3%|â–Ž         | 149/5197 [14:09:33<487:42:27, 347.81s/it]  3%|â–Ž         | 150/5197 [14:15:10<483:01:28, 344.54s/it]                                                            3%|â–Ž         | 150/5197 [14:15:10<483:01:28, 344.54s/it]  3%|â–Ž         | 151/5197 [14:20:55<483:08:38, 344.69s/it]                                                            3%|â–Ž         | 151/5197 [14:20:55<483:08:38, 344.69s/it]  3%|â–Ž         | 152/5197 [14:26:34<480:43:13, 343.03s/it]      {'loss': 0.2971, 'learning_rate': 0.00019487179487179487, 'epoch': 0.03}
{'loss': 1.0136, 'learning_rate': 0.00019615384615384615, 'epoch': 0.03}
{'loss': 1.0136, 'learning_rate': 0.00019615384615384615, 'epoch': 0.03}
{'loss': 0.996, 'learning_rate': 0.00019743589743589744, 'epoch': 0.03}
{'loss': 0.996, 'learning_rate': 0.00019743589743589744, 'epoch': 0.03}
                                    3%|â–Ž         | 149/5197 [14:09:06<487:41:48, 347.80s/it]  3%|â–Ž         | 150/5197 [14:14:42<483:01:23, 344.54s/it]                                                            3%|â–Ž         | 150/5197 [14:14:42<483:01:23, 344.54s/it]  3%|â–Ž         | 151/5197 [14:20:28<483:08:03, 344.69s/it]                                                            3%|â–Ž         | 151/5197 [14:20:28<483:08:03, 344.69s/it]  3%|â–Ž         | 152/5197 [14:26:07<480:42:42, 343.03s/it]                                                            3%|â–Ž         | 152/5197 [14:26:07<480:42:42, 343.03s/it]  3%|â–Ž         | 153/5197 [14:31:57<483:39:31, 345.20s/it]                                                            3%|â–Ž         | 153/5197 [14:31:57<483:39:31, 345.20s/it]  3%|â–Ž         | 154/5197 [14:37:57<489:55:38, 349.74s/it]                                                            3%|â–Ž         | 154/5197 [14:37:57<489:55:38, 349.74s/it]  3%|â–Ž         {'loss': 1.0054, 'learning_rate': 0.00019871794871794874, 'epoch': 0.03}
{'loss': 1.0054, 'learning_rate': 0.00019871794871794874, 'epoch': 0.03}
{'loss': 1.0423, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 1.0423, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.9812, 'learning_rate': 0.00019999998058057615, 'epoch': 0.03}
{'loss': 0.9812, 'learning_rate': 0.00019999998058057615, 'epoch': 0.03}
                                                      3%|â–Ž         | 152/5197 [14:26:34<480:43:13, 343.03s/it]  3%|â–Ž         | 153/5197 [14:32:24<483:39:51, 345.20s/it]                                                            3%|â–Ž         | 153/5197 [14:32:24<483:39:51, 345.20s/it]  3%|â–Ž         | 154/5197 [14:38:25<489:56:03, 349.74s/it]                                                            3%|â–Ž         | 154/5197 [14:38:25<489:56:03, 349.74s/it]  3%|â–Ž         | 155/5197 [14:44:22<492:54:38, 351.94s/it]                                                            3%|â–Ž         | 155/5197 [14:44:22<492:54:38, 351.94s/it]  3%|â–Ž         | 156/5197 [14:50:10<491:18:15, 350.86s/it]                                                            3%|â–Ž         | 156/5197 [14:50:10<491:18:15, 350.86s/it]  3%|â–Ž         | 157/5197 [14:55:56<489:04:42, 349.34s/it]                                                            3%|â–Ž         | 157/5197 [14:55:56<489:04:42, 349.34s/it]{'loss': 0.9882, 'learning_rate': 0.00019999992232231216, 'epoch': 0.03}
{'loss': 0.9882, 'learning_rate': 0.00019999992232231216, 'epoch': 0.03}
{'loss': 0.972, 'learning_rate': 0.0001999998252252306, 'epoch': 0.03}
{'loss': 0.972, 'learning_rate': 0.0001999998252252306, 'epoch': 0.03}
{'loss': 1.0227, 'learning_rate': 0.00019999968928936926, 'epoch': 0.03}
{'loss': 1.0227, 'learning_rate': 0.00019999968928936926, 'epoch': 0.03}
| 155/5197 [14:43:54<492:54:40, 351.94s/it]                                                            3%|â–Ž         | 155/5197 [14:43:54<492:54:40, 351.94s/it]  3%|â–Ž         | 156/5197 [14:49:43<491:18:14, 350.86s/it]                                                            3%|â–Ž         | 156/5197 [14:49:43<491:18:14, 350.86s/it]  3%|â–Ž         | 157/5197 [14:55:28<489:05:02, 349.35s/it]                                                            3%|â–Ž         | 157/5197 [14:55:29<489:05:02, 349.35s/it]  3%|â–Ž         | 158/5197 [15:01:14<487:28:48, 348.27s/it]                                                            3%|â–Ž         | 158/5197 [15:01:14<487:28:48, 348.27s/it]  3%|â–Ž         | 159/5197 [15:06:43<479:14:10, 342.45s/it]                                                            3%|â–Ž         | 159/5197 [15:06:43<479:14:10, 342.45s/it]  3%|â–Ž         | 160/5197 [15:12:29<480:46:12, 343.61s/it]                                                            3%|â–Ž  {'loss': 0.976, 'learning_rate': 0.00019999951451478087, 'epoch': 0.03}
{'loss': 0.976, 'learning_rate': 0.00019999951451478087, 'epoch': 0.03}
{'loss': 0.9898, 'learning_rate': 0.00019999930090153334, 'epoch': 0.03}
{'loss': 0.9898, 'learning_rate': 0.00019999930090153334, 'epoch': 0.03}
{'loss': 0.9435, 'learning_rate': 0.00019999904844970962, 'epoch': 0.03}
  3%|â–Ž         | 158/5197 [15:01:42<487:29:02, 348.27s/it]                                                            3%|â–Ž         | 158/5197 [15:01:42<487:29:02, 348.27s/it]  3%|â–Ž         | 159/5197 [15:07:11<479:13:59, 342.45s/it]                                                            3%|â–Ž         | 159/5197 [15:07:11<479:13:59, 342.45s/it]  3%|â–Ž         | 160/5197 [15:12:57<480:45:51, 343.61s/it]                                                            3%|â–Ž         | 160/5197 [15:12:57<480:45:51, 343.61s/it]  3%|â–Ž         | 161/5197 [15:18:44<482:06:17, 344.63s/it]                                                            3%|â–Ž         | 161/5197 [15:18:44<482:06:17, 344.63s/it]  3%|â–Ž         | 162/5197 [15:24:21<478:44:06, 342.29s/it]                                                            3%|â–Ž         | 162/5197 [15:24:21<478:44:06, 342.29s/it]  3%|â–Ž         | 163/5197 [15:30:05<479:15:57, 342.74s/it]                                                    {'loss': 0.9435, 'learning_rate': 0.00019999904844970962, 'epoch': 0.03}
[2023-11-23 00:58:25,110] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2554, 'learning_rate': 0.00019999875715940782, 'epoch': 0.03}
{'loss': 0.2554, 'learning_rate': 0.00019999875715940782, 'epoch': 0.03}
[2023-11-23 01:04:13,142] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0764, 'learning_rate': 0.000199998427030741, 'epoch': 0.03}
{'loss': 1.0764, 'learning_rate': 0.000199998427030741, 'epoch': 0.03}
{'loss': 1.0032, 'learning_rate': 0.00019999805806383738, 'epoch': 0.03}
       | 160/5197 [15:12:29<480:46:12, 343.61s/it]  3%|â–Ž         | 161/5197 [15:18:16<482:06:36, 344.64s/it]                                                            3%|â–Ž         | 161/5197 [15:18:16<482:06:36, 344.64s/it]  3%|â–Ž         | 162/5197 [15:23:53<478:44:48, 342.30s/it]                                                            3%|â–Ž         | 162/5197 [15:23:53<478:44:48, 342.30s/it]  3%|â–Ž         | 163/5197 [15:29:37<479:16:32, 342.75s/it]                                                            3%|â–Ž         | 163/5197 [15:29:37<479:16:32, 342.75s/it]  3%|â–Ž         | 164/5197 [15:35:30<483:34:40, 345.89s/it]                                                            3%|â–Ž         | 164/5197 [15:35:30<483:34:40, 345.89s/it]  3%|â–Ž         | 165/5197 [15:41:18<484:22:45, 346.54s/it]                                                            3%|â–Ž         | 165/5197 [15:41:18<484:22:45, 346.54s/it]  3%|â–Ž         | 166/5197 [15:47:09<485:55:02, 347.70s/it]  {'loss': 1.0032, 'learning_rate': 0.00019999805806383738, 'epoch': 0.03}
{'loss': 1.0579, 'learning_rate': 0.0001999976502588403, 'epoch': 0.03}
{'loss': 1.0579, 'learning_rate': 0.0001999976502588403, 'epoch': 0.03}
                                                          3%|â–Ž         | 166/5197 [15:47:09<485:55:02, 347.70s/it]  3%|â–Ž         | 167/5197 [15:53:05<489:31:14, 350.35s/it]                                                            3%|â–Ž         | 167/5197 [15:53:05<489:31:14, 350.35s/it]wandb: Network error (ConnectionError), entering retry loop.
{'loss': 0.9589, 'learning_rate': 0.0001999972036159081, 'epoch': 0.03}
{'loss': 0.9589, 'learning_rate': 0.0001999972036159081, 'epoch': 0.03}
        3%|â–Ž         | 163/5197 [15:30:05<479:15:57, 342.74s/it]  3%|â–Ž         | 164/5197 [15:35:58<483:34:06, 345.89s/it]                                                            3%|â–Ž         | 164/5197 [15:35:58<483:34:06, 345.89s/it]  3%|â–Ž         | 165/5197 [15:41:46<484:22:17, 346.53s/it]                                                            3%|â–Ž         | 165/5197 [15:41:46<484:22:17, 346.53s/it]  3%|â–Ž         | 166/5197 [15:47:36<485:54:53, 347.70s/it]                                                            3%|â–Ž         | 166/5197 [15:47:36<485:54:53, 347.70s/it]  3%|â–Ž         | 167/5197 [15:53:33<489:30:43, 350.35s/it]                                                            3%|â–Ž         | 167/5197 [15:53:33<489:30:43, 350.35s/it]  3%|â–Ž         | 168/5197 [15:59:47<499:24:25, 357.50s/it]                                                            3%|â–Ž         | 168/5197 [15:59:47<499:24:25, 357.50s/it]  3%|â–Ž         | 169/5197 [16:05:56<504:06:{'loss': 0.9362, 'learning_rate': 0.00019999671813521435, 'epoch': 0.03}
{'loss': 0.9362, 'learning_rate': 0.00019999671813521435, 'epoch': 0.03}
{'loss': 0.9801, 'learning_rate': 0.0001999961938169475, 'epoch': 0.03}
{'loss': 0.9801, 'learning_rate': 0.0001999961938169475, 'epoch': 0.03}
[2023-11-23 01:40:07,071] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9941, 'learning_rate': 0.00019999563066131124, 'epoch': 0.03}
{'loss': 0.9941, 'learning_rate': 0.00019999563066131124, 'epoch': 0.03}
{'loss': 1.0539, 'learning_rate': 0.00019999502866852425, 'epoch': 0.03}
{'loss': 1.0539, 'learning_rate': 0.00019999502866852425, 'epoch': 0.03}
{'loss': 1.0146, 'learning_rate': 0.0001999943878388204, 'epoch': 0.03}
  3%|â–Ž         | 168/5197 [15:59:20<499:24:09, 357.50s/it]                                                            3%|â–Ž         | 168/5197 [15:59:20<499:24:09, 357.50s/it]  3%|â–Ž         | 169/5197 [16:05:28<504:06:37, 360.94s/it]                                                            3%|â–Ž         | 169/5197 [16:05:28<504:06:37, 360.94s/it]  3%|â–Ž         | 170/5197 [16:11:25<502:19:11, 359.73s/it]                                                            3%|â–Ž         | 170/5197 [16:11:25<502:19:11, 359.73s/it]  3%|â–Ž         | 171/5197 [16:17:12<496:51:25, 355.89s/it]                                                            3%|â–Ž         | 171/5197 [16:17:12<496:51:25, 355.89s/it]  3%|â–Ž         | 172/5197 [16:23:03<494:38:23, 354.37s/it]                                                            3%|â–Ž         | 172/5197 [16:23:03<494:38:23, 354.37s/it]  3%|â–Ž         | 173/5197 [16:29:08<498:46:44, 357.41s/it]                                                    {'loss': 1.0146, 'learning_rate': 0.0001999943878388204, 'epoch': 0.03}
{'loss': 0.9792, 'learning_rate': 0.00019999370817244853, 'epoch': 0.03}
13, 360.93s/it]                                                            3%|â–Ž         | 169/5197 [16:05:56<504:06:13, 360.93s/it]  3%|â–Ž         | 170/5197 [16:11:53<502:19:01, 359.73s/it]                                                            3%|â–Ž         | 170/5197 [16:11:53<502:19:01, 359.73s/it]  3%|â–Ž         | 171/5197 [16:17:40<496:51:38, 355.89s/it]                                                            3%|â–Ž         | 171/5197 [16:17:40<496:51:38, 355.89s/it]  3%|â–Ž         | 172/5197 [16:23:31<494:38:35, 354.37s/it]                                                            3%|â–Ž         | 172/5197 [16:23:31<494:38:35, 354.37s/it]  3%|â–Ž         | 173/5197 [16:29:35<498:47:15, 357.41s/it]                                                            3%|â–Ž         | 173/5197 [16:29:35<498:47:15, 357.41s/it]  3%|â–Ž         | 174/5197 [16:35:54<507:54:30, 364.02s/it]                                                            3%|â–Ž         | 174/5197 [16:35:54<{'loss': 0.9792, 'learning_rate': 0.00019999370817244853, 'epoch': 0.03}
{'loss': 0.9579, 'learning_rate': 0.00019999298966967265, 'epoch': 0.03}
{'loss': 0.9579, 'learning_rate': 0.00019999298966967265, 'epoch': 0.03}
{'loss': 0.9246, 'learning_rate': 0.00019999223233077177, 'epoch': 0.03}
{'loss': 0.9246, 'learning_rate': 0.00019999223233077177, 'epoch': 0.03}
{'loss': 0.9338, 'learning_rate': 0.0001999914361560401, 'epoch': 0.03}
{'loss': 0.9338, 'learning_rate': 0.0001999914361560401, 'epoch': 0.03}
{'loss': 0.9701, 'learning_rate': 0.00019999060114578684, 'epoch': 0.03}
{'loss': 0.9701, 'learning_rate': 0.00019999060114578684, 'epoch': 0.03}
        3%|â–Ž         | 173/5197 [16:29:08<498:46:44, 357.41s/it]  3%|â–Ž         | 174/5197 [16:35:27<507:54:37, 364.02s/it]                                                            3%|â–Ž         | 174/5197 [16:35:27<507:54:37, 364.02s/it]  3%|â–Ž         | 175/5197 [16:41:42<512:27:01, 367.35s/it]                                                            3%|â–Ž         | 175/5197 [16:41:42<512:27:01, 367.35s/it]  3%|â–Ž         | 176/5197 [16:47:58<516:04:09, 370.02s/it]                                                            3%|â–Ž         | 176/5197 [16:47:58<516:04:09, 370.02s/it]  3%|â–Ž         | 177/5197 [16:54:14<518:21:32, 371.73s/it]                                                            3%|â–Ž         | 177/5197 [16:54:14<518:21:32, 371.73s/it]  3%|â–Ž         | 178/5197 [17:00:23<517:07:46, 370.92s/it]                                                            3%|â–Ž         | 178/5197 [17:00:23<517:07:46, 370.92s/it]  3%|â–Ž         | 179/5197 [17:06:39<518:57:{'loss': 0.966, 'learning_rate': 0.00019998972730033622, 'epoch': 0.03}
{'loss': 0.966, 'learning_rate': 0.00019998972730033622, 'epoch': 0.03}
[2023-11-23 02:35:52,239] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
507:54:30, 364.02s/it]  3%|â–Ž         | 175/5197 [16:42:10<512:27:08, 367.35s/it]                                                            3%|â–Ž         | 175/5197 [16:42:10<512:27:08, 367.35s/it]  3%|â–Ž         | 176/5197 [16:48:26<516:04:18, 370.02s/it]                                                            3%|â–Ž         | 176/5197 [16:48:26<516:04:18, 370.02s/it]  3%|â–Ž         | 177/5197 [16:54:42<518:21:54, 371.74s/it]                                                            3%|â–Ž         | 177/5197 [16:54:42<518:21:54, 371.74s/it]  3%|â–Ž         | 178/5197 [17:00:51<517:08:22, 370.93s/it]                                                            3%|â–Ž         | 178/5197 [17:00:51<517:08:22, 370.93s/it]  3%|â–Ž         | 179/5197 [17:07:06<518:57:13, 372.31s/it]                                                            3%|â–Ž         | 179/5197 [17:07:06<518:57:13, 372.31s/it]  3%|â–Ž         | 180/5197 [17:13:25<521:32:45, 374.24s/it]                              {'loss': 0.2617, 'learning_rate': 0.00019998881462002778, 'epoch': 0.03}
{'loss': 0.2617, 'learning_rate': 0.00019998881462002778, 'epoch': 0.03}
[2023-11-23 02:42:10,376] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9583, 'learning_rate': 0.00019998786310521585, 'epoch': 0.03}
{'loss': 0.9583, 'learning_rate': 0.00019998786310521585, 'epoch': 0.03}
[2023-11-23 02:48:21,559] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0004, 'learning_rate': 0.00019998687275627006, 'epoch': 0.04}
{'loss': 1.0004, 'learning_rate': 0.00019998687275627006, 'epoch': 0.04}
{'loss': 1.0185, 'learning_rate': 0.00019998584357357502, 'epoch': 0.04}
{'loss': 1.0185, 'learning_rate': 0.00019998584357357502, 'epoch': 0.04}
{'loss': 0.9989, 'learning_rate': 0.00019998477555753055, 'epoch': 0.04}
{'loss': 0.9989, 'learning_rate': 0.00019998477555753055, 'epoch': 0.04}
03, 372.30s/it]                                                            3%|â–Ž         | 179/5197 [17:06:39<518:57:03, 372.30s/it]  3%|â–Ž         | 180/5197 [17:12:57<521:32:40, 374.24s/it]                                                            3%|â–Ž         | 180/5197 [17:12:57<521:32:40, 374.24s/it]  3%|â–Ž         | 181/5197 [17:19:16<523:03:53, 375.41s/it]                                                            3%|â–Ž         | 181/5197 [17:19:16<523:03:53, 375.41s/it]  4%|â–Ž         | 182/5197 [17:25:27<521:11:58, 374.14s/it]                                                            4%|â–Ž         | 182/5197 [17:25:27<521:11:58, 374.14s/it]  4%|â–Ž         | 183/5197 [17:31:44<522:19:17, 375.02s/it]                                                            4%|â–Ž         | 183/5197 [17:31:44<522:19:17, 375.02s/it]  4%|â–Ž         | 184/5197 [17:37:47<517:18:58, 371.50s/it]                                                            4%|â–Ž         | 184/5197 [17:37:47<{'loss': 0.9901, 'learning_rate': 0.00019998366870855133, 'epoch': 0.04}
{'loss': 0.9901, 'learning_rate': 0.00019998366870855133, 'epoch': 0.04}
                              3%|â–Ž         | 180/5197 [17:13:25<521:32:45, 374.24s/it]  3%|â–Ž         | 181/5197 [17:19:43<523:04:24, 375.41s/it]                                                            3%|â–Ž         | 181/5197 [17:19:43<523:04:24, 375.41s/it]  4%|â–Ž         | 182/5197 [17:25:54<521:11:46, 374.14s/it]                                                            4%|â–Ž         | 182/5197 [17:25:54<521:11:46, 374.14s/it]  4%|â–Ž         | 183/5197 [17:32:11<522:19:29, 375.02s/it]                                                            4%|â–Ž         | 183/5197 [17:32:11<522:19:29, 375.02s/it]  4%|â–Ž         | 184/5197 [17:38:15<517:18:55, 371.50s/it]                                                            4%|â–Ž         | 184/5197 [17:38:15<517:18:55, 371.50s/it]  4%|â–Ž         | 185/5197 [17:44:27<517:34:24, 371.76s/it]                                                            4%|â–Ž         | 185/5197 [17:44:27<517:34:24, 371.76s/it]  4%|â–Ž         | 186/{'loss': 0.9848, 'learning_rate': 0.0001999825230270673, 'epoch': 0.04}
{'loss': 0.9848, 'learning_rate': 0.0001999825230270673, 'epoch': 0.04}
[2023-11-23 03:18:56,468] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2652, 'learning_rate': 0.0001999813385135234, 'epoch': 0.04}
{'loss': 0.2652, 'learning_rate': 0.0001999813385135234, 'epoch': 0.04}
[2023-11-23 03:25:12,548] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0409, 'learning_rate': 0.00019998011516837974, 'epoch': 0.04}
{'loss': 1.0409, 'learning_rate': 0.00019998011516837974, 'epoch': 0.04}
{'loss': 1.059, 'learning_rate': 0.0001999788529921114, 'epoch': 0.04}
{'loss': 1.059, 'learning_rate': 0.0001999788529921114, 'epoch': 0.04}
[2023-11-23 03:37:18,967] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2505, 'learning_rate': 0.0001999775519852086, 'epoch': 0.04}
517:18:58, 371.50s/it]  4%|â–Ž         | 185/5197 [17:44:00<517:34:40, 371.76s/it]                                                            4%|â–Ž         | 185/5197 [17:44:00<517:34:40, 371.76s/it]  4%|â–Ž         | 186/5197 [17:50:01<513:19:11, 368.78s/it]                                                            4%|â–Ž         | 186/5197 [17:50:01<513:19:11, 368.78s/it]  4%|â–Ž         | 187/5197 [17:56:02<509:41:48, 366.25s/it]                                                            4%|â–Ž         | 187/5197 [17:56:02<509:41:48, 366.25s/it]  4%|â–Ž         | 188/5197 [18:02:18<513:41:58, 369.20s/it]                                                            4%|â–Ž         | 188/5197 [18:02:18<513:41:58, 369.20s/it]  4%|â–Ž         | 189/5197 [18:08:22<511:26:13, 367.65s/it]                                                            4%|â–Ž         | 189/5197 [18:08:22<511:26:13, 367.65s/it]  4%|â–Ž         | 190/5197 [18:14:24<509:08:44, 366.07s/it]                              {'loss': 0.2505, 'learning_rate': 0.0001999775519852086, 'epoch': 0.04}
[2023-11-23 03:43:17,008] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0118, 'learning_rate': 0.00019997621214817667, 'epoch': 0.04}
5197 [17:50:29<513:18:53, 368.78s/it]                                                            4%|â–Ž         | 186/5197 [17:50:29<513:18:53, 368.78s/it]  4%|â–Ž         | 187/5197 [17:56:29<509:41:28, 366.25s/it]                                                            4%|â–Ž         | 187/5197 [17:56:29<509:41:28, 366.25s/it]  4%|â–Ž         | 188/5197 [18:02:45<513:42:01, 369.20s/it]                                                            4%|â–Ž         | 188/5197 [18:02:45<513:42:01, 369.20s/it]  4%|â–Ž         | 189/5197 [18:08:49<511:26:15, 367.65s/it]                                                            4%|â–Ž         | 189/5197 [18:08:49<511:26:15, 367.65s/it]  4%|â–Ž         | 190/5197 [18:14:52<509:08:18, 366.07s/it]                                                            4%|â–Ž         | 190/5197 [18:14:52<509:08:18, 366.07s/it]  4%|â–Ž         | 191/5197 [18:20:50<505:41:15, 363.66s/it]                                                            4%|â–Ž        {'loss': 1.0118, 'learning_rate': 0.00019997621214817667, 'epoch': 0.04}
{'loss': 0.9414, 'learning_rate': 0.000199974833481536, 'epoch': 0.04}
{'loss': 0.9414, 'learning_rate': 0.000199974833481536, 'epoch': 0.04}
{'loss': 0.9938, 'learning_rate': 0.00019997341598582195, 'epoch': 0.04}
{'loss': 0.9938, 'learning_rate': 0.00019997341598582195, 'epoch': 0.04}
{'loss': 1.0494, 'learning_rate': 0.00019997195966158518, 'epoch': 0.04}
{'loss': 1.0494, 'learning_rate': 0.00019997195966158518, 'epoch': 0.04}
[2023-11-23 04:06:23,676] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0007, 'learning_rate': 0.0001999704645093912, 'epoch': 0.04}
{'loss': 1.0007, 'learning_rate': 0.0001999704645093912, 'epoch': 0.04}
                              4%|â–Ž         | 190/5197 [18:14:24<509:08:44, 366.07s/it]  4%|â–Ž         | 191/5197 [18:20:22<505:41:15, 363.66s/it]                                                            4%|â–Ž         | 191/5197 [18:20:22<505:41:15, 363.66s/it]  4%|â–Ž         | 192/5197 [18:26:12<499:53:19, 359.56s/it]                                                            4%|â–Ž         | 192/5197 [18:26:12<499:53:19, 359.56s/it]  4%|â–Ž         | 193/5197 [18:31:44<488:11:14, 351.21s/it]                                                            4%|â–Ž         | 193/5197 [18:31:44<488:11:14, 351.21s/it]  4%|â–Ž         | 194/5197 [18:37:39<489:49:06, 352.46s/it]                                                            4%|â–Ž         | 194/5197 [18:37:39<489:49:06, 352.46s/it]  4%|â–         | 195/5197 [18:43:29<488:31:13, 351.59s/it]                                                            4%|â–         | 195/5197 [18:43:29<488:31:13, 351.59s/it]  4%|â–         | 196/{'loss': 0.9767, 'learning_rate': 0.0001999689305298208, 'epoch': 0.04}
{'loss': 0.9767, 'learning_rate': 0.0001999689305298208, 'epoch': 0.04}
{'loss': 0.9344, 'learning_rate': 0.00019996735772346972, 'epoch': 0.04}
{'loss': 0.9344, 'learning_rate': 0.00019996735772346972, 'epoch': 0.04}
 | 191/5197 [18:20:50<505:41:15, 363.66s/it]  4%|â–Ž         | 192/5197 [18:26:40<499:53:21, 359.56s/it]                                                            4%|â–Ž         | 192/5197 [18:26:40<499:53:21, 359.56s/it]  4%|â–Ž         | 193/5197 [18:32:11<488:11:01, 351.21s/it]                                                            4%|â–Ž         | 193/5197 [18:32:11<488:11:01, 351.21s/it]  4%|â–Ž         | 194/5197 [18:38:07<489:49:21, 352.46s/it]                                                            4%|â–Ž         | 194/5197 [18:38:07<489:49:21, 352.46s/it]  4%|â–         | 195/5197 [18:43:56<488:31:00, 351.59s/it]                                                            4%|â–         | 195/5197 [18:43:56<488:31:00, 351.59s/it]  4%|â–         | 196/5197 [18:49:35<483:00:44, 347.70s/it]                                                            4%|â–         | 196/5197 [18:49:35<483:00:44, 347.70s/it]  4%|â–         | 197/5197 [18:55:46<492:28:31, 354.58s/it]        {'loss': 0.9085, 'learning_rate': 0.00019996574609094884, 'epoch': 0.04}
{'loss': 0.9085, 'learning_rate': 0.00019996574609094884, 'epoch': 0.04}
{'loss': 1.0101, 'learning_rate': 0.00019996409563288406, 'epoch': 0.04}
{'loss': 1.0101, 'learning_rate': 0.00019996409563288406, 'epoch': 0.04}
{'loss': 0.9395, 'learning_rate': 0.00019996240634991642, 'epoch': 0.04}
{'loss': 0.9395, 'learning_rate': 0.00019996240634991642, 'epoch': 0.04}
{'loss': 0.9639, 'learning_rate': 0.00019996067824270205, 'epoch': 0.04}
{'loss': 0.9639, 'learning_rate': 0.00019996067824270205, 'epoch': 0.04}
5197 [18:49:08<483:00:51, 347.70s/it]                                                            4%|â–         | 196/5197 [18:49:08<483:00:51, 347.70s/it]  4%|â–         | 197/5197 [18:55:18<492:28:21, 354.58s/it]                                                            4%|â–         | 197/5197 [18:55:18<492:28:21, 354.58s/it]  4%|â–         | 198/5197 [19:01:32<500:24:45, 360.37s/it]                                                            4%|â–         | 198/5197 [19:01:32<500:24:45, 360.37s/it]  4%|â–         | 199/5197 [19:07:53<508:59:40, 366.62s/it]                                                            4%|â–         | 199/5197 [19:07:53<508:59:40, 366.62s/it]  4%|â–         | 200/5197 [19:14:02<509:42:50, 367.21s/it]                                                            4%|â–         | 200/5197 [19:14:02<509:42:50, 367.21s/it]  4%|â–         | 201/5197 [19:20:15<512:15:47, 369.12s/it]                                                            4%|â–        {'loss': 1.0428, 'learning_rate': 0.00019995891131191205, 'epoch': 0.04}
{'loss': 1.0428, 'learning_rate': 0.00019995891131191205, 'epoch': 0.04}
                                                    4%|â–         | 197/5197 [18:55:46<492:28:31, 354.58s/it]  4%|â–         | 198/5197 [19:01:59<500:24:41, 360.37s/it]                                                            4%|â–         | 198/5197 [19:01:59<500:24:41, 360.37s/it]  4%|â–         | 199/5197 [19:08:21<508:59:37, 366.62s/it]                                                            4%|â–         | 199/5197 [19:08:21<508:59:37, 366.62s/it]  4%|â–         | 200/5197 [19:14:29<509:42:32, 367.21s/it]                                                            4%|â–         | 200/5197 [19:14:29<509:42:32, 367.21s/it]  4%|â–         | 201/5197 [19:20:43<512:16:09, 369.13s/it]                                                            4%|â–         | 201/5197 [19:20:43<512:16:09, 369.13s/it]  4%|â–         | 202/5197 [19:26:58<514:47:58, 371.03s/it]                                                            4%|â–         | 202/5197 [19:26:58<514:47:58, 371.03s/it] {'loss': 0.9323, 'learning_rate': 0.00019995710555823276, 'epoch': 0.04}
{'loss': 0.9323, 'learning_rate': 0.00019995710555823276, 'epoch': 0.04}
{'loss': 1.0021, 'learning_rate': 0.00019995526098236547, 'epoch': 0.04}
{'loss': 1.0021, 'learning_rate': 0.00019995526098236547, 'epoch': 0.04}
[2023-11-23 05:08:11,502] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2616, 'learning_rate': 0.0001999533775850266, 'epoch': 0.04}
{'loss': 0.2616, 'learning_rate': 0.0001999533775850266, 'epoch': 0.04}
[2023-11-23 05:14:26,830] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0001, 'learning_rate': 0.00019995145536694762, 'epoch': 0.04}
{'loss': 1.0001, 'learning_rate': 0.00019995145536694762, 'epoch': 0.04}
{'loss': 0.9535, 'learning_rate': 0.00019994949432887514, 'epoch': 0.04}
 | 201/5197 [19:20:15<512:15:47, 369.12s/it]  4%|â–         | 202/5197 [19:26:31<514:48:12, 371.03s/it]                                                            4%|â–         | 202/5197 [19:26:31<514:48:12, 371.03s/it]  4%|â–         | 203/5197 [19:32:46<516:34:40, 372.38s/it]                                                            4%|â–         | 203/5197 [19:32:46<516:34:40, 372.38s/it]  4%|â–         | 204/5197 [19:39:00<517:04:55, 372.82s/it]                                                            4%|â–         | 204/5197 [19:39:00<517:04:55, 372.82s/it]  4%|â–         | 205/5197 [19:45:17<518:29:09, 373.91s/it]                                                            4%|â–         | 205/5197 [19:45:17<518:29:09, 373.91s/it]  4%|â–         | 206/5197 [19:51:32<518:58:45, 374.34s/it]                                                            4%|â–         | 206/5197 [19:51:32<518:58:45, 374.34s/it]  4%|â–         | 207/5197 [19:57:24<509:36:07, 367.65s/it]        {'loss': 0.9535, 'learning_rate': 0.00019994949432887514, 'epoch': 0.04}
{'loss': 1.0266, 'learning_rate': 0.00019994749447157077, 'epoch': 0.04}
 4%|â–         | 203/5197 [19:33:14<516:34:39, 372.38s/it]                                                            4%|â–         | 203/5197 [19:33:14<516:34:39, 372.38s/it]  4%|â–         | 204/5197 [19:39:28<517:04:33, 372.82s/it]                                                            4%|â–         | 204/5197 [19:39:28<517:04:33, 372.82s/it]  4%|â–         | 205/5197 [19:45:44<518:29:47, 373.92s/it]                                                            4%|â–         | 205/5197 [19:45:44<518:29:47, 373.92s/it]  4%|â–         | 206/5197 [19:51:59<518:58:54, 374.34s/it]                                                            4%|â–         | 206/5197 [19:52:00<518:58:54, 374.34s/it]  4%|â–         | 207/5197 [19:57:52<509:36:39, 367.66s/it]                                                            4%|â–         | 207/5197 [19:57:52<509:36:39, 367.66s/it]  4%|â–         | 208/5197 [20:03:57<508:24:57, 366.87s/it]                                                      {'loss': 1.0266, 'learning_rate': 0.00019994749447157077, 'epoch': 0.04}
{'loss': 0.9621, 'learning_rate': 0.00019994545579581123, 'epoch': 0.04}
{'loss': 0.9621, 'learning_rate': 0.00019994545579581123, 'epoch': 0.04}
{'loss': 1.0017, 'learning_rate': 0.00019994337830238834, 'epoch': 0.04}
{'loss': 1.0017, 'learning_rate': 0.00019994337830238834, 'epoch': 0.04}
{'loss': 0.9478, 'learning_rate': 0.00019994126199210897, 'epoch': 0.04}
{'loss': 0.9478, 'learning_rate': 0.00019994126199210897, 'epoch': 0.04}
{'loss': 0.9729, 'learning_rate': 0.00019993910686579507, 'epoch': 0.04}
{'loss': 0.9729, 'learning_rate': 0.00019993910686579507, 'epoch': 0.04}
                                                    4%|â–         | 207/5197 [19:57:24<509:36:07, 367.65s/it]  4%|â–         | 208/5197 [20:03:29<508:24:36, 366.86s/it]                                                            4%|â–         | 208/5197 [20:03:29<508:24:36, 366.86s/it]  4%|â–         | 209/5197 [20:09:42<510:43:36, 368.61s/it]                                                            4%|â–         | 209/5197 [20:09:42<510:43:36, 368.61s/it]  4%|â–         | 210/5197 [20:15:54<511:57:17, 369.57s/it]                                                            4%|â–         | 210/5197 [20:15:54<511:57:17, 369.57s/it]  4%|â–         | 211/5197 [20:22:16<517:08:53, 373.39s/it]                                                            4%|â–         | 211/5197 [20:22:16<517:08:53, 373.39s/it]  4%|â–         | 212/5197 [20:28:33<518:24:41, 374.38s/it]                                                            4%|â–         | 212/5197 [20:28:33<518:24:41, 374.38s/it] {'loss': 0.963, 'learning_rate': 0.00019993691292428365, 'epoch': 0.04}
{'loss': 0.963, 'learning_rate': 0.00019993691292428365, 'epoch': 0.04}
      4%|â–         | 208/5197 [20:03:57<508:24:57, 366.87s/it]  4%|â–         | 209/5197 [20:10:09<510:44:11, 368.61s/it]                                                            4%|â–         | 209/5197 [20:10:09<510:44:11, 368.61s/it]  4%|â–         | 210/5197 [20:16:21<511:57:32, 369.57s/it]                                                            4%|â–         | 210/5197 [20:16:21<511:57:32, 369.57s/it]  4%|â–         | 211/5197 [20:22:43<517:08:33, 373.39s/it]                                                            4%|â–         | 211/5197 [20:22:43<517:08:33, 373.39s/it]  4%|â–         | 212/5197 [20:29:00<518:24:09, 374.37s/it]                                                            4%|â–         | 212/5197 [20:29:00<518:24:09, 374.37s/it]  4%|â–         | 213/5197 [20:34:33<500:56:44, 361.84s/it]                                                            4%|â–         | 213/5197 [20:34:33<500:56:44, 361.84s/it]  4%|â–         | 214/5197 [20:40:25<496:57:07{'loss': 0.9424, 'learning_rate': 0.00019993468016842682, 'epoch': 0.04}
{'loss': 0.9424, 'learning_rate': 0.00019993468016842682, 'epoch': 0.04}
{'loss': 0.9106, 'learning_rate': 0.00019993240859909176, 'epoch': 0.04}
{'loss': 0.9106, 'learning_rate': 0.00019993240859909176, 'epoch': 0.04}
{'loss': 0.9865, 'learning_rate': 0.00019993009821716074, 'epoch': 0.04}
{'loss': 0.9865, 'learning_rate': 0.00019993009821716074, 'epoch': 0.04}
[2023-11-23 06:20:45,148] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2845, 'learning_rate': 0.00019992774902353105, 'epoch': 0.04}
{'loss': 0.2845, 'learning_rate': 0.00019992774902353105, 'epoch': 0.04}
[2023-11-23 06:26:45,137] [WARNING] [stage3.py:1850:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2853, 'learning_rate': 0.0001999253610191151, 'epoch': 0.04}
 4%|â–         | 213/5197 [20:34:05<500:57:12, 361.84s/it]                                                            4%|â–         | 213/5197 [20:34:05<500:57:12, 361.84s/it]  4%|â–         | 214/5197 [20:39:58<496:57:28, 359.03s/it]                                                            4%|â–         | 214/5197 [20:39:58<496:57:28, 359.03s/it]  4%|â–         | 215/5197 [20:45:56<496:41:15, 358.91s/it]                                                            4%|â–         | 215/5197 [20:45:56<496:41:15, 358.91s/it]  4%|â–         | 216/5197 [20:51:55<496:36:20, 358.92s/it]                                                            4%|â–         | 216/5197 [20:51:55<496:36:20, 358.92s/it]  4%|â–         | 217/5197 [20:57:50<494:55:43, 357.78s/it]                                                            4%|â–         | 217/5197 [20:57:50<494:55:43, 357.78s/it]  4%|â–         | 218/5197 [21:03:50<495:44:40, 358.44s/it]                                                      {'loss': 0.2853, 'learning_rate': 0.0001999253610191151, 'epoch': 0.04}
{'loss': 0.9695, 'learning_rate': 0.00019992293420484039, 'epoch': 0.04}
, 359.03s/it]                                                            4%|â–         | 214/5197 [20:40:25<496:57:07, 359.03s/it]  4%|â–         | 215/5197 [20:46:24<496:40:55, 358.90s/it]                                                            4%|â–         | 215/5197 [20:46:24<496:40:55, 358.90s/it]  4%|â–         | 216/5197 [20:52:23<496:36:24, 358.92s/it]                                                            4%|â–         | 216/5197 [20:52:23<496:36:24, 358.92s/it]  4%|â–         | 217/5197 [20:58:18<494:56:05, 357.78s/it]                                                            4%|â–         | 217/5197 [20:58:18<494:56:05, 357.78s/it]  4%|â–         | 218/5197 [21:04:18<495:45:01, 358.45s/it]                                                            4%|â–         | 218/5197 [21:04:18<495:45:01, 358.45s/it]  4%|â–         | 219/5197 [21:10:11<493:26:50, 356.85s/it]                                                            4%|â–         | 219/5197 [21:10:11<49{'loss': 0.9695, 'learning_rate': 0.00019992293420484039, 'epoch': 0.04}
[2023-11-23 06:38:27,757] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3098, 'learning_rate': 0.00019992046858164944, 'epoch': 0.04}
{'loss': 0.3098, 'learning_rate': 0.00019992046858164944, 'epoch': 0.04}
[2023-11-23 06:44:24,426] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0573, 'learning_rate': 0.0001999179641504999, 'epoch': 0.04}
{'loss': 1.0573, 'learning_rate': 0.0001999179641504999, 'epoch': 0.04}
{'loss': 1.0053, 'learning_rate': 0.00019991542091236437, 'epoch': 0.04}
{'loss': 1.0053, 'learning_rate': 0.00019991542091236437, 'epoch': 0.04}
{'loss': 1.0454, 'learning_rate': 0.00019991283886823074, 'epoch': 0.04}
{'loss': 1.0454, 'learning_rate': 0.00019991283886823074, 'epoch': 0.04}
      4%|â–         | 218/5197 [21:03:50<495:44:40, 358.44s/it]  4%|â–         | 219/5197 [21:09:43<493:26:37, 356.85s/it]                                                            4%|â–         | 219/5197 [21:09:44<493:26:37, 356.85s/it]  4%|â–         | 220/5197 [21:15:33<490:17:27, 354.64s/it]                                                            4%|â–         | 220/5197 [21:15:33<490:17:27, 354.64s/it]  4%|â–         | 221/5197 [21:21:30<491:01:57, 355.25s/it]                                                            4%|â–         | 221/5197 [21:21:30<491:01:57, 355.25s/it]  4%|â–         | 222/5197 [21:27:27<491:36:39, 355.74s/it]                                                            4%|â–         | 222/5197 [21:27:27<491:36:39, 355.74s/it]  4%|â–         | 223/5197 [21:33:13<487:31:43, 352.86s/it]                                                            4%|â–         | 223/5197 [21:33:13<487:31:43, 352.86s/it]  4%|â–         | 224/5197 [21:39:16<491:49:44{'loss': 1.0139, 'learning_rate': 0.00019991021801910177, 'epoch': 0.04}
{'loss': 1.0139, 'learning_rate': 0.00019991021801910177, 'epoch': 0.04}
3:26:50, 356.85s/it]  4%|â–         | 220/5197 [21:16:00<490:17:40, 354.64s/it]                                                            4%|â–         | 220/5197 [21:16:00<490:17:40, 354.64s/it]  4%|â–         | 221/5197 [21:21:57<491:02:07, 355.25s/it]                                                            4%|â–         | 221/5197 [21:21:57<491:02:07, 355.25s/it]  4%|â–         | 222/5197 [21:27:54<491:36:27, 355.74s/it]                                                            4%|â–         | 222/5197 [21:27:54<491:36:27, 355.74s/it]  4%|â–         | 223/5197 [21:33:40<487:31:46, 352.86s/it]                                                            4%|â–         | 223/5197 [21:33:40<487:31:46, 352.86s/it]  4%|â–         | 224/5197 [21:39:44<491:49:15, 356.03s/it]                                                            4%|â–         | 224/5197 [21:39:44<491:49:15, 356.03s/it]  4%|â–         | 225/5197 [21:45:40<491:53:50, 356.16s/it]                                {'loss': 0.9401, 'learning_rate': 0.00019990755836599538, 'epoch': 0.04}
{'loss': 0.9401, 'learning_rate': 0.00019990755836599538, 'epoch': 0.04}
[2023-11-23 07:14:08,078] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2684, 'learning_rate': 0.0001999048599099446, 'epoch': 0.04}
{'loss': 0.2684, 'learning_rate': 0.0001999048599099446, 'epoch': 0.04}
[2023-11-23 07:20:02,282] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9974, 'learning_rate': 0.00019990212265199738, 'epoch': 0.04}
{'loss': 0.9974, 'learning_rate': 0.00019990212265199738, 'epoch': 0.04}
{'loss': 1.0001, 'learning_rate': 0.0001998993465932169, 'epoch': 0.04}
{'loss': 1.0001, 'learning_rate': 0.0001998993465932169, 'epoch': 0.04}
{'loss': 0.9695, 'learning_rate': 0.00019989653173468135, 'epoch': 0.04}
{'loss': 0.9695, 'learning_rate': 0.00019989653173468135, 'epoch': 0.04}
, 356.04s/it]                                                            4%|â–         | 224/5197 [21:39:16<491:49:44, 356.04s/it]  4%|â–         | 225/5197 [21:45:13<491:54:09, 356.16s/it]                                                            4%|â–         | 225/5197 [21:45:13<491:54:09, 356.16s/it]  4%|â–         | 226/5197 [21:51:13<493:41:26, 357.53s/it]                                                            4%|â–         | 226/5197 [21:51:13<493:41:26, 357.53s/it]  4%|â–         | 227/5197 [21:57:08<492:12:56, 356.53s/it]                                                            4%|â–         | 227/5197 [21:57:08<492:12:56, 356.53s/it]  4%|â–         | 228/5197 [22:02:22<474:31:45, 343.79s/it]                                                            4%|â–         | 228/5197 [22:02:22<474:31:45, 343.79s/it]  4%|â–         | 229/5197 [22:07:57<471:03:14, 341.34s/it]                                                            4%|â–         | 229/5197 [22:07:57<47{'loss': 0.9787, 'learning_rate': 0.000199893678077484, 'epoch': 0.04}
{'loss': 0.9787, 'learning_rate': 0.000199893678077484, 'epoch': 0.04}
                            4%|â–         | 225/5197 [21:45:40<491:53:50, 356.16s/it]  4%|â–         | 226/5197 [21:51:41<493:41:19, 357.53s/it]                                                            4%|â–         | 226/5197 [21:51:41<493:41:19, 357.53s/it]  4%|â–         | 227/5197 [21:57:35<492:12:47, 356.53s/it]                                                            4%|â–         | 227/5197 [21:57:35<492:12:47, 356.53s/it]  4%|â–         | 228/5197 [22:02:49<474:31:49, 343.79s/it]                                                            4%|â–         | 228/5197 [22:02:49<474:31:49, 343.79s/it]  4%|â–         | 229/5197 [22:08:25<471:03:07, 341.34s/it]                                                            4%|â–         | 229/5197 [22:08:25<471:03:07, 341.34s/it]  4%|â–         | 230/5197 [22:14:06<471:08:23, 341.47s/it]                                                            4%|â–         | 230/5197 [22:14:06<471:08:23, 341.47s/it]  4%|â–         | 231/51{'loss': 0.9787, 'learning_rate': 0.00019989078562273314, 'epoch': 0.04}
{'loss': 0.9787, 'learning_rate': 0.00019989078562273314, 'epoch': 0.04}
{'loss': 0.9842, 'learning_rate': 0.0001998878543715522, 'epoch': 0.04}
{'loss': 0.9842, 'learning_rate': 0.0001998878543715522, 'epoch': 0.04}
{'loss': 1.0239, 'learning_rate': 0.0001998848843250796, 'epoch': 0.04}
{'loss': 1.0239, 'learning_rate': 0.0001998848843250796, 'epoch': 0.04}
{'loss': 0.9393, 'learning_rate': 0.00019988187548446894, 'epoch': 0.05}
{'loss': 0.9393, 'learning_rate': 0.00019988187548446894, 'epoch': 0.05}
{'loss': 0.9967, 'learning_rate': 0.0001998788278508888, 'epoch': 0.05}
1:03:14, 341.34s/it]  4%|â–         | 230/5197 [22:13:39<471:08:33, 341.48s/it]                                                            4%|â–         | 230/5197 [22:13:39<471:08:33, 341.48s/it]  4%|â–         | 231/5197 [22:19:35<477:07:45, 345.89s/it]                                                            4%|â–         | 231/5197 [22:19:35<477:07:45, 345.89s/it]  4%|â–         | 232/5197 [22:25:32<481:39:43, 349.24s/it]                                                            4%|â–         | 232/5197 [22:25:32<481:39:43, 349.24s/it]  4%|â–         | 233/5197 [22:31:18<480:13:29, 348.27s/it]                                                            4%|â–         | 233/5197 [22:31:18<480:13:29, 348.27s/it]  5%|â–         | 234/5197 [22:37:01<477:59:40, 346.72s/it]                                                            5%|â–         | 234/5197 [22:37:01<477:59:40, 346.72s/it]  5%|â–         | 235/5197 [22:43:01<483:22:00, 350.69s/it]                                {'loss': 0.9967, 'learning_rate': 0.0001998788278508888, 'epoch': 0.05}
{'loss': 0.9002, 'learning_rate': 0.00019987574142552275, 'epoch': 0.05}
{'loss': 0.9002, 'learning_rate': 0.00019987574142552275, 'epoch': 0.05}
97 [22:20:03<477:07:50, 345.89s/it]                                                            4%|â–         | 231/5197 [22:20:03<477:07:50, 345.89s/it]  4%|â–         | 232/5197 [22:26:00<481:39:34, 349.24s/it]                                                            4%|â–         | 232/5197 [22:26:00<481:39:34, 349.24s/it]  4%|â–         | 233/5197 [22:31:46<480:13:33, 348.27s/it]                                                            4%|â–         | 233/5197 [22:31:46<480:13:33, 348.27s/it]  5%|â–         | 234/5197 [22:37:29<477:59:57, 346.73s/it]                                                            5%|â–         | 234/5197 [22:37:29<477:59:57, 346.73s/it]  5%|â–         | 235/5197 [22:43:29<483:22:25, 350.69s/it]                                                            5%|â–         | 235/5197 [22:43:29<483:22:25, 350.69s/it]  5%|â–         | 236/5197 [22:49:26<485:54:58, 352.61s/it]                                                            5%|â–         |{'loss': 0.9283, 'learning_rate': 0.00019987261620956964, 'epoch': 0.05}
{'loss': 0.9283, 'learning_rate': 0.00019987261620956964, 'epoch': 0.05}
{'loss': 1.033, 'learning_rate': 0.00019986945220424324, 'epoch': 0.05}
{'loss': 1.033, 'learning_rate': 0.00019986945220424324, 'epoch': 0.05}
{'loss': 0.9901, 'learning_rate': 0.00019986624941077238, 'epoch': 0.05}
{'loss': 0.9901, 'learning_rate': 0.00019986624941077238, 'epoch': 0.05}
{'loss': 1.0178, 'learning_rate': 0.000199863007830401, 'epoch': 0.05}
{'loss': 1.0178, 'learning_rate': 0.000199863007830401, 'epoch': 0.05}
                            5%|â–         | 235/5197 [22:43:01<483:22:00, 350.69s/it]  5%|â–         | 236/5197 [22:48:58<485:54:23, 352.60s/it]                                                            5%|â–         | 236/5197 [22:48:58<485:54:23, 352.60s/it]  5%|â–         | 237/5197 [22:55:04<491:11:41, 356.51s/it]                                                            5%|â–         | 237/5197 [22:55:04<491:11:41, 356.51s/it]  5%|â–         | 238/5197 [23:01:03<492:15:26, 357.36s/it]                                                            5%|â–         | 238/5197 [23:01:03<492:15:26, 357.36s/it]  5%|â–         | 239/5197 [23:06:59<491:22:41, 356.79s/it]                                                            5%|â–         | 239/5197 [23:06:59<491:22:41, 356.79s/it]  5%|â–         | 240/5197 [23:12:59<492:42:28, 357.83s/it]                                                            5%|â–         | 240/5197 [23:12:59<492:42:28, 357.83s/it]  5%|â–         | 241/51{'loss': 1.0638, 'learning_rate': 0.0001998597274643881, 'epoch': 0.05}
{'loss': 1.0638, 'learning_rate': 0.0001998597274643881, 'epoch': 0.05}
{'loss': 0.995, 'learning_rate': 0.00019985640831400776, 'epoch': 0.05}
 236/5197 [22:49:26<485:54:58, 352.61s/it]  5%|â–         | 237/5197 [22:55:31<491:12:09, 356.52s/it]                                                            5%|â–         | 237/5197 [22:55:31<491:12:09, 356.52s/it]  5%|â–         | 238/5197 [23:01:31<492:15:48, 357.36s/it]                                                            5%|â–         | 238/5197 [23:01:31<492:15:48, 357.36s/it]  5%|â–         | 239/5197 [23:07:26<491:22:56, 356.79s/it]                                                            5%|â–         | 239/5197 [23:07:26<491:22:56, 356.79s/it]  5%|â–         | 240/5197 [23:13:26<492:42:20, 357.83s/it]                                                            5%|â–         | 240/5197 [23:13:26<492:42:20, 357.83s/it]  5%|â–         | 241/5197 [23:19:23<492:06:18, 357.46s/it]                                                            5%|â–         | 241/5197 [23:19:23<492:06:18, 357.46s/it]  5%|â–         | 242/5197 [23:25:21<492:24:17, 357.75s/it]          {'loss': 0.995, 'learning_rate': 0.00019985640831400776, 'epoch': 0.05}
{'loss': 0.9413, 'learning_rate': 0.0001998530503805491, 'epoch': 0.05}
{'loss': 0.9413, 'learning_rate': 0.0001998530503805491, 'epoch': 0.05}
{'loss': 0.9905, 'learning_rate': 0.00019984965366531623, 'epoch': 0.05}
{'loss': 0.9905, 'learning_rate': 0.00019984965366531623, 'epoch': 0.05}
[2023-11-23 09:05:26,111] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9707, 'learning_rate': 0.00019984621816962844, 'epoch': 0.05}
{'loss': 0.9707, 'learning_rate': 0.00019984621816962844, 'epoch': 0.05}
{'loss': 0.9808, 'learning_rate': 0.00019984274389482005, 'epoch': 0.05}
{'loss': 0.9808, 'learning_rate': 0.00019984274389482005, 'epoch': 0.05}
97 [23:18:56<492:06:16, 357.46s/it]                                                            5%|â–         | 241/5197 [23:18:56<492:06:16, 357.46s/it]  5%|â–         | 242/5197 [23:24:54<492:24:01, 357.75s/it]                                                            5%|â–         | 242/5197 [23:24:54<492:24:01, 357.75s/it]  5%|â–         | 243/5197 [23:30:46<489:43:51, 355.88s/it]                                                            5%|â–         | 243/5197 [23:30:46<489:43:51, 355.88s/it]  5%|â–         | 244/5197 [23:36:31<485:28:04, 352.85s/it]                                                            5%|â–         | 244/5197 [23:36:31<485:28:04, 352.85s/it]  5%|â–         | 245/5197 [23:42:31<488:18:18, 354.99s/it]                                                            5%|â–         | 245/5197 [23:42:31<488:18:18, 354.99s/it]  5%|â–         | 246/5197 [23:48:27<488:39:40, 355.32s/it]                                                            5%|â–         |{'loss': 1.0194, 'learning_rate': 0.00019983923084224045, 'epoch': 0.05}
{'loss': 1.0194, 'learning_rate': 0.00019983923084224045, 'epoch': 0.05}
                                                  5%|â–         | 242/5197 [23:25:22<492:24:17, 357.75s/it]  5%|â–         | 243/5197 [23:31:13<489:43:31, 355.88s/it]                                                            5%|â–         | 243/5197 [23:31:13<489:43:31, 355.88s/it]  5%|â–         | 244/5197 [23:36:59<485:28:04, 352.85s/it]                                                            5%|â–         | 244/5197 [23:36:59<485:28:04, 352.85s/it]  5%|â–         | 245/5197 [23:42:59<488:18:17, 354.99s/it]                                                            5%|â–         | 245/5197 [23:42:59<488:18:17, 354.99s/it]  5%|â–         | 246/5197 [23:48:55<488:39:37, 355.32s/it]                                                            5%|â–         | 246/5197 [23:48:55<488:39:37, 355.32s/it]  5%|â–         | 247/5197 [23:54:50<488:39:34, 355.39s/it]                                                            5%|â–         | 247/5197 [23:54:50<488:39:34, 355.39s/it]  5{'loss': 1.0076, 'learning_rate': 0.00019983567901325403, 'epoch': 0.05}
{'loss': 1.0076, 'learning_rate': 0.00019983567901325403, 'epoch': 0.05}
{'loss': 0.9581, 'learning_rate': 0.00019983208840924026, 'epoch': 0.05}
{'loss': 0.9581, 'learning_rate': 0.00019983208840924026, 'epoch': 0.05}
{'loss': 1.0081, 'learning_rate': 0.0001998284590315937, 'epoch': 0.05}
{'loss': 1.0081, 'learning_rate': 0.0001998284590315937, 'epoch': 0.05}
[2023-11-23 09:40:26,171] [WARNING] [stage3.py:1850:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2806, 'learning_rate': 0.00019982479088172405, 'epoch': 0.05}
{'loss': 0.2806, 'learning_rate': 0.00019982479088172405, 'epoch': 0.05}
{'loss': 1.0526, 'learning_rate': 0.00019982108396105584, 'epoch': 0.05}
 246/5197 [23:48:27<488:39:40, 355.32s/it]  5%|â–         | 247/5197 [23:54:23<488:39:30, 355.39s/it]                                                            5%|â–         | 247/5197 [23:54:23<488:39:30, 355.39s/it]  5%|â–         | 248/5197 [24:00:13<486:09:09, 353.64s/it]                                                            5%|â–         | 248/5197 [24:00:13<486:09:09, 353.64s/it]  5%|â–         | 249/5197 [24:05:55<481:33:02, 350.36s/it]                                                            5%|â–         | 249/5197 [24:05:55<481:33:02, 350.36s/it]  5%|â–         | 250/5197 [24:11:47<482:00:31, 350.76s/it]                                                            5%|â–         | 250/5197 [24:11:47<482:00:31, 350.76s/it]  5%|â–         | 251/5197 [24:17:31<479:18:22, 348.87s/it]                                                            5%|â–         | 251/5197 [24:17:31<479:18:22, 348.87s/it]  5%|â–         | 252/5197 [24:23:30<483:11:46, 351.77s/it]          {'loss': 1.0526, 'learning_rate': 0.00019982108396105584, 'epoch': 0.05}
%|â–         | 248/5197 [24:00:40<486:08:54, 353.63s/it]                                                            5%|â–         | 248/5197 [24:00:40<486:08:54, 353.63s/it]  5%|â–         | 249/5197 [24:06:23<481:32:57, 350.36s/it]                                                            5%|â–         | 249/5197 [24:06:23<481:32:57, 350.36s/it]  5%|â–         | 250/5197 [24:12:14<482:00:32, 350.76s/it]                                                            5%|â–         | 250/5197 [24:12:14<482:00:32, 350.76s/it]  5%|â–         | 251/5197 [24:17:59<479:18:09, 348.87s/it]                                                            5%|â–         | 251/5197 [24:17:59<479:18:09, 348.87s/it]  5%|â–         | 252/5197 [24:23:57<483:11:33, 351.77s/it]                                                            5%|â–         | 252/5197 [24:23:57<483:11:33, 351.77s/it]  5%|â–         | 253/5197 [24:29:52<484:16:27, 352.63s/it]                                                        {'loss': 1.0259, 'learning_rate': 0.00019981733827102884, 'epoch': 0.05}
{'loss': 1.0259, 'learning_rate': 0.00019981733827102884, 'epoch': 0.05}
{'loss': 0.9457, 'learning_rate': 0.00019981355381309789, 'epoch': 0.05}
{'loss': 0.9457, 'learning_rate': 0.00019981355381309789, 'epoch': 0.05}
{'loss': 0.9333, 'learning_rate': 0.0001998097305887328, 'epoch': 0.05}
{'loss': 0.9333, 'learning_rate': 0.0001998097305887328, 'epoch': 0.05}
{'loss': 0.938, 'learning_rate': 0.00019980586859941847, 'epoch': 0.05}
{'loss': 0.938, 'learning_rate': 0.00019980586859941847, 'epoch': 0.05}
{'loss': 0.9835, 'learning_rate': 0.00019980196784665478, 'epoch': 0.05}
{'loss': 0.9835, 'learning_rate': 0.00019980196784665478, 'epoch': 0.05}
                                                  5%|â–         | 252/5197 [24:23:30<483:11:46, 351.77s/it]  5%|â–         | 253/5197 [24:29:25<484:16:43, 352.63s/it]                                                            5%|â–         | 253/5197 [24:29:25<484:16:43, 352.63s/it]  5%|â–         | 254/5197 [24:35:21<485:37:57, 353.69s/it]                                                            5%|â–         | 254/5197 [24:35:21<485:37:57, 353.69s/it]  5%|â–         | 255/5197 [24:41:17<486:30:34, 354.40s/it]                                                            5%|â–         | 255/5197 [24:41:17<486:30:34, 354.40s/it]  5%|â–         | 256/5197 [24:47:03<482:53:15, 351.83s/it]                                                            5%|â–         | 256/5197 [24:47:03<482:53:15, 351.83s/it]  5%|â–         | 257/5197 [24:52:36<474:59:51, 346.15s/it]                                                            5%|â–         | 257/5197 [24:52:36<474:59:51, 346.15s/it]  5{'loss': 0.9703, 'learning_rate': 0.00019979802833195682, 'epoch': 0.05}
{'loss': 0.9703, 'learning_rate': 0.00019979802833195682, 'epoch': 0.05}
    5%|â–         | 253/5197 [24:29:52<484:16:27, 352.63s/it]  5%|â–         | 254/5197 [24:35:48<485:37:44, 353.68s/it]                                                            5%|â–         | 254/5197 [24:35:48<485:37:44, 353.68s/it]  5%|â–         | 255/5197 [24:41:44<486:30:25, 354.40s/it]                                                            5%|â–         | 255/5197 [24:41:44<486:30:25, 354.40s/it]  5%|â–         | 256/5197 [24:47:30<482:53:30, 351.83s/it]                                                            5%|â–         | 256/5197 [24:47:30<482:53:30, 351.83s/it]  5%|â–         | 257/5197 [24:53:03<474:59:57, 346.15s/it]                                                            5%|â–         | 257/5197 [24:53:03<474:59:57, 346.15s/it]  5%|â–         | 258/5197 [24:58:50<475:04:25, 346.28s/it]                                                            5%|â–         | 258/5197 [24:58:50<475:04:25, 346.28s/it]  5%|â–         | 259/5197 [25:04:41<477:12:53, {'loss': 0.9312, 'learning_rate': 0.00019979405005685465, 'epoch': 0.05}
{'loss': 0.9312, 'learning_rate': 0.00019979405005685465, 'epoch': 0.05}
{'loss': 0.9375, 'learning_rate': 0.00019979003302289335, 'epoch': 0.05}
{'loss': 0.9375, 'learning_rate': 0.00019979003302289335, 'epoch': 0.05}
{'loss': 1.0105, 'learning_rate': 0.0001997859772316331, 'epoch': 0.05}
{'loss': 1.0105, 'learning_rate': 0.0001997859772316331, 'epoch': 0.05}
{'loss': 0.9249, 'learning_rate': 0.00019978188268464912, 'epoch': 0.05}
{'loss': 0.9249, 'learning_rate': 0.00019978188268464912, 'epoch': 0.05}
{'loss': 0.9619, 'learning_rate': 0.0001997777493835317, 'epoch': 0.05}
%|â–         | 258/5197 [24:58:22<475:04:28, 346.28s/it]                                                            5%|â–         | 258/5197 [24:58:22<475:04:28, 346.28s/it]  5%|â–         | 259/5197 [25:04:14<477:12:20, 347.90s/it]                                                            5%|â–         | 259/5197 [25:04:14<477:12:20, 347.90s/it]  5%|â–Œ         | 260/5197 [25:10:09<480:13:43, 350.18s/it]                                                            5%|â–Œ         | 260/5197 [25:10:09<480:13:43, 350.18s/it]  5%|â–Œ         | 261/5197 [25:16:07<483:20:31, 352.52s/it]                                                            5%|â–Œ         | 261/5197 [25:16:07<483:20:31, 352.52s/it]  5%|â–Œ         | 262/5197 [25:22:02<483:58:30, 353.05s/it]                                                            5%|â–Œ         | 262/5197 [25:22:02<483:58:30, 353.05s/it]  5%|â–Œ         | 263/5197 [25:27:49<481:23:56, 351.24s/it]                                                        {'loss': 0.9619, 'learning_rate': 0.0001997777493835317, 'epoch': 0.05}
{'loss': 0.9081, 'learning_rate': 0.00019977357732988614, 'epoch': 0.05}
{'loss': 0.9081, 'learning_rate': 0.00019977357732988614, 'epoch': 0.05}
347.91s/it]                                                            5%|â–         | 259/5197 [25:04:41<477:12:53, 347.91s/it]  5%|â–Œ         | 260/5197 [25:10:37<480:13:59, 350.18s/it]                                                            5%|â–Œ         | 260/5197 [25:10:37<480:13:59, 350.18s/it]  5%|â–Œ         | 261/5197 [25:16:35<483:20:24, 352.52s/it]                                                            5%|â–Œ         | 261/5197 [25:16:35<483:20:24, 352.52s/it]  5%|â–Œ         | 262/5197 [25:22:29<483:58:12, 353.05s/it]                                                            5%|â–Œ         | 262/5197 [25:22:29<483:58:12, 353.05s/it]  5%|â–Œ         | 263/5197 [25:28:16<481:23:41, 351.24s/it]                                                            5%|â–Œ         | 263/5197 [25:28:16<481:23:41, 351.24s/it]  5%|â–Œ         | 264/5197 [25:34:14<484:12:35, 353.37s/it]                                                            5%|â–Œ         | 264/5197 [25:34:14<484:{'loss': 0.9929, 'learning_rate': 0.0001997693665253329, 'epoch': 0.05}
{'loss': 0.9929, 'learning_rate': 0.0001997693665253329, 'epoch': 0.05}
{'loss': 0.9835, 'learning_rate': 0.0001997651169715073, 'epoch': 0.05}
{'loss': 0.9835, 'learning_rate': 0.0001997651169715073, 'epoch': 0.05}
{'loss': 1.0527, 'learning_rate': 0.00019976082867005984, 'epoch': 0.05}
{'loss': 1.0527, 'learning_rate': 0.00019976082867005984, 'epoch': 0.05}
[2023-11-23 11:20:01,714] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.986, 'learning_rate': 0.00019975650162265608, 'epoch': 0.05}
{'loss': 0.986, 'learning_rate': 0.00019975650162265608, 'epoch': 0.05}
[2023-11-23 11:25:51,612] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
    5%|â–Œ         | 263/5197 [25:27:49<481:23:56, 351.24s/it]  5%|â–Œ         | 264/5197 [25:33:47<484:12:23, 353.36s/it]                                                            5%|â–Œ         | 264/5197 [25:33:47<484:12:23, 353.36s/it]  5%|â–Œ         | 265/5197 [25:39:40<483:52:39, 353.20s/it]                                                            5%|â–Œ         | 265/5197 [25:39:40<483:52:39, 353.20s/it]  5%|â–Œ         | 266/5197 [25:45:32<483:26:17, 352.95s/it]                                                            5%|â–Œ         | 266/5197 [25:45:32<483:26:17, 352.95s/it]  5%|â–Œ         | 267/5197 [25:51:17<480:12:15, 350.66s/it]                                                            5%|â–Œ         | 267/5197 [25:51:17<480:12:15, 350.66s/it]  5%|â–Œ         | 268/5197 [25:57:07<479:39:39, 350.33s/it]                                                            5%|â–Œ         | 268/5197 [25:57:07<479:39:39, 350.33s/it]  5%|â–Œ         | 269/5197 [26:02:57<479:23:02, {'loss': 0.9112, 'learning_rate': 0.0001997521358309766, 'epoch': 0.05}
{'loss': 0.9112, 'learning_rate': 0.0001997521358309766, 'epoch': 0.05}
[2023-11-23 11:31:45,421] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
12:35, 353.37s/it]  5%|â–Œ         | 265/5197 [25:40:07<483:52:02, 353.19s/it]                                                            5%|â–Œ         | 265/5197 [25:40:07<483:52:02, 353.19s/it]  5%|â–Œ         | 266/5197 [25:45:59<483:25:42, 352.94s/it]                                                            5%|â–Œ         | 266/5197 [25:45:59<483:25:42, 352.94s/it]  5%|â–Œ         | 267/5197 [25:51:45<480:11:44, 350.65s/it]                                                            5%|â–Œ         | 267/5197 [25:51:45<480:11:44, 350.65s/it]  5%|â–Œ         | 268/5197 [25:57:34<479:39:21, 350.33s/it]                                                            5%|â–Œ         | 268/5197 [25:57:34<479:39:21, 350.33s/it]  5%|â–Œ         | 269/5197 [26:03:24<479:23:09, 350.20s/it]                                                            5%|â–Œ         | 269/5197 [26:03:24<479:23:09, 350.20s/it]  5%|â–Œ         | 270/5197 [26:09:18<480:46:16, 351.28s/it]                                  {'loss': 0.9592, 'learning_rate': 0.00019974773129671701, 'epoch': 0.05}
{'loss': 0.9592, 'learning_rate': 0.00019974773129671701, 'epoch': 0.05}
{'loss': 0.9414, 'learning_rate': 0.00019974328802158797, 'epoch': 0.05}
{'loss': 0.9414, 'learning_rate': 0.00019974328802158797, 'epoch': 0.05}
[2023-11-23 11:43:23,040] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9817, 'learning_rate': 0.0001997388060073152, 'epoch': 0.05}
{'loss': 0.9817, 'learning_rate': 0.0001997388060073152, 'epoch': 0.05}
{'loss': 0.9296, 'learning_rate': 0.00019973428525563947, 'epoch': 0.05}
{'loss': 0.9296, 'learning_rate': 0.00019973428525563947, 'epoch': 0.05}
{'loss': 0.9938, 'learning_rate': 0.00019972972576831656, 'epoch': 0.05}
{'loss': 0.9938, 'learning_rate': 0.00019972972576831656, 'epoch': 0.05}
350.20s/it]                                                            5%|â–Œ         | 269/5197 [26:02:57<479:23:02, 350.20s/it]  5%|â–Œ         | 270/5197 [26:08:51<480:46:07, 351.28s/it]                                                            5%|â–Œ         | 270/5197 [26:08:51<480:46:07, 351.28s/it]  5%|â–Œ         | 271/5197 [26:14:46<482:19:33, 352.49s/it]                                                            5%|â–Œ         | 271/5197 [26:14:46<482:19:33, 352.49s/it]  5%|â–Œ         | 272/5197 [26:20:28<478:03:01, 349.44s/it]                                                            5%|â–Œ         | 272/5197 [26:20:28<478:03:01, 349.44s/it]  5%|â–Œ         | 273/5197 [26:26:22<479:38:42, 350.67s/it]                                                            5%|â–Œ         | 273/5197 [26:26:22<479:38:42, 350.67s/it]  5%|â–Œ         | 274/5197 [26:32:13<479:41:50, 350.78s/it]                                                            5%|â–Œ         | 274/5197 [26:32:13<479:[2023-11-23 12:01:02,836] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9234, 'learning_rate': 0.0001997251275471174, 'epoch': 0.05}
{'loss': 0.9234, 'learning_rate': 0.0001997251275471174, 'epoch': 0.05}
                          5%|â–Œ         | 270/5197 [26:09:18<480:46:16, 351.28s/it]  5%|â–Œ         | 271/5197 [26:15:13<482:19:46, 352.49s/it]                                                            5%|â–Œ         | 271/5197 [26:15:13<482:19:46, 352.49s/it]  5%|â–Œ         | 272/5197 [26:20:56<478:02:52, 349.44s/it]                                                            5%|â–Œ         | 272/5197 [26:20:56<478:02:52, 349.44s/it]  5%|â–Œ         | 273/5197 [26:26:49<479:38:46, 350.68s/it]                                                            5%|â–Œ         | 273/5197 [26:26:49<479:38:46, 350.68s/it]  5%|â–Œ         | 274/5197 [26:32:40<479:41:57, 350.79s/it]                                                            5%|â–Œ         | 274/5197 [26:32:40<479:41:57, 350.79s/it]  5%|â–Œ         | 275/5197 [26:38:35<481:24:57, 352.11s/it]                                                            5%|â–Œ         | 275/5197 [26:38:36<481:24:57, 352.11s/it]  5%|â–Œ         | 276/5197{'loss': 0.9319, 'learning_rate': 0.00019972049059382782, 'epoch': 0.05}
{'loss': 0.9319, 'learning_rate': 0.00019972049059382782, 'epoch': 0.05}
{'loss': 0.9972, 'learning_rate': 0.00019971581491024873, 'epoch': 0.05}
{'loss': 0.9972, 'learning_rate': 0.00019971581491024873, 'epoch': 0.05}
{'loss': 0.9628, 'learning_rate': 0.0001997111004981962, 'epoch': 0.05}
{'loss': 0.9628, 'learning_rate': 0.0001997111004981962, 'epoch': 0.05}
{'loss': 0.9345, 'learning_rate': 0.00019970634735950115, 'epoch': 0.05}
{'loss': 0.9345, 'learning_rate': 0.00019970634735950115, 'epoch': 0.05}
{'loss': 1.0248, 'learning_rate': 0.00019970155549600978, 'epoch': 0.05}
41:50, 350.78s/it]  5%|â–Œ         | 275/5197 [26:38:08<481:24:30, 352.11s/it]                                                            5%|â–Œ         | 275/5197 [26:38:08<481:24:30, 352.11s/it]  5%|â–Œ         | 276/5197 [26:44:05<483:17:11, 353.55s/it]                                                            5%|â–Œ         | 276/5197 [26:44:05<483:17:11, 353.55s/it]  5%|â–Œ         | 277/5197 [26:49:55<481:52:44, 352.59s/it]                                                            5%|â–Œ         | 277/5197 [26:49:55<481:52:44, 352.59s/it]  5%|â–Œ         | 278/5197 [26:55:51<482:54:24, 353.42s/it]                                                            5%|â–Œ         | 278/5197 [26:55:51<482:54:24, 353.42s/it]  5%|â–Œ         | 279/5197 [27:01:51<485:37:37, 355.48s/it]                                                            5%|â–Œ         | 279/5197 [27:01:51<485:37:37, 355.48s/it]  5%|â–Œ         | 280/5197 [27:08:07<494:05:25, 361.75s/it]                                  {'loss': 1.0248, 'learning_rate': 0.00019970155549600978, 'epoch': 0.05}
{'loss': 1.0023, 'learning_rate': 0.00019969672490958304, 'epoch': 0.05}
{'loss': 1.0023, 'learning_rate': 0.00019969672490958304, 'epoch': 0.05}
 [26:44:32<483:17:38, 353.56s/it]                                                            5%|â–Œ         | 276/5197 [26:44:32<483:17:38, 353.56s/it]  5%|â–Œ         | 277/5197 [26:50:23<481:53:15, 352.60s/it]                                                            5%|â–Œ         | 277/5197 [26:50:23<481:53:15, 352.60s/it]  5%|â–Œ         | 278/5197 [26:56:18<482:54:59, 353.43s/it]                                                            5%|â–Œ         | 278/5197 [26:56:18<482:54:59, 353.43s/it]  5%|â–Œ         | 279/5197 [27:02:18<485:38:04, 355.49s/it]                                                            5%|â–Œ         | 279/5197 [27:02:18<485:38:04, 355.49s/it]  5%|â–Œ         | 280/5197 [27:08:35<494:05:23, 361.75s/it]                                                            5%|â–Œ         | 280/5197 [27:08:35<494:05:23, 361.75s/it]  5%|â–Œ         | 281/5197 [27:14:31<491:54:39, 360.23s/it]                                                            5%|â–Œ         | 2{'loss': 1.0168, 'learning_rate': 0.0001996918556020972, 'epoch': 0.05}
{'loss': 1.0168, 'learning_rate': 0.0001996918556020972, 'epoch': 0.05}
{'loss': 0.9951, 'learning_rate': 0.0001996869475754434, 'epoch': 0.05}
{'loss': 0.9951, 'learning_rate': 0.0001996869475754434, 'epoch': 0.05}
{'loss': 0.907, 'learning_rate': 0.00019968200083152782, 'epoch': 0.05}
{'loss': 0.907, 'learning_rate': 0.00019968200083152782, 'epoch': 0.05}
{'loss': 0.9387, 'learning_rate': 0.00019967701537227175, 'epoch': 0.05}
{'loss': 0.9387, 'learning_rate': 0.00019967701537227175, 'epoch': 0.05}
                          5%|â–Œ         | 280/5197 [27:08:07<494:05:25, 361.75s/it]  5%|â–Œ         | 281/5197 [27:14:04<491:54:46, 360.23s/it]                                                            5%|â–Œ         | 281/5197 [27:14:04<491:54:46, 360.23s/it]  5%|â–Œ         | 282/5197 [27:20:28<501:42:34, 367.48s/it]                                                            5%|â–Œ         | 282/5197 [27:20:28<501:42:34, 367.48s/it]  5%|â–Œ         | 283/5197 [27:26:50<507:30:46, 371.80s/it]                                                            5%|â–Œ         | 283/5197 [27:26:50<507:30:46, 371.80s/it]  5%|â–Œ         | 284/5197 [27:33:08<509:50:16, 373.58s/it]                                                            5%|â–Œ         | 284/5197 [27:33:08<509:50:16, 373.58s/it]  5%|â–Œ         | 285/5197 [27:39:06<503:07:18, 368.74s/it]                                                            5%|â–Œ         | 285/5197 [27:39:06<503:07:18, 368.74s/it]  6%|â–Œ         | 286/5197{'loss': 0.932, 'learning_rate': 0.00019967199119961152, 'epoch': 0.06}
{'loss': 0.932, 'learning_rate': 0.00019967199119961152, 'epoch': 0.06}
[2023-11-23 13:14:42,840] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9653, 'learning_rate': 0.0001996669283154984, 'epoch': 0.06}
{'loss': 0.9653, 'learning_rate': 0.0001996669283154984, 'epoch': 0.06}
81/5197 [27:14:31<491:54:39, 360.23s/it]  5%|â–Œ         | 282/5197 [27:20:56<501:42:29, 367.48s/it]                                                            5%|â–Œ         | 282/5197 [27:20:56<501:42:29, 367.48s/it]  5%|â–Œ         | 283/5197 [27:27:18<507:30:18, 371.80s/it]                                                            5%|â–Œ         | 283/5197 [27:27:18<507:30:18, 371.80s/it]  5%|â–Œ         | 284/5197 [27:33:35<509:49:45, 373.58s/it]                                                            5%|â–Œ         | 284/5197 [27:33:35<509:49:45, 373.58s/it]  5%|â–Œ         | 285/5197 [27:39:33<503:06:54, 368.73s/it]                                                            5%|â–Œ         | 285/5197 [27:39:33<503:06:54, 368.73s/it]  6%|â–Œ         | 286/5197 [27:45:51<506:57:56, 371.63s/it]                                                            6%|â–Œ         | 286/5197 [27:45:51<506:57:56, 371.63s/it]  6%|â–Œ         | 287/5197 [27:52:15<512:00:11, 375.40s/it]            [2023-11-23 13:21:06,623] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0479, 'learning_rate': 0.0001996618267218988, 'epoch': 0.06}
{'loss': 1.0479, 'learning_rate': 0.0001996618267218988, 'epoch': 0.06}
{'loss': 0.9448, 'learning_rate': 0.00019965668642079408, 'epoch': 0.06}
{'loss': 0.9448, 'learning_rate': 0.00019965668642079408, 'epoch': 0.06}
{'loss': 0.9666, 'learning_rate': 0.00019965150741418073, 'epoch': 0.06}
{'loss': 0.9666, 'learning_rate': 0.00019965150741418073, 'epoch': 0.06}
{'loss': 0.9514, 'learning_rate': 0.0001996462897040702, 'epoch': 0.06}
{'loss': 0.9514, 'learning_rate': 0.0001996462897040702, 'epoch': 0.06}
 [27:45:24<506:58:06, 371.63s/it]                                                            6%|â–Œ         | 286/5197 [27:45:24<506:58:06, 371.63s/it]  6%|â–Œ         | 287/5197 [27:51:48<511:59:53, 375.40s/it]                                                            6%|â–Œ         | 287/5197 [27:51:48<511:59:53, 375.40s/it]  6%|â–Œ         | 288/5197 [27:58:12<515:19:30, 377.91s/it]                                                            6%|â–Œ         | 288/5197 [27:58:12<515:19:30, 377.91s/it]  6%|â–Œ         | 289/5197 [28:04:41<519:41:13, 381.19s/it]                                                            6%|â–Œ         | 289/5197 [28:04:41<519:41:13, 381.19s/it]  6%|â–Œ         | 290/5197 [28:10:28<505:52:03, 371.13s/it]                                                            6%|â–Œ         | 290/5197 [28:10:28<505:52:03, 371.13s/it]  6%|â–Œ         | 291/5197 [28:15:59<489:20:54, 359.08s/it]                                                            6%|â–Œ         | 2{'loss': 0.9374, 'learning_rate': 0.0001996410332924889, 'epoch': 0.06}
{'loss': 0.9374, 'learning_rate': 0.0001996410332924889, 'epoch': 0.06}
                                                6%|â–Œ         | 287/5197 [27:52:15<512:00:11, 375.40s/it]  6%|â–Œ         | 288/5197 [27:58:39<515:19:26, 377.91s/it]                                                            6%|â–Œ         | 288/5197 [27:58:39<515:19:26, 377.91s/it]  6%|â–Œ         | 289/5197 [28:05:08<519:40:41, 381.18s/it]                                                            6%|â–Œ         | 289/5197 [28:05:08<519:40:41, 381.18s/it]  6%|â–Œ         | 290/5197 [28:10:56<505:52:06, 371.13s/it]                                                            6%|â–Œ         | 290/5197 [28:10:56<505:52:06, 371.13s/it]  6%|â–Œ         | 291/5197 [28:16:27<489:20:54, 359.08s/it]                                                            6%|â–Œ         | 291/5197 [28:16:27<489:20:54, 359.08s/it]  6%|â–Œ         | 292/5197 [28:21:47<473:26:58, 347.49s/it]                                                            6%|â–Œ         | 292/5197 [28:21:47<473:26:58, 347.49s/it]  6%|{'loss': 1.0368, 'learning_rate': 0.0001996357381814785, 'epoch': 0.06}
{'loss': 1.0368, 'learning_rate': 0.0001996357381814785, 'epoch': 0.06}
{'loss': 0.9563, 'learning_rate': 0.00019963040437309549, 'epoch': 0.06}
{'loss': 0.9563, 'learning_rate': 0.00019963040437309549, 'epoch': 0.06}
{'loss': 1.0507, 'learning_rate': 0.00019962503186941142, 'epoch': 0.06}
{'loss': 1.0507, 'learning_rate': 0.00019962503186941142, 'epoch': 0.06}
{'loss': 0.8927, 'learning_rate': 0.00019961962067251298, 'epoch': 0.06}
{'loss': 0.8927, 'learning_rate': 0.00019961962067251298, 'epoch': 0.06}
[2023-11-23 14:15:07,289] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3203, 'learning_rate': 0.00019961417078450178, 'epoch': 0.06}
91/5197 [28:15:59<489:20:54, 359.08s/it]  6%|â–Œ         | 292/5197 [28:21:20<473:26:47, 347.48s/it]                                                            6%|â–Œ         | 292/5197 [28:21:20<473:26:47, 347.48s/it]  6%|â–Œ         | 293/5197 [28:27:29<482:04:23, 353.89s/it]                                                            6%|â–Œ         | 293/5197 [28:27:29<482:04:23, 353.89s/it]  6%|â–Œ         | 294/5197 [28:33:51<493:27:22, 362.32s/it]                                                            6%|â–Œ         | 294/5197 [28:33:51<493:27:22, 362.32s/it]  6%|â–Œ         | 295/5197 [28:40:12<501:02:36, 367.96s/it]                                                            6%|â–Œ         | 295/5197 [28:40:12<501:02:36, 367.96s/it]  6%|â–Œ         | 296/5197 [28:46:14<498:44:30, 366.35s/it]                                                            6%|â–Œ         | 296/5197 [28:46:14<498:44:30, 366.35s/it]  6%|â–Œ         | 297/5197 [28:52:13<495:20:26, 363.92s/it]            {'loss': 0.3203, 'learning_rate': 0.00019961417078450178, 'epoch': 0.06}
â–Œ         | 293/5197 [28:27:56<482:04:31, 353.89s/it]                                                            6%|â–Œ         | 293/5197 [28:27:56<482:04:31, 353.89s/it]  6%|â–Œ         | 294/5197 [28:34:18<493:27:47, 362.32s/it]                                                            6%|â–Œ         | 294/5197 [28:34:18<493:27:47, 362.32s/it]  6%|â–Œ         | 295/5197 [28:40:39<501:02:49, 367.97s/it]                                                            6%|â–Œ         | 295/5197 [28:40:39<501:02:49, 367.97s/it]  6%|â–Œ         | 296/5197 [28:46:42<498:44:49, 366.35s/it]                                                            6%|â–Œ         | 296/5197 [28:46:42<498:44:49, 366.35s/it]  6%|â–Œ         | 297/5197 [28:52:40<495:20:15, 363.92s/it]                                                            6%|â–Œ         | 297/5197 [28:52:40<495:20:15, 363.92s/it]  6%|â–Œ         | 298/5197 [28:58:02<478:16:54, 351.46s/it]                                                          {'loss': 0.9395, 'learning_rate': 0.00019960868220749448, 'epoch': 0.06}
{'loss': 0.9395, 'learning_rate': 0.00019960868220749448, 'epoch': 0.06}
{'loss': 0.9687, 'learning_rate': 0.00019960315494362284, 'epoch': 0.06}
{'loss': 0.9687, 'learning_rate': 0.00019960315494362284, 'epoch': 0.06}
{'loss': 0.9273, 'learning_rate': 0.00019959758899503353, 'epoch': 0.06}
{'loss': 0.9273, 'learning_rate': 0.00019959758899503353, 'epoch': 0.06}
{'loss': 1.018, 'learning_rate': 0.0001995919843638883, 'epoch': 0.06}
{'loss': 1.018, 'learning_rate': 0.0001995919843638883, 'epoch': 0.06}
{'loss': 0.9916, 'learning_rate': 0.00019958634105236395, 'epoch': 0.06}
{'loss': 0.9916, 'learning_rate': 0.00019958634105236395, 'epoch': 0.06}
                                                6%|â–Œ         | 297/5197 [28:52:13<495:20:26, 363.92s/it]  6%|â–Œ         | 298/5197 [28:57:35<478:16:39, 351.46s/it]                                                            6%|â–Œ         | 298/5197 [28:57:35<478:16:39, 351.46s/it]  6%|â–Œ         | 299/5197 [29:03:32<480:36:23, 353.24s/it]                                                            6%|â–Œ         | 299/5197 [29:03:32<480:36:23, 353.24s/it]  6%|â–Œ         | 300/5197 [29:09:48<489:49:41, 360.09s/it]                                                            6%|â–Œ         | 300/5197 [29:09:48<489:49:41, 360.09s/it]  6%|â–Œ         | 301/5197 [29:16:11<499:04:07, 366.96s/it]                                                            6%|â–Œ         | 301/5197 [29:16:11<499:04:07, 366.96s/it]  6%|â–Œ         | 302/5197 [29:22:34<505:13:27, 371.56s/it]                                                            6%|â–Œ         | 302/5197 [29:22:34<505:13:27, 371.56s/it]  6%|{'loss': 0.9282, 'learning_rate': 0.00019958065906265228, 'epoch': 0.06}
{'loss': 0.9282, 'learning_rate': 0.00019958065906265228, 'epoch': 0.06}
  6%|â–Œ         | 298/5197 [28:58:02<478:16:54, 351.46s/it]  6%|â–Œ         | 299/5197 [29:04:00<480:36:50, 353.25s/it]                                                            6%|â–Œ         | 299/5197 [29:04:00<480:36:50, 353.25s/it]  6%|â–Œ         | 300/5197 [29:10:16<489:50:14, 360.10s/it]                                                            6%|â–Œ         | 300/5197 [29:10:16<489:50:14, 360.10s/it]  6%|â–Œ         | 301/5197 [29:16:39<499:04:44, 366.97s/it]                                                            6%|â–Œ         | 301/5197 [29:16:39<499:04:44, 366.97s/it]  6%|â–Œ         | 302/5197 [29:23:01<505:13:47, 371.57s/it]                                                            6%|â–Œ         | 302/5197 [29:23:01<505:13:47, 371.57s/it]  6%|â–Œ         | 303/5197 [29:29:12<504:56:21, 371.43s/it]                                                            6%|â–Œ         | 303/5197 [29:29:12<504:56:21, 371.43s/it]  6%|â–Œ         | 304/5197 [29:35:29<506:49:59, 37{'loss': 1.0186, 'learning_rate': 0.0001995749383969601, 'epoch': 0.06}
{'loss': 1.0186, 'learning_rate': 0.0001995749383969601, 'epoch': 0.06}
{'loss': 0.8968, 'learning_rate': 0.00019956917905750924, 'epoch': 0.06}
{'loss': 0.8968, 'learning_rate': 0.00019956917905750924, 'epoch': 0.06}
{'loss': 0.9616, 'learning_rate': 0.00019956338104653657, 'epoch': 0.06}
{'loss': 0.9616, 'learning_rate': 0.00019956338104653657, 'epoch': 0.06}
{'loss': 1.0051, 'learning_rate': 0.00019955754436629399, 'epoch': 0.06}
{'loss': 1.0051, 'learning_rate': 0.00019955754436629399, 'epoch': 0.06}
â–Œ         | 303/5197 [29:28:45<504:56:08, 371.43s/it]                                                            6%|â–Œ         | 303/5197 [29:28:45<504:56:08, 371.43s/it]  6%|â–Œ         | 304/5197 [29:35:01<506:49:38, 372.90s/it]                                                            6%|â–Œ         | 304/5197 [29:35:01<506:49:38, 372.90s/it]  6%|â–Œ         | 305/5197 [29:40:49<496:37:43, 365.47s/it]                                                            6%|â–Œ         | 305/5197 [29:40:49<496:37:43, 365.47s/it]  6%|â–Œ         | 306/5197 [29:46:27<485:20:26, 357.23s/it]                                                            6%|â–Œ         | 306/5197 [29:46:27<485:20:26, 357.23s/it]  6%|â–Œ         | 307/5197 [29:52:26<485:43:43, 357.59s/it]                                                            6%|â–Œ         | 307/5197 [29:52:26<485:43:43, 357.59s/it]  6%|â–Œ         | 308/5197 [29:58:23<485:25:09, 357.44s/it]                                                          {'loss': 0.9392, 'learning_rate': 0.00019955166901904837, 'epoch': 0.06}
{'loss': 0.9392, 'learning_rate': 0.00019955166901904837, 'epoch': 0.06}
{'loss': 1.0096, 'learning_rate': 0.00019954575500708162, 'epoch': 0.06}
{'loss': 1.0096, 'learning_rate': 0.00019954575500708162, 'epoch': 0.06}
2.90s/it]                                                            6%|â–Œ         | 304/5197 [29:35:29<506:49:59, 372.90s/it]  6%|â–Œ         | 305/5197 [29:41:17<496:37:34, 365.46s/it]                                                            6%|â–Œ         | 305/5197 [29:41:17<496:37:34, 365.46s/it]  6%|â–Œ         | 306/5197 [29:46:55<485:20:19, 357.23s/it]                                                            6%|â–Œ         | 306/5197 [29:46:55<485:20:19, 357.23s/it]  6%|â–Œ         | 307/5197 [29:52:53<485:43:41, 357.59s/it]                                                            6%|â–Œ         | 307/5197 [29:52:53<485:43:41, 357.59s/it]  6%|â–Œ         | 308/5197 [29:58:50<485:24:56, 357.43s/it]                                                            6%|â–Œ         | 308/5197 [29:58:50<485:24:56, 357.43s/it]  6%|â–Œ         | 309/5197 [30:05:01<490:50:13, 361.50s/it]                                                            6%|â–Œ         | 309/5197 [30:05:01<490:50{'loss': 1.0329, 'learning_rate': 0.0001995398023326907, 'epoch': 0.06}
{'loss': 1.0329, 'learning_rate': 0.0001995398023326907, 'epoch': 0.06}
{'loss': 0.9319, 'learning_rate': 0.00019953381099818755, 'epoch': 0.06}
{'loss': 0.9319, 'learning_rate': 0.00019953381099818755, 'epoch': 0.06}
{'loss': 0.9634, 'learning_rate': 0.00019952778100589913, 'epoch': 0.06}
{'loss': 0.9634, 'learning_rate': 0.00019952778100589913, 'epoch': 0.06}
{'loss': 0.9745, 'learning_rate': 0.00019952171235816747, 'epoch': 0.06}
{'loss': 0.9745, 'learning_rate': 0.00019952171235816747, 'epoch': 0.06}
  6%|â–Œ         | 308/5197 [29:58:23<485:25:09, 357.44s/it]  6%|â–Œ         | 309/5197 [30:04:34<490:50:26, 361.50s/it]                                                            6%|â–Œ         | 309/5197 [30:04:34<490:50:26, 361.50s/it]  6%|â–Œ         | 310/5197 [30:10:39<492:19:15, 362.67s/it]                                                            6%|â–Œ         | 310/5197 [30:10:39<492:19:15, 362.67s/it]  6%|â–Œ         | 311/5197 [30:17:01<500:04:56, 368.46s/it]                                                            6%|â–Œ         | 311/5197 [30:17:01<500:04:56, 368.46s/it]  6%|â–Œ         | 312/5197 [30:23:01<496:19:59, 365.77s/it]                                                            6%|â–Œ         | 312/5197 [30:23:01<496:19:59, 365.77s/it]  6%|â–Œ         | 313/5197 [30:29:22<502:44:33, 370.57s/it]                                                            6%|â–Œ         | 313/5197 [30:29:22<502:44:33, 370.57s/it]  6%|â–Œ         | 314/5197 [30:35:15<495:15:51, 36{'loss': 0.966, 'learning_rate': 0.00019951560505734945, 'epoch': 0.06}
{'loss': 0.966, 'learning_rate': 0.00019951560505734945, 'epoch': 0.06}
{'loss': 0.9017, 'learning_rate': 0.00019950945910581717, 'epoch': 0.06}
:13, 361.50s/it]  6%|â–Œ         | 310/5197 [30:11:07<492:19:06, 362.67s/it]                                                            6%|â–Œ         | 310/5197 [30:11:07<492:19:06, 362.67s/it]  6%|â–Œ         | 311/5197 [30:17:29<500:04:03, 368.45s/it]                                                            6%|â–Œ         | 311/5197 [30:17:29<500:04:03, 368.45s/it]  6%|â–Œ         | 312/5197 [30:23:28<496:19:09, 365.76s/it]                                                            6%|â–Œ         | 312/5197 [30:23:28<496:19:09, 365.76s/it]  6%|â–Œ         | 313/5197 [30:29:50<502:43:47, 370.56s/it]                                                            6%|â–Œ         | 313/5197 [30:29:50<502:43:47, 370.56s/it]  6%|â–Œ         | 314/5197 [30:35:42<495:15:32, 365.13s/it]                                                            6%|â–Œ         | 314/5197 [30:35:42<495:15:32, 365.13s/it]  6%|â–Œ         | 315/5197 [30:41:31<488:21:02, 360.11s/it]                                    {'loss': 0.9017, 'learning_rate': 0.00019950945910581717, 'epoch': 0.06}
{'loss': 0.9893, 'learning_rate': 0.00019950327450595764, 'epoch': 0.06}
{'loss': 0.9893, 'learning_rate': 0.00019950327450595764, 'epoch': 0.06}
{'loss': 0.9636, 'learning_rate': 0.00019949705126017287, 'epoch': 0.06}
{'loss': 0.9636, 'learning_rate': 0.00019949705126017287, 'epoch': 0.06}
{'loss': 1.0472, 'learning_rate': 0.00019949078937087986, 'epoch': 0.06}
{'loss': 1.0472, 'learning_rate': 0.00019949078937087986, 'epoch': 0.06}
{'loss': 0.9527, 'learning_rate': 0.0001994844888405107, 'epoch': 0.06}
{'loss': 0.9527, 'learning_rate': 0.0001994844888405107, 'epoch': 0.06}
5.13s/it]                                                            6%|â–Œ         | 314/5197 [30:35:15<495:15:51, 365.13s/it]  6%|â–Œ         | 315/5197 [30:41:03<488:21:25, 360.12s/it]                                                            6%|â–Œ         | 315/5197 [30:41:03<488:21:25, 360.12s/it]  6%|â–Œ         | 316/5197 [30:47:09<490:43:31, 361.94s/it]                                                            6%|â–Œ         | 316/5197 [30:47:09<490:43:31, 361.94s/it]  6%|â–Œ         | 317/5197 [30:53:33<499:19:38, 368.36s/it]                                                            6%|â–Œ         | 317/5197 [30:53:33<499:19:38, 368.36s/it]  6%|â–Œ         | 318/5197 [30:59:56<505:15:18, 372.81s/it]                                                            6%|â–Œ         | 318/5197 [30:59:56<505:15:18, 372.81s/it]  6%|â–Œ         | 319/5197 [31:05:54<499:05:10, 368.33s/it]                                                            6%|â–Œ         | 319/5197 [31:05:54<499:05{'loss': 1.0108, 'learning_rate': 0.00019947814967151244, 'epoch': 0.06}
{'loss': 1.0108, 'learning_rate': 0.00019947814967151244, 'epoch': 0.06}
                        6%|â–Œ         | 315/5197 [30:41:31<488:21:02, 360.11s/it]  6%|â–Œ         | 316/5197 [30:47:37<490:43:27, 361.94s/it]                                                            6%|â–Œ         | 316/5197 [30:47:37<490:43:27, 361.94s/it]  6%|â–Œ         | 317/5197 [30:54:00<499:19:58, 368.36s/it]                                                            6%|â–Œ         | 317/5197 [30:54:00<499:19:58, 368.36s/it]  6%|â–Œ         | 318/5197 [31:00:23<505:15:22, 372.81s/it]                                                            6%|â–Œ         | 318/5197 [31:00:23<505:15:22, 372.81s/it]  6%|â–Œ         | 319/5197 [31:06:21<499:05:46, 368.34s/it]                                                            6%|â–Œ         | 319/5197 [31:06:21<499:05:46, 368.34s/it]  6%|â–Œ         | 320/5197 [31:12:41<503:25:44, 371.61s/it]                                                            6%|â–Œ         | 320/5197 [31:12:41<503:25:44, 371.61s/it]  6%|â–Œ         | 321/5197 [{'loss': 0.9635, 'learning_rate': 0.00019947177186634715, 'epoch': 0.06}
{'loss': 0.9635, 'learning_rate': 0.00019947177186634715, 'epoch': 0.06}
[2023-11-23 16:47:34,558] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2948, 'learning_rate': 0.00019946535542749184, 'epoch': 0.06}
{'loss': 0.2948, 'learning_rate': 0.00019946535542749184, 'epoch': 0.06}
{'loss': 0.981, 'learning_rate': 0.00019945890035743866, 'epoch': 0.06}
{'loss': 0.981, 'learning_rate': 0.00019945890035743866, 'epoch': 0.06}
{'loss': 1.0138, 'learning_rate': 0.00019945240665869465, 'epoch': 0.06}
{'loss': 1.0138, 'learning_rate': 0.00019945240665869465, 'epoch': 0.06}
{'loss': 0.9448, 'learning_rate': 0.00019944587433378186, 'epoch': 0.06}
:10, 368.33s/it]  6%|â–Œ         | 320/5197 [31:12:13<503:25:22, 371.61s/it]                                                            6%|â–Œ         | 320/5197 [31:12:13<503:25:22, 371.61s/it]  6%|â–Œ         | 321/5197 [31:18:27<504:14:07, 372.28s/it]                                                            6%|â–Œ         | 321/5197 [31:18:27<504:14:07, 372.28s/it]  6%|â–Œ         | 322/5197 [31:24:40<504:21:26, 372.45s/it]                                                            6%|â–Œ         | 322/5197 [31:24:40<504:21:26, 372.45s/it]  6%|â–Œ         | 323/5197 [31:30:52<504:21:14, 372.52s/it]                                                            6%|â–Œ         | 323/5197 [31:30:52<504:21:14, 372.52s/it]  6%|â–Œ         | 324/5197 [31:36:54<499:40:26, 369.14s/it]                                                            6%|â–Œ         | 324/5197 [31:36:54<499:40:26, 369.14s/it]  6%|â–‹         | 325/5197 [31:42:48<493:38:43, 364.76s/it]                                    {'loss': 0.9448, 'learning_rate': 0.00019944587433378186, 'epoch': 0.06}
{'loss': 0.8923, 'learning_rate': 0.0001994393033852374, 'epoch': 0.06}
{'loss': 0.8923, 'learning_rate': 0.0001994393033852374, 'epoch': 0.06}
31:18:54<504:14:36, 372.29s/it]                                                            6%|â–Œ         | 321/5197 [31:18:54<504:14:36, 372.29s/it]  6%|â–Œ         | 322/5197 [31:25:07<504:21:34, 372.45s/it]                                                            6%|â–Œ         | 322/5197 [31:25:07<504:21:34, 372.45s/it]  6%|â–Œ         | 323/5197 [31:31:20<504:21:33, 372.53s/it]                                                            6%|â–Œ         | 323/5197 [31:31:20<504:21:33, 372.53s/it]  6%|â–Œ         | 324/5197 [31:37:21<499:40:27, 369.14s/it]                                                            6%|â–Œ         | 324/5197 [31:37:21<499:40:27, 369.14s/it]  6%|â–‹         | 325/5197 [31:43:16<493:38:34, 364.76s/it]                                                            6%|â–‹         | 325/5197 [31:43:16<493:38:34, 364.76s/it]  6%|â–‹         | 326/5197 [31:48:45<479:20:10, 354.26s/it]                                                            6%|â–‹         | 326{'loss': 1.044, 'learning_rate': 0.00019943269381561334, 'epoch': 0.06}
{'loss': 1.044, 'learning_rate': 0.00019943269381561334, 'epoch': 0.06}
{'loss': 0.9255, 'learning_rate': 0.00019942604562747678, 'epoch': 0.06}
{'loss': 0.9255, 'learning_rate': 0.00019942604562747678, 'epoch': 0.06}
{'loss': 0.9509, 'learning_rate': 0.00019941935882340976, 'epoch': 0.06}
{'loss': 0.9509, 'learning_rate': 0.00019941935882340976, 'epoch': 0.06}
{'loss': 0.9107, 'learning_rate': 0.00019941263340600939, 'epoch': 0.06}
{'loss': 0.9107, 'learning_rate': 0.00019941263340600939, 'epoch': 0.06}
                        6%|â–‹         | 325/5197 [31:42:48<493:38:43, 364.76s/it]  6%|â–‹         | 326/5197 [31:48:18<479:20:23, 354.26s/it]                                                            6%|â–‹         | 326/5197 [31:48:18<479:20:23, 354.26s/it]  6%|â–‹         | 327/5197 [31:54:28<485:37:17, 358.98s/it]                                                            6%|â–‹         | 327/5197 [31:54:28<485:37:17, 358.98s/it]  6%|â–‹         | 328/5197 [32:00:28<486:03:09, 359.37s/it]                                                            6%|â–‹         | 328/5197 [32:00:28<486:03:09, 359.37s/it]  6%|â–‹         | 329/5197 [32:06:44<492:39:48, 364.34s/it]                                                            6%|â–‹         | 329/5197 [32:06:44<492:39:48, 364.34s/it]  6%|â–‹         | 330/5197 [32:12:58<496:34:13, 367.30s/it]                                                            6%|â–‹         | 330/5197 [32:12:58<496:34:13, 367.30s/it]  6%|â–‹         | 331/5197 [{'loss': 0.9837, 'learning_rate': 0.00019940586937788776, 'epoch': 0.06}
{'loss': 0.9837, 'learning_rate': 0.00019940586937788776, 'epoch': 0.06}
{'loss': 0.9253, 'learning_rate': 0.0001993990667416719, 'epoch': 0.06}
{'loss': 0.9253, 'learning_rate': 0.0001993990667416719, 'epoch': 0.06}
/5197 [31:48:45<479:20:10, 354.26s/it]  6%|â–‹         | 327/5197 [31:54:55<485:36:55, 358.98s/it]                                                            6%|â–‹         | 327/5197 [31:54:55<485:36:55, 358.98s/it]  6%|â–‹         | 328/5197 [32:00:56<486:03:24, 359.38s/it]                                                            6%|â–‹         | 328/5197 [32:00:56<486:03:24, 359.38s/it]  6%|â–‹         | 329/5197 [32:07:12<492:39:25, 364.33s/it]                                                            6%|â–‹         | 329/5197 [32:07:12<492:39:25, 364.33s/it]  6%|â–‹         | 330/5197 [32:13:26<496:34:14, 367.30s/it]                                                            6%|â–‹         | 330/5197 [32:13:26<496:34:14, 367.30s/it]  6%|â–‹         | 331/5197 [32:19:36<497:48:36, 368.29s/it]                                                            6%|â–‹         | 331/5197 [32:19:37<497:48:36, 368.29s/it]  6%|â–‹         | 332/5197 [32:25:58<503:11:23, 372.35s/it]              {'loss': 0.981, 'learning_rate': 0.0001993922255000039, 'epoch': 0.06}
{'loss': 0.981, 'learning_rate': 0.0001993922255000039, 'epoch': 0.06}
[2023-11-23 18:00:59,849] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9979, 'learning_rate': 0.0001993853456555408, 'epoch': 0.06}
{'loss': 0.9979, 'learning_rate': 0.0001993853456555408, 'epoch': 0.06}
{'loss': 0.9469, 'learning_rate': 0.00019937842721095468, 'epoch': 0.06}
{'loss': 0.9469, 'learning_rate': 0.00019937842721095468, 'epoch': 0.06}
{'loss': 1.0084, 'learning_rate': 0.00019937147016893257, 'epoch': 0.06}
{'loss': 1.0084, 'learning_rate': 0.00019937147016893257, 'epoch': 0.06}
32:19:09<497:48:36, 368.29s/it]                                                            6%|â–‹         | 331/5197 [32:19:09<497:48:36, 368.29s/it]  6%|â–‹         | 332/5197 [32:25:31<503:11:24, 372.35s/it]                                                            6%|â–‹         | 332/5197 [32:25:31<503:11:24, 372.35s/it]  6%|â–‹         | 333/5197 [32:31:45<503:46:46, 372.86s/it]                                                            6%|â–‹         | 333/5197 [32:31:45<503:46:46, 372.86s/it]  6%|â–‹         | 334/5197 [32:38:05<506:37:38, 375.05s/it]                                                            6%|â–‹         | 334/5197 [32:38:05<506:37:38, 375.05s/it]  6%|â–‹         | 335/5197 [32:44:16<504:43:35, 373.72s/it]                                                            6%|â–‹         | 335/5197 [32:44:16<504:43:35, 373.72s/it]  6%|â–‹         | 336/5197 [32:50:39<508:36:23, 376.67s/it]                                                            6%|â–‹         | 336{'loss': 0.9617, 'learning_rate': 0.00019936447453217646, 'epoch': 0.06}
{'loss': 0.9617, 'learning_rate': 0.00019936447453217646, 'epoch': 0.06}
[2023-11-23 18:25:57,170] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
                                              6%|â–‹         | 332/5197 [32:25:58<503:11:23, 372.35s/it]  6%|â–‹         | 333/5197 [32:32:12<503:46:58, 372.87s/it]                                                            6%|â–‹         | 333/5197 [32:32:12<503:46:58, 372.87s/it]  6%|â–‹         | 334/5197 [32:38:32<506:37:23, 375.04s/it]                                                            6%|â–‹         | 334/5197 [32:38:33<506:37:23, 375.04s/it]  6%|â–‹         | 335/5197 [32:44:43<504:43:47, 373.72s/it]                                                            6%|â–‹         | 335/5197 [32:44:43<504:43:47, 373.72s/it]  6%|â–‹         | 336/5197 [32:51:07<508:36:35, 376.67s/it]                                                            6%|â–‹         | 336/5197 [32:51:07<508:36:35, 376.67s/it]  6%|â–‹         | 337/5197 [32:57:30<511:07:18, 378.61s/it]                                                            6%|â–‹         | 337/5197 [32:57:30<511:07:18, 378.61s/it]  7%|â–{'loss': 0.94, 'learning_rate': 0.00019935744030340346, 'epoch': 0.07}
{'loss': 0.94, 'learning_rate': 0.00019935744030340346, 'epoch': 0.07}
{'loss': 0.9498, 'learning_rate': 0.00019935036748534553, 'epoch': 0.07}
{'loss': 0.9498, 'learning_rate': 0.00019935036748534553, 'epoch': 0.07}
{'loss': 0.9458, 'learning_rate': 0.0001993432560807497, 'epoch': 0.07}
{'loss': 0.9458, 'learning_rate': 0.0001993432560807497, 'epoch': 0.07}
{'loss': 0.9879, 'learning_rate': 0.00019933610609237793, 'epoch': 0.07}
{'loss': 0.9879, 'learning_rate': 0.00019933610609237793, 'epoch': 0.07}
{'loss': 0.884, 'learning_rate': 0.00019932891752300717, 'epoch': 0.07}
/5197 [32:50:39<508:36:23, 376.67s/it]  6%|â–‹         | 337/5197 [32:57:02<511:07:12, 378.61s/it]                                                            6%|â–‹         | 337/5197 [32:57:02<511:07:12, 378.61s/it]  7%|â–‹         | 338/5197 [33:03:02<503:29:01, 373.03s/it]                                                            7%|â–‹         | 338/5197 [33:03:02<503:29:01, 373.03s/it]  7%|â–‹         | 339/5197 [33:09:28<508:32:38, 376.85s/it]                                                            7%|â–‹         | 339/5197 [33:09:28<508:32:38, 376.85s/it]  7%|â–‹         | 340/5197 [33:15:08<493:32:03, 365.81s/it]                                                            7%|â–‹         | 340/5197 [33:15:08<493:32:03, 365.81s/it]  7%|â–‹         | 341/5197 [33:21:23<497:11:30, 368.59s/it]                                                            7%|â–‹         | 341/5197 [33:21:23<497:11:30, 368.59s/it]  7%|â–‹         | 342/5197 [33:27:15<490:07:14, 363.43s/it]              {'loss': 0.884, 'learning_rate': 0.00019932891752300717, 'epoch': 0.07}
{'loss': 1.0226, 'learning_rate': 0.00019932169037542946, 'epoch': 0.07}
{'loss': 1.0226, 'learning_rate': 0.00019932169037542946, 'epoch': 0.07}
‹         | 338/5197 [33:03:30<503:29:19, 373.03s/it]                                                            7%|â–‹         | 338/5197 [33:03:30<503:29:19, 373.03s/it]  7%|â–‹         | 339/5197 [33:09:56<508:32:39, 376.85s/it]                                                            7%|â–‹         | 339/5197 [33:09:56<508:32:39, 376.85s/it]  7%|â–‹         | 340/5197 [33:15:36<493:32:00, 365.81s/it]                                                            7%|â–‹         | 340/5197 [33:15:36<493:32:00, 365.81s/it]  7%|â–‹         | 341/5197 [33:21:51<497:11:36, 368.59s/it]                                                            7%|â–‹         | 341/5197 [33:21:51<497:11:36, 368.59s/it]  7%|â–‹         | 342/5197 [33:27:42<490:06:53, 363.42s/it]                                                            7%|â–‹         | 342/5197 [33:27:42<490:06:53, 363.42s/it]  7%|â–‹         | 343/5197 [33:33:56<494:07:59, 366.48s/it]                                                          {'loss': 0.9726, 'learning_rate': 0.00019931442465245165, 'epoch': 0.07}
{'loss': 0.9726, 'learning_rate': 0.00019931442465245165, 'epoch': 0.07}
{'loss': 0.9056, 'learning_rate': 0.00019930712035689575, 'epoch': 0.07}
{'loss': 0.9056, 'learning_rate': 0.00019930712035689575, 'epoch': 0.07}
{'loss': 0.9436, 'learning_rate': 0.00019929977749159859, 'epoch': 0.07}
{'loss': 0.9436, 'learning_rate': 0.00019929977749159859, 'epoch': 0.07}
{'loss': 0.96, 'learning_rate': 0.00019929239605941208, 'epoch': 0.07}
{'loss': 0.96, 'learning_rate': 0.00019929239605941208, 'epoch': 0.07}
                                              7%|â–‹         | 342/5197 [33:27:15<490:07:14, 363.43s/it]  7%|â–‹         | 343/5197 [33:33:28<494:08:24, 366.48s/it]                                                            7%|â–‹         | 343/5197 [33:33:28<494:08:24, 366.48s/it]  7%|â–‹         | 344/5197 [33:39:52<501:10:26, 371.78s/it]                                                            7%|â–‹         | 344/5197 [33:39:52<501:10:26, 371.78s/it]  7%|â–‹         | 345/5197 [33:46:06<501:40:45, 372.23s/it]                                                            7%|â–‹         | 345/5197 [33:46:06<501:40:45, 372.23s/it]  7%|â–‹         | 346/5197 [33:52:09<497:49:40, 369.45s/it]                                                            7%|â–‹         | 346/5197 [33:52:09<497:49:40, 369.45s/it]  7%|â–‹         | 347/5197 [33:58:30<502:33:32, 373.03s/it]                                                            7%|â–‹         | 347/5197 [33:58:30<502:33:32, 373.03s/it]  7%|â–{'loss': 0.938, 'learning_rate': 0.0001992849760632031, 'epoch': 0.07}
{'loss': 0.938, 'learning_rate': 0.0001992849760632031, 'epoch': 0.07}
  7%|â–‹         | 343/5197 [33:33:56<494:07:59, 366.48s/it]  7%|â–‹         | 344/5197 [33:40:20<501:10:14, 371.77s/it]                                                            7%|â–‹         | 344/5197 [33:40:20<501:10:14, 371.77s/it]  7%|â–‹         | 345/5197 [33:46:33<501:40:19, 372.22s/it]                                                            7%|â–‹         | 345/5197 [33:46:33<501:40:19, 372.22s/it]  7%|â–‹         | 346/5197 [33:52:36<497:49:28, 369.44s/it]                                                            7%|â–‹         | 346/5197 [33:52:36<497:49:28, 369.44s/it]  7%|â–‹         | 347/5197 [33:58:57<502:33:19, 373.03s/it]                                                            7%|â–‹         | 347/5197 [33:58:57<502:33:19, 373.03s/it]  7%|â–‹         | 348/5197 [34:05:00<498:07:56, 369.82s/it]                                                            7%|â–‹         | 348/5197 [34:05:00<498:07:56, 369.82s/it]  7%|â–‹         | 349/5197 [34:10:49<489:45:07, 363.{'loss': 1.0149, 'learning_rate': 0.00019927751750585347, 'epoch': 0.07}
{'loss': 1.0149, 'learning_rate': 0.00019927751750585347, 'epoch': 0.07}
{'loss': 0.9481, 'learning_rate': 0.00019927002039026002, 'epoch': 0.07}
{'loss': 0.9481, 'learning_rate': 0.00019927002039026002, 'epoch': 0.07}
{'loss': 1.0386, 'learning_rate': 0.00019926248471933454, 'epoch': 0.07}
{'loss': 1.0386, 'learning_rate': 0.00019926248471933454, 'epoch': 0.07}
[2023-11-23 19:51:37,810] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.323, 'learning_rate': 0.0001992549104960038, 'epoch': 0.07}
{'loss': 0.323, 'learning_rate': 0.0001992549104960038, 'epoch': 0.07}
{'loss': 0.9704, 'learning_rate': 0.0001992472977232095, 'epoch': 0.07}
{'loss': 0.9704, 'learning_rate': 0.0001992472977232095, 'epoch': 0.07}
‹         | 348/5197 [34:04:32<498:08:39, 369.83s/it]                                                            7%|â–‹         | 348/5197 [34:04:32<498:08:39, 369.83s/it]  7%|â–‹         | 349/5197 [34:10:22<489:45:14, 363.68s/it]                                                            7%|â–‹         | 349/5197 [34:10:22<489:45:14, 363.68s/it]  7%|â–‹         | 350/5197 [34:16:11<483:50:15, 359.36s/it]                                                            7%|â–‹         | 350/5197 [34:16:11<483:50:15, 359.36s/it]  7%|â–‹         | 351/5197 [34:22:32<492:28:50, 365.85s/it]                                                            7%|â–‹         | 351/5197 [34:22:32<492:28:50, 365.85s/it]  7%|â–‹         | 352/5197 [34:28:43<494:27:32, 367.40s/it]                                                            7%|â–‹         | 352/5197 [34:28:43<494:27:32, 367.40s/it]  7%|â–‹         | 353/5197 [34:34:53<495:21:35, 368.15s/it]                                                          {'loss': 0.9509, 'learning_rate': 0.00019923964640390843, 'epoch': 0.07}
{'loss': 0.9509, 'learning_rate': 0.00019923964640390843, 'epoch': 0.07}
68s/it]                                                            7%|â–‹         | 349/5197 [34:10:49<489:45:07, 363.68s/it]  7%|â–‹         | 350/5197 [34:16:38<483:50:12, 359.36s/it]                                                            7%|â–‹         | 350/5197 [34:16:38<483:50:12, 359.36s/it]  7%|â–‹         | 351/5197 [34:22:59<492:28:41, 365.85s/it]                                                            7%|â–‹         | 351/5197 [34:22:59<492:28:41, 365.85s/it]  7%|â–‹         | 352/5197 [34:29:10<494:27:31, 367.40s/it]                                                            7%|â–‹         | 352/5197 [34:29:10<494:27:31, 367.40s/it]  7%|â–‹         | 353/5197 [34:35:20<495:21:14, 368.14s/it]                                                            7%|â–‹         | 353/5197 [34:35:20<495:21:14, 368.14s/it]  7%|â–‹         | 354/5197 [34:41:47<502:52:38, 373.81s/it]                                                            7%|â–‹         | 354/5197 [34:41:47<502:52:3[2023-11-23 20:10:42,726] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9062, 'learning_rate': 0.00019923195654107225, 'epoch': 0.07}
{'loss': 0.9062, 'learning_rate': 0.00019923195654107225, 'epoch': 0.07}
{'loss': 0.9992, 'learning_rate': 0.00019922422813768758, 'epoch': 0.07}
{'loss': 0.9992, 'learning_rate': 0.00019922422813768758, 'epoch': 0.07}
[2023-11-23 20:22:29,486] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0182, 'learning_rate': 0.00019921646119675605, 'epoch': 0.07}
{'loss': 1.0182, 'learning_rate': 0.00019921646119675605, 'epoch': 0.07}
{'loss': 0.9861, 'learning_rate': 0.00019920865572129425, 'epoch': 0.07}
{'loss': 0.9861, 'learning_rate': 0.00019920865572129425, 'epoch': 0.07}
  7%|â–‹         | 353/5197 [34:34:53<495:21:35, 368.15s/it]  7%|â–‹         | 354/5197 [34:41:20<502:51:59, 373.80s/it]                                                            7%|â–‹         | 354/5197 [34:41:20<502:51:59, 373.80s/it]  7%|â–‹         | 355/5197 [34:47:48<508:30:30, 378.07s/it]                                                            7%|â–‹         | 355/5197 [34:47:48<508:30:30, 378.07s/it]  7%|â–‹         | 356/5197 [34:53:51<502:30:54, 373.69s/it]                                                            7%|â–‹         | 356/5197 [34:53:51<502:30:54, 373.69s/it]  7%|â–‹         | 357/5197 [34:59:35<490:08:40, 364.57s/it]                                                            7%|â–‹         | 357/5197 [34:59:35<490:08:40, 364.57s/it]  7%|â–‹         | 358/5197 [35:05:54<496:06:54, 369.09s/it]                                                            7%|â–‹         | 358/5197 [35:05:54<496:06:54, 369.09s/it]  7%|â–‹         | 359/5197 [35:11:49<490:23:25, 364.{'loss': 0.9437, 'learning_rate': 0.00019920081171433379, 'epoch': 0.07}
{'loss': 0.9437, 'learning_rate': 0.00019920081171433379, 'epoch': 0.07}
{'loss': 1.0262, 'learning_rate': 0.00019919292917892112, 'epoch': 0.07}
{'loss': 1.0262, 'learning_rate': 0.00019919292917892112, 'epoch': 0.07}
8, 373.81s/it]  7%|â–‹         | 355/5197 [34:48:15<508:30:58, 378.08s/it]                                                            7%|â–‹         | 355/5197 [34:48:15<508:30:58, 378.08s/it]  7%|â–‹         | 356/5197 [34:54:19<502:31:42, 373.70s/it]                                                            7%|â–‹         | 356/5197 [34:54:19<502:31:42, 373.70s/it]  7%|â–‹         | 357/5197 [35:00:02<490:08:59, 364.57s/it]                                                            7%|â–‹         | 357/5197 [35:00:02<490:08:59, 364.57s/it]  7%|â–‹         | 358/5197 [35:06:22<496:07:22, 369.09s/it]                                                            7%|â–‹         | 358/5197 [35:06:22<496:07:22, 369.09s/it]  7%|â–‹         | 359/5197 [35:12:17<490:23:26, 364.90s/it]                                                            7%|â–‹         | 359/5197 [35:12:17<490:23:26, 364.90s/it]  7%|â–‹         | 360/5197 [35:18:59<505:25:17, 376.17s/it]                                      {'loss': 0.9709, 'learning_rate': 0.00019918500811811778, 'epoch': 0.07}
{'loss': 0.9709, 'learning_rate': 0.00019918500811811778, 'epoch': 0.07}
{'loss': 1.0199, 'learning_rate': 0.00019917704853500016, 'epoch': 0.07}
{'loss': 1.0199, 'learning_rate': 0.00019917704853500016, 'epoch': 0.07}
{'loss': 0.9382, 'learning_rate': 0.00019916905043265972, 'epoch': 0.07}
{'loss': 0.9382, 'learning_rate': 0.00019916905043265972, 'epoch': 0.07}
{'loss': 1.1055, 'learning_rate': 0.00019916101381420285, 'epoch': 0.07}
{'loss': 1.1055, 'learning_rate': 0.00019916101381420285, 'epoch': 0.07}
90s/it]                                                            7%|â–‹         | 359/5197 [35:11:50<490:23:25, 364.90s/it]  7%|â–‹         | 360/5197 [35:18:32<505:25:08, 376.16s/it]                                                            7%|â–‹         | 360/5197 [35:18:32<505:25:08, 376.16s/it]  7%|â–‹         | 361/5197 [35:25:11<514:41:33, 383.15s/it]                                                            7%|â–‹         | 361/5197 [35:25:11<514:41:33, 383.15s/it]  7%|â–‹         | 362/5197 [35:31:48<520:00:50, 387.19s/it]                                                            7%|â–‹         | 362/5197 [35:31:48<520:00:50, 387.19s/it]  7%|â–‹         | 363/5197 [35:37:59<513:33:16, 382.46s/it]                                                            7%|â–‹         | 363/5197 [35:37:59<513:33:16, 382.46s/it]  7%|â–‹         | 364/5197 [35:44:12<509:27:22, 379.48s/it]                                                            7%|â–‹         | 364/5197 [35:44:12<509:27:2{'loss': 0.9445, 'learning_rate': 0.00019915293868275083, 'epoch': 0.07}
{'loss': 0.9445, 'learning_rate': 0.00019915293868275083, 'epoch': 0.07}
                      7%|â–‹         | 360/5197 [35:18:59<505:25:17, 376.17s/it]  7%|â–‹         | 361/5197 [35:25:39<514:41:48, 383.15s/it]                                                            7%|â–‹         | 361/5197 [35:25:39<514:41:48, 383.15s/it]  7%|â–‹         | 362/5197 [35:32:15<520:00:58, 387.19s/it]                                                            7%|â–‹         | 362/5197 [35:32:15<520:00:58, 387.19s/it]  7%|â–‹         | 363/5197 [35:38:27<513:33:14, 382.46s/it]                                                            7%|â–‹         | 363/5197 [35:38:27<513:33:14, 382.46s/it]  7%|â–‹         | 364/5197 [35:44:39<509:27:27, 379.48s/it]                                                            7%|â–‹         | 364/5197 [35:44:39<509:27:27, 379.48s/it]  7%|â–‹         | 365/5197 [35:50:26<496:10:25, 369.67s/it]                                                            7%|â–‹         | 365/5197 [35:50:26<496:10:25, 369.67s/it]  7%|â–‹         | 366/5197 [35{'loss': 0.9005, 'learning_rate': 0.00019914482504143995, 'epoch': 0.07}
{'loss': 0.9005, 'learning_rate': 0.00019914482504143995, 'epoch': 0.07}
{'loss': 1.0107, 'learning_rate': 0.00019913667289342147, 'epoch': 0.07}
{'loss': 1.0107, 'learning_rate': 0.00019913667289342147, 'epoch': 0.07}
{'loss': 0.9553, 'learning_rate': 0.0001991284822418616, 'epoch': 0.07}
{'loss': 0.9553, 'learning_rate': 0.0001991284822418616, 'epoch': 0.07}
{'loss': 1.0015, 'learning_rate': 0.00019912025308994148, 'epoch': 0.07}
{'loss': 1.0015, 'learning_rate': 0.00019912025308994148, 'epoch': 0.07}
{'loss': 0.8729, 'learning_rate': 0.00019911198544085722, 'epoch': 0.07}
2, 379.48s/it]  7%|â–‹         | 365/5197 [35:49:59<496:10:41, 369.67s/it]                                                            7%|â–‹         | 365/5197 [35:49:59<496:10:41, 369.67s/it]  7%|â–‹         | 366/5197 [35:55:56<491:15:54, 366.08s/it]                                                            7%|â–‹         | 366/5197 [35:55:56<491:15:54, 366.08s/it]  7%|â–‹         | 367/5197 [36:01:38<481:09:18, 358.62s/it]                                                            7%|â–‹         | 367/5197 [36:01:38<481:09:18, 358.62s/it]  7%|â–‹         | 368/5197 [36:07:19<474:17:02, 353.58s/it]                                                            7%|â–‹         | 368/5197 [36:07:19<474:17:02, 353.58s/it]  7%|â–‹         | 369/5197 [36:13:04<470:30:36, 350.84s/it]                                                            7%|â–‹         | 369/5197 [36:13:04<470:30:36, 350.84s/it]  7%|â–‹         | 370/5197 [36:18:45<466:31:47, 347.94s/it]                                      {'loss': 0.8729, 'learning_rate': 0.00019911198544085722, 'epoch': 0.07}
{'loss': 1.0142, 'learning_rate': 0.00019910367929781988, 'epoch': 0.07}
{'loss': 1.0142, 'learning_rate': 0.00019910367929781988, 'epoch': 0.07}
:56:24<491:15:28, 366.08s/it]                                                            7%|â–‹         | 366/5197 [35:56:24<491:15:28, 366.08s/it]  7%|â–‹         | 367/5197 [36:02:05<481:09:04, 358.62s/it]                                                            7%|â–‹         | 367/5197 [36:02:05<481:09:04, 358.62s/it]  7%|â–‹         | 368/5197 [36:07:47<474:16:26, 353.57s/it]                                                            7%|â–‹         | 368/5197 [36:07:47<474:16:26, 353.57s/it]  7%|â–‹         | 369/5197 [36:13:31<470:30:43, 350.84s/it]                                                            7%|â–‹         | 369/5197 [36:13:31<470:30:43, 350.84s/it]  7%|â–‹         | 370/5197 [36:19:12<466:31:59, 347.94s/it]                                                            7%|â–‹         | 370/5197 [36:19:13<466:31:59, 347.94s/it]  7%|â–‹         | 371/5197 [36:24:38<457:15:36, 341.10s/it]                                                            7%|â–‹         | 371/5{'loss': 0.9389, 'learning_rate': 0.00019909533466405546, 'epoch': 0.07}
{'loss': 0.9389, 'learning_rate': 0.00019909533466405546, 'epoch': 0.07}
{'loss': 0.9362, 'learning_rate': 0.00019908695154280496, 'epoch': 0.07}
{'loss': 0.9362, 'learning_rate': 0.00019908695154280496, 'epoch': 0.07}
{'loss': 1.033, 'learning_rate': 0.00019907852993732424, 'epoch': 0.07}
{'loss': 1.033, 'learning_rate': 0.00019907852993732424, 'epoch': 0.07}
{'loss': 1.0232, 'learning_rate': 0.0001990700698508842, 'epoch': 0.07}
{'loss': 1.0232, 'learning_rate': 0.0001990700698508842, 'epoch': 0.07}
                      7%|â–‹         | 370/5197 [36:18:45<466:31:47, 347.94s/it]  7%|â–‹         | 371/5197 [36:24:10<457:15:08, 341.09s/it]                                                            7%|â–‹         | 371/5197 [36:24:10<457:15:08, 341.09s/it]  7%|â–‹         | 372/5197 [36:29:44<454:19:21, 338.98s/it]                                                            7%|â–‹         | 372/5197 [36:29:44<454:19:21, 338.98s/it]  7%|â–‹         | 373/5197 [36:35:36<459:16:08, 342.74s/it]                                                            7%|â–‹         | 373/5197 [36:35:36<459:16:08, 342.74s/it]  7%|â–‹         | 374/5197 [36:41:00<451:36:52, 337.10s/it]                                                            7%|â–‹         | 374/5197 [36:41:00<451:36:52, 337.10s/it]  7%|â–‹         | 375/5197 [36:46:46<455:11:02, 339.83s/it]                                                            7%|â–‹         | 375/5197 [36:46:46<455:11:02, 339.83s/it]  7%|â–‹         | 376/5197 [36{'loss': 0.886, 'learning_rate': 0.0001990615712867706, 'epoch': 0.07}
{'loss': 0.886, 'learning_rate': 0.0001990615712867706, 'epoch': 0.07}
197 [36:24:38<457:15:36, 341.10s/it]  7%|â–‹         | 372/5197 [36:30:12<454:19:38, 338.98s/it]                                                            7%|â–‹         | 372/5197 [36:30:12<454:19:38, 338.98s/it]  7%|â–‹         | 373/5197 [36:36:03<459:16:46, 342.75s/it]                                                            7%|â–‹         | 373/5197 [36:36:03<459:16:46, 342.75s/it]  7%|â–‹         | 374/5197 [36:41:27<451:37:21, 337.10s/it]                                                            7%|â–‹         | 374/5197 [36:41:27<451:37:21, 337.10s/it]  7%|â–‹         | 375/5197 [36:47:13<455:11:21, 339.83s/it]                                                            7%|â–‹         | 375/5197 [36:47:13<455:11:21, 339.83s/it]  7%|â–‹         | 376/5197 [36:53:08<461:06:05, 344.32s/it]                                                            7%|â–‹         | 376/5197 [36:53:08<461:06:05, 344.32s/it]  7%|â–‹         | 377/5197 [36:59:12<468:45:54, 350.11s/it]                {'loss': 0.9705, 'learning_rate': 0.00019905303424828417, 'epoch': 0.07}
{'loss': 0.9705, 'learning_rate': 0.00019905303424828417, 'epoch': 0.07}
{'loss': 0.9729, 'learning_rate': 0.00019904445873874068, 'epoch': 0.07}
{'loss': 0.9729, 'learning_rate': 0.00019904445873874068, 'epoch': 0.07}
{'loss': 0.9264, 'learning_rate': 0.00019903584476147065, 'epoch': 0.07}
{'loss': 0.9264, 'learning_rate': 0.00019903584476147065, 'epoch': 0.07}
{'loss': 0.9312, 'learning_rate': 0.00019902719231981974, 'epoch': 0.07}
{'loss': 0.9312, 'learning_rate': 0.00019902719231981974, 'epoch': 0.07}
{'loss': 1.0674, 'learning_rate': 0.00019901850141714841, 'epoch': 0.07}
:52:41<461:05:44, 344.32s/it]                                                            7%|â–‹         | 376/5197 [36:52:41<461:05:44, 344.32s/it]  7%|â–‹         | 377/5197 [36:58:44<468:46:04, 350.12s/it]                                                            7%|â–‹         | 377/5197 [36:58:44<468:46:04, 350.12s/it]  7%|â–‹         | 378/5197 [37:04:33<468:14:07, 349.79s/it]                                                            7%|â–‹         | 378/5197 [37:04:33<468:14:07, 349.79s/it]  7%|â–‹         | 379/5197 [37:10:10<462:54:59, 345.89s/it]                                                            7%|â–‹         | 379/5197 [37:10:10<462:54:59, 345.89s/it]  7%|â–‹         | 380/5197 [37:16:23<473:36:11, 353.95s/it]                                                            7%|â–‹         | 380/5197 [37:16:23<473:36:11, 353.95s/it]  7%|â–‹         | 381/5197 [37:22:21<475:10:04, 355.19s/it]                                                            7%|â–‹         | 381/5{'loss': 1.0674, 'learning_rate': 0.00019901850141714841, 'epoch': 0.07}
{'loss': 0.9478, 'learning_rate': 0.0001990097720568321, 'epoch': 0.07}
{'loss': 0.9478, 'learning_rate': 0.0001990097720568321, 'epoch': 0.07}
                                            7%|â–‹         | 377/5197 [36:59:12<468:45:54, 350.11s/it]  7%|â–‹         | 378/5197 [37:05:01<468:13:46, 349.79s/it]                                                            7%|â–‹         | 378/5197 [37:05:01<468:13:46, 349.79s/it]  7%|â–‹         | 379/5197 [37:10:38<462:54:36, 345.89s/it]                                                            7%|â–‹         | 379/5197 [37:10:38<462:54:36, 345.89s/it]  7%|â–‹         | 380/5197 [37:16:50<473:35:48, 353.94s/it]                                                            7%|â–‹         | 380/5197 [37:16:50<473:35:48, 353.94s/it]  7%|â–‹         | 381/5197 [37:22:48<475:09:40, 355.19s/it]                                                            7%|â–‹         | 381/5197 [37:22:48<475:09:40, 355.19s/it]  7%|â–‹         | 382/5197 [37:28:36<471:56:27, 352.85s/it]                                                            7%|â–‹         | 382/5197 [37:28:36<471:56:27, 352.85s/it]  7%|â–‹ {'loss': 0.9238, 'learning_rate': 0.00019900100424226125, 'epoch': 0.07}
{'loss': 0.9238, 'learning_rate': 0.00019900100424226125, 'epoch': 0.07}
{'loss': 0.9493, 'learning_rate': 0.00019899219797684113, 'epoch': 0.07}
{'loss': 0.9493, 'learning_rate': 0.00019899219797684113, 'epoch': 0.07}
{'loss': 0.9071, 'learning_rate': 0.000198983353263992, 'epoch': 0.07}
{'loss': 0.9071, 'learning_rate': 0.000198983353263992, 'epoch': 0.07}
{'loss': 0.9182, 'learning_rate': 0.00019897447010714905, 'epoch': 0.07}
{'loss': 0.9182, 'learning_rate': 0.00019897447010714905, 'epoch': 0.07}
{'loss': 0.9134, 'learning_rate': 0.00019896554850976238, 'epoch': 0.07}
197 [37:22:21<475:10:04, 355.19s/it]  7%|â–‹         | 382/5197 [37:28:08<471:56:37, 352.86s/it]                                                            7%|â–‹         | 382/5197 [37:28:08<471:56:37, 352.86s/it]  7%|â–‹         | 383/5197 [37:34:04<473:04:12, 353.77s/it]                                                            7%|â–‹         | 383/5197 [37:34:04<473:04:12, 353.77s/it]  7%|â–‹         | 384/5197 [37:40:03<474:55:10, 355.23s/it]                                                            7%|â–‹         | 384/5197 [37:40:03<474:55:10, 355.23s/it]  7%|â–‹         | 385/5197 [37:45:55<473:21:39, 354.14s/it]                                                            7%|â–‹         | 385/5197 [37:45:55<473:21:39, 354.14s/it]  7%|â–‹         | 386/5197 [37:51:44<471:33:55, 352.87s/it]                                                            7%|â–‹         | 386/5197 [37:51:44<471:33:55, 352.87s/it]  7%|â–‹         | 387/5197 [37:57:32<469:19:15, 351.26s/it]                {'loss': 0.9134, 'learning_rate': 0.00019896554850976238, 'epoch': 0.07}
{'loss': 1.0326, 'learning_rate': 0.00019895658847529708, 'epoch': 0.07}
{'loss': 1.0326, 'learning_rate': 0.00019895658847529708, 'epoch': 0.07}
        | 383/5197 [37:34:32<473:04:29, 353.77s/it]                                                            7%|â–‹         | 383/5197 [37:34:32<473:04:29, 353.77s/it]  7%|â–‹         | 384/5197 [37:40:30<474:55:27, 355.23s/it]                                                            7%|â–‹         | 384/5197 [37:40:30<474:55:27, 355.23s/it]  7%|â–‹         | 385/5197 [37:46:22<473:21:36, 354.13s/it]                                                            7%|â–‹         | 385/5197 [37:46:22<473:21:36, 354.13s/it]  7%|â–‹         | 386/5197 [37:52:12<471:33:59, 352.87s/it]                                                            7%|â–‹         | 386/5197 [37:52:12<471:33:59, 352.87s/it]  7%|â–‹         | 387/5197 [37:57:59<469:19:10, 351.26s/it]                                                            7%|â–‹         | 387/5197 [37:57:59<469:19:10, 351.26s/it]  7%|â–‹         | 388/5197 [38:03:31<461:30:56, 345.49s/it]                                                            {'loss': 1.0002, 'learning_rate': 0.00019894759000723306, 'epoch': 0.07}
{'loss': 1.0002, 'learning_rate': 0.00019894759000723306, 'epoch': 0.07}
{'loss': 0.948, 'learning_rate': 0.00019893855310906526, 'epoch': 0.08}
{'loss': 0.948, 'learning_rate': 0.00019893855310906526, 'epoch': 0.08}
{'loss': 1.0208, 'learning_rate': 0.0001989294777843035, 'epoch': 0.08}
{'loss': 1.0208, 'learning_rate': 0.0001989294777843035, 'epoch': 0.08}
{'loss': 0.8971, 'learning_rate': 0.00019892036403647254, 'epoch': 0.08}
{'loss': 0.8971, 'learning_rate': 0.00019892036403647254, 'epoch': 0.08}
                                            7%|â–‹         | 387/5197 [37:57:32<469:19:15, 351.26s/it]  7%|â–‹         | 388/5197 [38:03:04<461:31:21, 345.49s/it]                                                            7%|â–‹         | 388/5197 [38:03:04<461:31:21, 345.49s/it]  7%|â–‹         | 389/5197 [38:08:37<456:22:22, 341.71s/it]                                                            7%|â–‹         | 389/5197 [38:08:37<456:22:22, 341.71s/it]  8%|â–Š         | 390/5197 [38:14:12<453:29:32, 339.62s/it]                                                            8%|â–Š         | 390/5197 [38:14:12<453:29:32, 339.62s/it]  8%|â–Š         | 391/5197 [38:20:19<464:29:12, 347.93s/it]                                                            8%|â–Š         | 391/5197 [38:20:19<464:29:12, 347.93s/it]  8%|â–Š         | 392/5197 [38:25:49<457:21:08, 342.66s/it]                                                            8%|â–Š         | 392/5197 [38:25:49<457:21:08, 342.66s/it]  8%|â–Š {'loss': 0.9396, 'learning_rate': 0.00019891121186911206, 'epoch': 0.08}
{'loss': 0.9396, 'learning_rate': 0.00019891121186911206, 'epoch': 0.08}
7%|â–‹         | 388/5197 [38:03:31<461:30:56, 345.49s/it]  7%|â–‹         | 389/5197 [38:09:04<456:21:45, 341.70s/it]                                                            7%|â–‹         | 389/5197 [38:09:04<456:21:45, 341.70s/it]  8%|â–Š         | 390/5197 [38:14:39<453:29:07, 339.62s/it]                                                            8%|â–Š         | 390/5197 [38:14:39<453:29:07, 339.62s/it]  8%|â–Š         | 391/5197 [38:20:46<464:28:34, 347.92s/it]                                                            8%|â–Š         | 391/5197 [38:20:46<464:28:34, 347.92s/it]  8%|â–Š         | 392/5197 [38:26:17<457:20:39, 342.65s/it]                                                            8%|â–Š         | 392/5197 [38:26:17<457:20:39, 342.65s/it]  8%|â–Š         | 393/5197 [38:32:16<464:04:53, 347.77s/it]                                                            8%|â–Š         | 393/5197 [38:32:16<464:04:53, 347.77s/it]  8%|â–Š         | 394/5197 [38:38:31<474:33:28, 355.70{'loss': 0.95, 'learning_rate': 0.00019890202128577662, 'epoch': 0.08}
{'loss': 0.95, 'learning_rate': 0.00019890202128577662, 'epoch': 0.08}
{'loss': 0.9954, 'learning_rate': 0.00019889279229003576, 'epoch': 0.08}
{'loss': 0.9954, 'learning_rate': 0.00019889279229003576, 'epoch': 0.08}
{'loss': 1.0018, 'learning_rate': 0.00019888352488547394, 'epoch': 0.08}
{'loss': 1.0018, 'learning_rate': 0.00019888352488547394, 'epoch': 0.08}
{'loss': 0.9625, 'learning_rate': 0.00019887421907569048, 'epoch': 0.08}
{'loss': 0.9625, 'learning_rate': 0.00019887421907569048, 'epoch': 0.08}
{'loss': 0.9754, 'learning_rate': 0.00019886487486429964, 'epoch': 0.08}
{'loss': 0.9754, 'learning_rate': 0.00019886487486429964, 'epoch': 0.08}
        | 393/5197 [38:31:49<464:05:49, 347.78s/it]                                                            8%|â–Š         | 393/5197 [38:31:49<464:05:49, 347.78s/it]  8%|â–Š         | 394/5197 [38:38:03<474:33:56, 355.70s/it]                                                            8%|â–Š         | 394/5197 [38:38:03<474:33:56, 355.70s/it]  8%|â–Š         | 395/5197 [38:44:14<480:30:23, 360.23s/it]                                                            8%|â–Š         | 395/5197 [38:44:14<480:30:23, 360.23s/it]  8%|â–Š         | 396/5197 [38:50:11<478:58:27, 359.16s/it]                                                            8%|â–Š         | 396/5197 [38:50:11<478:58:27, 359.16s/it]  8%|â–Š         | 397/5197 [38:55:54<472:26:28, 354.33s/it]                                                            8%|â–Š         | 397/5197 [38:55:54<472:26:28, 354.33s/it]  8%|â–Š         | 398/5197 [39:01:58<476:09:05, 357.19s/it]                                                            {'loss': 1.0087, 'learning_rate': 0.00019885549225493064, 'epoch': 0.08}
{'loss': 1.0087, 'learning_rate': 0.00019885549225493064, 'epoch': 0.08}
s/it]                                                            8%|â–Š         | 394/5197 [38:38:31<474:33:28, 355.70s/it]  8%|â–Š         | 395/5197 [38:44:41<480:30:04, 360.23s/it]                                                            8%|â–Š         | 395/5197 [38:44:41<480:30:04, 360.23s/it]  8%|â–Š         | 396/5197 [38:50:38<478:57:49, 359.15s/it]                                                            8%|â–Š         | 396/5197 [38:50:38<478:57:49, 359.15s/it]  8%|â–Š         | 397/5197 [38:56:21<472:26:21, 354.33s/it]                                                            8%|â–Š         | 397/5197 [38:56:21<472:26:21, 354.33s/it]  8%|â–Š         | 398/5197 [39:02:25<476:09:27, 357.19s/it]                                                            8%|â–Š         | 398/5197 [39:02:25<476:09:27, 357.19s/it]  8%|â–Š         | 399/5197 [39:08:47<485:50:05, 364.53s/it]                                                            8%|â–Š         | 399/5197 [39:08:47<485:50:05,{'loss': 0.9762, 'learning_rate': 0.00019884607125122755, 'epoch': 0.08}
{'loss': 0.9762, 'learning_rate': 0.00019884607125122755, 'epoch': 0.08}
{'loss': 0.968, 'learning_rate': 0.0001988366118568494, 'epoch': 0.08}
{'loss': 0.968, 'learning_rate': 0.0001988366118568494, 'epoch': 0.08}
{'loss': 0.9522, 'learning_rate': 0.0001988271140754701, 'epoch': 0.08}
{'loss': 0.9522, 'learning_rate': 0.0001988271140754701, 'epoch': 0.08}
{'loss': 0.9077, 'learning_rate': 0.00019881757791077845, 'epoch': 0.08}
{'loss': 0.9077, 'learning_rate': 0.00019881757791077845, 'epoch': 0.08}
8%|â–Š         | 398/5197 [39:01:58<476:09:05, 357.19s/it]  8%|â–Š         | 399/5197 [39:08:19<485:49:47, 364.52s/it]                                                            8%|â–Š         | 399/5197 [39:08:19<485:49:47, 364.52s/it]  8%|â–Š         | 400/5197 [39:14:30<488:18:32, 366.46s/it]                                                            8%|â–Š         | 400/5197 [39:14:30<488:18:32, 366.46s/it]  8%|â–Š         | 401/5197 [39:20:32<486:26:01, 365.13s/it]                                                            8%|â–Š         | 401/5197 [39:20:32<486:26:01, 365.13s/it]  8%|â–Š         | 402/5197 [39:26:35<485:19:41, 364.38s/it]                                                            8%|â–Š         | 402/5197 [39:26:35<485:19:41, 364.38s/it]  8%|â–Š         | 403/5197 [39:32:52<490:27:58, 368.31s/it]                                                            8%|â–Š         | 403/5197 [39:32:52<490:27:58, 368.31s/it]  8%|â–Š         | 404/5197 [39:38:28<477:16:04, 358.47{'loss': 0.921, 'learning_rate': 0.00019880800336647824, 'epoch': 0.08}
{'loss': 0.921, 'learning_rate': 0.00019880800336647824, 'epoch': 0.08}
 364.53s/it]  8%|â–Š         | 400/5197 [39:14:58<488:18:41, 366.46s/it]                                                            8%|â–Š         | 400/5197 [39:14:58<488:18:41, 366.46s/it]  8%|â–Š         | 401/5197 [39:21:00<486:26:41, 365.14s/it]                                                            8%|â–Š         | 401/5197 [39:21:00<486:26:41, 365.14s/it]  8%|â–Š         | 402/5197 [39:27:02<485:20:29, 364.39s/it]                                                            8%|â–Š         | 402/5197 [39:27:02<485:20:29, 364.39s/it]  8%|â–Š         | 403/5197 [39:33:20<490:28:41, 368.32s/it]                                                            8%|â–Š         | 403/5197 [39:33:20<490:28:41, 368.32s/it]  8%|â–Š         | 404/5197 [39:38:55<477:16:39, 358.48s/it]                                                            8%|â–Š         | 404/5197 [39:38:55<477:16:39, 358.48s/it]  8%|â–Š         | 405/5197 [39:44:35<469:50:47, 352.97s/it]                                        {'loss': 0.9467, 'learning_rate': 0.0001987983904462881, 'epoch': 0.08}
{'loss': 0.9467, 'learning_rate': 0.0001987983904462881, 'epoch': 0.08}
{'loss': 0.9277, 'learning_rate': 0.00019878873915394154, 'epoch': 0.08}
{'loss': 0.9277, 'learning_rate': 0.00019878873915394154, 'epoch': 0.08}
{'loss': 0.9571, 'learning_rate': 0.00019877904949318703, 'epoch': 0.08}
{'loss': 0.9571, 'learning_rate': 0.00019877904949318703, 'epoch': 0.08}
{'loss': 0.8882, 'learning_rate': 0.00019876932146778794, 'epoch': 0.08}
{'loss': 0.8882, 'learning_rate': 0.00019876932146778794, 'epoch': 0.08}
[2023-11-24 01:31:04,824] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2758, 'learning_rate': 0.00019875955508152253, 'epoch': 0.08}
{'loss': 0.2758, 'learning_rate': 0.00019875955508152253, 'epoch': 0.08}
s/it]                                                            8%|â–Š         | 404/5197 [39:38:28<477:16:04, 358.47s/it]  8%|â–Š         | 405/5197 [39:44:08<469:50:15, 352.97s/it]                                                            8%|â–Š         | 405/5197 [39:44:08<469:50:15, 352.97s/it]  8%|â–Š         | 406/5197 [39:50:20<477:16:17, 358.63s/it]                                                            8%|â–Š         | 406/5197 [39:50:20<477:16:17, 358.63s/it]  8%|â–Š         | 407/5197 [39:56:31<482:18:31, 362.49s/it]                                                            8%|â–Š         | 407/5197 [39:56:31<482:18:31, 362.49s/it]  8%|â–Š         | 408/5197 [40:02:20<476:51:15, 358.46s/it]                                                            8%|â–Š         | 408/5197 [40:02:20<476:51:15, 358.46s/it]  8%|â–Š         | 409/5197 [40:08:10<473:15:38, 355.84s/it]                                                            8%|â–Š         | 409/5197 [40:08:10<473:15:38,[2023-11-24 01:37:17,018] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2725, 'learning_rate': 0.0001987497503381839, 'epoch': 0.08}
{'loss': 0.2725, 'learning_rate': 0.0001987497503381839, 'epoch': 0.08}
                    8%|â–Š         | 405/5197 [39:44:35<469:50:47, 352.97s/it]  8%|â–Š         | 406/5197 [39:50:47<477:16:33, 358.63s/it]                                                            8%|â–Š         | 406/5197 [39:50:47<477:16:33, 358.63s/it]  8%|â–Š         | 407/5197 [39:56:59<482:18:45, 362.49s/it]                                                            8%|â–Š         | 407/5197 [39:56:59<482:18:45, 362.49s/it]  8%|â–Š         | 408/5197 [40:02:48<476:51:09, 358.46s/it]                                                            8%|â–Š         | 408/5197 [40:02:48<476:51:09, 358.46s/it]  8%|â–Š         | 409/5197 [40:08:37<473:15:11, 355.83s/it]                                                            8%|â–Š         | 409/5197 [40:08:37<473:15:11, 355.83s/it]  8%|â–Š         | 410/5197 [40:14:50<479:40:49, 360.74s/it]                                                            8%|â–Š         | 410/5197 [40:14:50<479:40:49, 360.74s/it]  8%|â–Š         | 411/5197 [40:2{'loss': 0.9893, 'learning_rate': 0.00019873990724158014, 'epoch': 0.08}
{'loss': 0.9893, 'learning_rate': 0.00019873990724158014, 'epoch': 0.08}
{'loss': 0.9212, 'learning_rate': 0.00019873002579553418, 'epoch': 0.08}
{'loss': 0.9212, 'learning_rate': 0.00019873002579553418, 'epoch': 0.08}
{'loss': 0.9391, 'learning_rate': 0.00019872010600388392, 'epoch': 0.08}
{'loss': 0.9391, 'learning_rate': 0.00019872010600388392, 'epoch': 0.08}
{'loss': 0.9773, 'learning_rate': 0.00019871014787048197, 'epoch': 0.08}
{'loss': 0.9773, 'learning_rate': 0.00019871014787048197, 'epoch': 0.08}
{'loss': 0.907, 'learning_rate': 0.00019870015139919606, 'epoch': 0.08}
 355.84s/it]  8%|â–Š         | 410/5197 [40:14:22<479:41:12, 360.74s/it]                                                            8%|â–Š         | 410/5197 [40:14:22<479:41:12, 360.74s/it]  8%|â–Š         | 411/5197 [40:20:42<487:01:20, 366.34s/it]                                                            8%|â–Š         | 411/5197 [40:20:42<487:01:20, 366.34s/it]  8%|â–Š         | 412/5197 [40:26:54<489:22:38, 368.18s/it]                                                            8%|â–Š         | 412/5197 [40:26:54<489:22:38, 368.18s/it]  8%|â–Š         | 413/5197 [40:32:58<487:42:50, 367.01s/it]                                                            8%|â–Š         | 413/5197 [40:32:58<487:42:50, 367.01s/it]  8%|â–Š         | 414/5197 [40:38:51<481:45:22, 362.60s/it]                                                            8%|â–Š         | 414/5197 [40:38:51<481:45:22, 362.60s/it]  8%|â–Š         | 415/5197 [40:45:06<486:30:36, 366.26s/it]                                        {'loss': 0.907, 'learning_rate': 0.00019870015139919606, 'epoch': 0.08}
{'loss': 1.0432, 'learning_rate': 0.00019869011659390866, 'epoch': 0.08}
{'loss': 1.0432, 'learning_rate': 0.00019869011659390866, 'epoch': 0.08}
1:09<487:00:42, 366.33s/it]                                                            8%|â–Š         | 411/5197 [40:21:09<487:00:42, 366.33s/it]  8%|â–Š         | 412/5197 [40:27:21<489:21:37, 368.17s/it]                                                            8%|â–Š         | 412/5197 [40:27:22<489:21:37, 368.17s/it]  8%|â–Š         | 413/5197 [40:33:26<487:42:24, 367.00s/it]                                                            8%|â–Š         | 413/5197 [40:33:26<487:42:24, 367.00s/it]  8%|â–Š         | 414/5197 [40:39:18<481:45:02, 362.60s/it]                                                            8%|â–Š         | 414/5197 [40:39:18<481:45:02, 362.60s/it]  8%|â–Š         | 415/5197 [40:45:33<486:30:22, 366.25s/it]                                                            8%|â–Š         | 415/5197 [40:45:33<486:30:22, 366.25s/it]  8%|â–Š         | 416/5197 [40:51:33<484:04:57, 364.50s/it]                                                            8%|â–Š         | 416/519{'loss': 0.9896, 'learning_rate': 0.00019868004345851716, 'epoch': 0.08}
{'loss': 0.9896, 'learning_rate': 0.00019868004345851716, 'epoch': 0.08}
{'loss': 0.9175, 'learning_rate': 0.00019866993199693392, 'epoch': 0.08}
{'loss': 0.9175, 'learning_rate': 0.00019866993199693392, 'epoch': 0.08}
{'loss': 0.9391, 'learning_rate': 0.000198659782213086, 'epoch': 0.08}
{'loss': 0.9391, 'learning_rate': 0.000198659782213086, 'epoch': 0.08}
{'loss': 0.9142, 'learning_rate': 0.00019864959411091556, 'epoch': 0.08}
{'loss': 0.9142, 'learning_rate': 0.00019864959411091556, 'epoch': 0.08}
                    8%|â–Š         | 415/5197 [40:45:06<486:30:36, 366.26s/it]  8%|â–Š         | 416/5197 [40:51:06<484:05:11, 364.51s/it]                                                            8%|â–Š         | 416/5197 [40:51:06<484:05:11, 364.51s/it]  8%|â–Š         | 417/5197 [40:57:15<485:37:57, 365.75s/it]                                                            8%|â–Š         | 417/5197 [40:57:15<485:37:57, 365.75s/it]  8%|â–Š         | 418/5197 [41:03:33<490:26:46, 369.45s/it]                                                            8%|â–Š         | 418/5197 [41:03:33<490:26:46, 369.45s/it]  8%|â–Š         | 419/5197 [41:09:12<478:11:00, 360.29s/it]                                                            8%|â–Š         | 419/5197 [41:09:12<478:11:00, 360.29s/it]  8%|â–Š         | 420/5197 [41:14:53<470:24:23, 354.50s/it]                                                            8%|â–Š         | 420/5197 [41:14:53<470:24:23, 354.50s/it]  8%|â–Š         | 421/5197 [41:2{'loss': 0.9889, 'learning_rate': 0.00019863936769437955, 'epoch': 0.08}
{'loss': 0.9889, 'learning_rate': 0.00019863936769437955, 'epoch': 0.08}
7 [40:51:33<484:04:57, 364.50s/it]  8%|â–Š         | 417/5197 [40:57:42<485:37:36, 365.74s/it]                                                            8%|â–Š         | 417/5197 [40:57:42<485:37:36, 365.74s/it]  8%|â–Š         | 418/5197 [41:04:00<490:26:56, 369.45s/it]                                                            8%|â–Š         | 418/5197 [41:04:00<490:26:56, 369.45s/it]  8%|â–Š         | 419/5197 [41:09:39<478:11:16, 360.29s/it]                                                            8%|â–Š         | 419/5197 [41:09:39<478:11:16, 360.29s/it]  8%|â–Š         | 420/5197 [41:15:20<470:24:31, 354.51s/it]                                                            8%|â–Š         | 420/5197 [41:15:20<470:24:31, 354.51s/it]  8%|â–Š         | 421/5197 [41:21:08<467:33:26, 352.43s/it]                                                            8%|â–Š         | 421/5197 [41:21:08<467:33:26, 352.43s/it]  8%|â–Š         | 422/5197 [41:27:23<476:39:52, 359.37s/it]                  {'loss': 0.916, 'learning_rate': 0.00019862910296744967, 'epoch': 0.08}
{'loss': 0.916, 'learning_rate': 0.00019862910296744967, 'epoch': 0.08}
{'loss': 0.9461, 'learning_rate': 0.00019861879993411275, 'epoch': 0.08}
{'loss': 0.9461, 'learning_rate': 0.00019861879993411275, 'epoch': 0.08}
{'loss': 0.8778, 'learning_rate': 0.00019860845859837032, 'epoch': 0.08}
{'loss': 0.8778, 'learning_rate': 0.00019860845859837032, 'epoch': 0.08}
{'loss': 0.9493, 'learning_rate': 0.00019859807896423882, 'epoch': 0.08}
{'loss': 0.9493, 'learning_rate': 0.00019859807896423882, 'epoch': 0.08}
{'loss': 1.0046, 'learning_rate': 0.0001985876610357496, 'epoch': 0.08}
{'loss': 1.0046, 'learning_rate': 0.0001985876610357496, 'epoch': 0.08}
0:40<467:33:12, 352.43s/it]                                                            8%|â–Š         | 421/5197 [41:20:40<467:33:12, 352.43s/it]  8%|â–Š         | 422/5197 [41:26:56<476:39:12, 359.36s/it]                                                            8%|â–Š         | 422/5197 [41:26:56<476:39:12, 359.36s/it]  8%|â–Š         | 423/5197 [41:33:09<482:13:04, 363.63s/it]                                                            8%|â–Š         | 423/5197 [41:33:09<482:13:04, 363.63s/it]  8%|â–Š         | 424/5197 [41:39:24<486:27:41, 366.91s/it]                                                            8%|â–Š         | 424/5197 [41:39:24<486:27:41, 366.91s/it]  8%|â–Š         | 425/5197 [41:45:32<486:42:10, 367.17s/it]                                                            8%|â–Š         | 425/5197 [41:45:32<486:42:10, 367.17s/it]  8%|â–Š         | 426/5197 [41:51:43<488:22:16, 368.50s/it]                                                            8%|â–Š         | 426/519{'loss': 0.9193, 'learning_rate': 0.00019857720481694885, 'epoch': 0.08}
{'loss': 0.9193, 'learning_rate': 0.00019857720481694885, 'epoch': 0.08}
                                          8%|â–Š         | 422/5197 [41:27:23<476:39:52, 359.37s/it]  8%|â–Š         | 423/5197 [41:33:37<482:14:01, 363.64s/it]                                                            8%|â–Š         | 423/5197 [41:33:37<482:14:01, 363.64s/it]  8%|â–Š         | 424/5197 [41:39:51<486:28:24, 366.92s/it]                                                            8%|â–Š         | 424/5197 [41:39:51<486:28:24, 366.92s/it]  8%|â–Š         | 425/5197 [41:45:59<486:42:00, 367.17s/it]                                                            8%|â–Š         | 425/5197 [41:45:59<486:42:00, 367.17s/it]  8%|â–Š         | 426/5197 [41:52:11<488:22:31, 368.51s/it]                                                            8%|â–Š         | 426/5197 [41:52:11<488:22:31, 368.51s/it]  8%|â–Š         | 427/5197 [41:58:31<493:03:47, 372.12s/it]                                                            8%|â–Š         | 427/5197 [41:58:31<493:03:47, 372.12s/it]  8%|â–Š   {'loss': 0.9171, 'learning_rate': 0.00019856671031189766, 'epoch': 0.08}
{'loss': 0.9171, 'learning_rate': 0.00019856671031189766, 'epoch': 0.08}
{'loss': 0.9237, 'learning_rate': 0.000198556177524672, 'epoch': 0.08}
{'loss': 0.9237, 'learning_rate': 0.000198556177524672, 'epoch': 0.08}
{'loss': 0.9424, 'learning_rate': 0.0001985456064593626, 'epoch': 0.08}
{'loss': 0.9424, 'learning_rate': 0.0001985456064593626, 'epoch': 0.08}
{'loss': 0.9991, 'learning_rate': 0.00019853499712007522, 'epoch': 0.08}
{'loss': 0.9991, 'learning_rate': 0.00019853499712007522, 'epoch': 0.08}
{'loss': 0.9734, 'learning_rate': 0.00019852434951093034, 'epoch': 0.08}
7 [41:51:43<488:22:16, 368.50s/it]  8%|â–Š         | 427/5197 [41:58:04<493:03:50, 372.12s/it]                                                            8%|â–Š         | 427/5197 [41:58:04<493:03:50, 372.12s/it]  8%|â–Š         | 428/5197 [42:04:10<490:26:05, 370.22s/it]                                                            8%|â–Š         | 428/5197 [42:04:10<490:26:05, 370.22s/it]  8%|â–Š         | 429/5197 [42:10:10<486:18:40, 367.18s/it]                                                            8%|â–Š         | 429/5197 [42:10:10<486:18:40, 367.18s/it]  8%|â–Š         | 430/5197 [42:16:01<480:03:40, 362.54s/it]                                                            8%|â–Š         | 430/5197 [42:16:01<480:03:40, 362.54s/it]  8%|â–Š         | 431/5197 [42:22:18<485:33:07, 366.76s/it]                                                            8%|â–Š         | 431/5197 [42:22:18<485:33:07, 366.76s/it]  8%|â–Š         | 432/5197 [42:28:26<485:55:56, 367.13s/it]                  {'loss': 0.9734, 'learning_rate': 0.00019852434951093034, 'epoch': 0.08}
{'loss': 0.8999, 'learning_rate': 0.00019851366363606346, 'epoch': 0.08}
      | 428/5197 [42:04:37<490:25:45, 370.21s/it]                                                            8%|â–Š         | 428/5197 [42:04:37<490:25:45, 370.21s/it]  8%|â–Š         | 429/5197 [42:10:37<486:18:49, 367.18s/it]                                                            8%|â–Š         | 429/5197 [42:10:37<486:18:49, 367.18s/it]  8%|â–Š         | 430/5197 [42:16:29<480:03:40, 362.54s/it]                                                            8%|â–Š         | 430/5197 [42:16:29<480:03:40, 362.54s/it]  8%|â–Š         | 431/5197 [42:22:45<485:32:39, 366.76s/it]                                                            8%|â–Š         | 431/5197 [42:22:45<485:32:39, 366.76s/it]  8%|â–Š         | 432/5197 [42:28:53<485:55:35, 367.12s/it]                                                            8%|â–Š         | 432/5197 [42:28:53<485:55:35, 367.12s/it]  8%|â–Š         | 433/5197 [42:35:05<487:33:49, 368.44s/it]                                                            8%{'loss': 0.8999, 'learning_rate': 0.00019851366363606346, 'epoch': 0.08}
[2023-11-24 04:03:32,460] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3072, 'learning_rate': 0.00019850293949962478, 'epoch': 0.08}
{'loss': 0.3072, 'learning_rate': 0.00019850293949962478, 'epoch': 0.08}
[2023-11-24 04:09:46,202] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2774, 'learning_rate': 0.00019849217710577946, 'epoch': 0.08}
{'loss': 0.2774, 'learning_rate': 0.00019849217710577946, 'epoch': 0.08}
[2023-11-24 04:16:04,828] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9722, 'learning_rate': 0.00019848137645870747, 'epoch': 0.08}
{'loss': 0.9722, 'learning_rate': 0.00019848137645870747, 'epoch': 0.08}
{'loss': 0.9311, 'learning_rate': 0.0001984705375626036, 'epoch': 0.08}
{'loss': 0.9311, 'learning_rate': 0.0001984705375626036, 'epoch': 0.08}
                                          8%|â–Š         | 432/5197 [42:28:26<485:55:56, 367.13s/it]  8%|â–Š         | 433/5197 [42:34:37<487:34:01, 368.44s/it]                                                            8%|â–Š         | 433/5197 [42:34:37<487:34:01, 368.44s/it]  8%|â–Š         | 434/5197 [42:40:38<484:11:42, 365.97s/it]                                                            8%|â–Š         | 434/5197 [42:40:38<484:11:42, 365.97s/it]  8%|â–Š         | 435/5197 [42:46:51<487:10:45, 368.30s/it]                                                            8%|â–Š         | 435/5197 [42:46:51<487:10:45, 368.30s/it]  8%|â–Š         | 436/5197 [42:53:10<491:10:47, 371.40s/it]                                                            8%|â–Š         | 436/5197 [42:53:10<491:10:47, 371.40s/it]  8%|â–Š         | 437/5197 [42:58:52<479:23:10, 362.56s/it]                                                            8%|â–Š         | 437/5197 [42:58:52<479:23:10, 362.56s/it]  8%|â–Š   {'loss': 0.9087, 'learning_rate': 0.0001984596604216777, 'epoch': 0.08}
{'loss': 0.9087, 'learning_rate': 0.0001984596604216777, 'epoch': 0.08}
|â–Š         | 433/5197 [42:35:05<487:33:49, 368.44s/it]  8%|â–Š         | 434/5197 [42:41:05<484:11:55, 365.97s/it]                                                            8%|â–Š         | 434/5197 [42:41:05<484:11:55, 365.97s/it]  8%|â–Š         | 435/5197 [42:47:19<487:10:55, 368.30s/it]                                                            8%|â–Š         | 435/5197 [42:47:19<487:10:55, 368.30s/it]  8%|â–Š         | 436/5197 [42:53:37<491:10:14, 371.40s/it]                                                            8%|â–Š         | 436/5197 [42:53:37<491:10:14, 371.40s/it]  8%|â–Š         | 437/5197 [42:59:19<479:22:33, 362.55s/it]                                                            8%|â–Š         | 437/5197 [42:59:19<479:22:33, 362.55s/it]  8%|â–Š         | 438/5197 [43:05:30<482:40:20, 365.12s/it]                                                            8%|â–Š         | 438/5197 [43:05:31<482:40:20, 365.12s/it]  8%|â–Š         | 439/5197 [43:11:30<480:25:50, 363.50s/{'loss': 0.8804, 'learning_rate': 0.0001984487450401542, 'epoch': 0.08}
{'loss': 0.8804, 'learning_rate': 0.0001984487450401542, 'epoch': 0.08}
{'loss': 0.921, 'learning_rate': 0.00019843779142227256, 'epoch': 0.08}
{'loss': 0.921, 'learning_rate': 0.00019843779142227256, 'epoch': 0.08}
{'loss': 0.9774, 'learning_rate': 0.00019842679957228704, 'epoch': 0.08}
{'loss': 0.9774, 'learning_rate': 0.00019842679957228704, 'epoch': 0.08}
{'loss': 0.9517, 'learning_rate': 0.00019841576949446675, 'epoch': 0.09}
{'loss': 0.9517, 'learning_rate': 0.00019841576949446675, 'epoch': 0.09}
{'loss': 0.9522, 'learning_rate': 0.0001984047011930956, 'epoch': 0.09}
{'loss': 0.9522, 'learning_rate': 0.0001984047011930956, 'epoch': 0.09}
      | 438/5197 [43:05:03<482:40:49, 365.13s/it]                                                            8%|â–Š         | 438/5197 [43:05:03<482:40:49, 365.13s/it]  8%|â–Š         | 439/5197 [43:11:03<480:25:27, 363.50s/it]                                                            8%|â–Š         | 439/5197 [43:11:03<480:25:27, 363.50s/it]  8%|â–Š         | 440/5197 [43:17:19<485:10:15, 367.17s/it]                                                            8%|â–Š         | 440/5197 [43:17:19<485:10:15, 367.17s/it]  8%|â–Š         | 441/5197 [43:23:11<479:19:50, 362.82s/it]                                                            8%|â–Š         | 441/5197 [43:23:11<479:19:50, 362.82s/it]  9%|â–Š         | 442/5197 [43:28:59<473:11:28, 358.25s/it]                                                            9%|â–Š         | 442/5197 [43:28:59<473:11:28, 358.25s/it]  9%|â–Š         | 443/5197 [43:35:08<477:25:15, 361.53s/it]                                                            9%{'loss': 0.9962, 'learning_rate': 0.00019839359467247242, 'epoch': 0.09}
{'loss': 0.9962, 'learning_rate': 0.00019839359467247242, 'epoch': 0.09}
it]                                                            8%|â–Š         | 439/5197 [43:11:30<480:25:50, 363.50s/it]  8%|â–Š         | 440/5197 [43:17:46<485:10:31, 367.17s/it]                                                            8%|â–Š         | 440/5197 [43:17:46<485:10:31, 367.17s/it]  8%|â–Š         | 441/5197 [43:23:39<479:20:17, 362.83s/it]                                                            8%|â–Š         | 441/5197 [43:23:39<479:20:17, 362.83s/it]  9%|â–Š         | 442/5197 [43:29:26<473:11:47, 358.26s/it]                                                            9%|â–Š         | 442/5197 [43:29:26<473:11:47, 358.26s/it]  9%|â–Š         | 443/5197 [43:35:35<477:25:29, 361.53s/it]                                                            9%|â–Š         | 443/5197 [43:35:35<477:25:29, 361.53s/it]  9%|â–Š         | 444/5197 [43:41:57<485:21:16, 367.62s/it]                                                            9%|â–Š         | 444/5197 [43:41:57<485:21:16, 3{'loss': 0.9813, 'learning_rate': 0.0001983824499369109, 'epoch': 0.09}
{'loss': 0.9813, 'learning_rate': 0.0001983824499369109, 'epoch': 0.09}
{'loss': 1.0332, 'learning_rate': 0.00019837126699073947, 'epoch': 0.09}
{'loss': 1.0332, 'learning_rate': 0.00019837126699073947, 'epoch': 0.09}
{'loss': 0.934, 'learning_rate': 0.00019836004583830146, 'epoch': 0.09}
{'loss': 0.934, 'learning_rate': 0.00019836004583830146, 'epoch': 0.09}
{'loss': 1.0566, 'learning_rate': 0.00019834878648395505, 'epoch': 0.09}
{'loss': 1.0566, 'learning_rate': 0.00019834878648395505, 'epoch': 0.09}
|â–Š         | 443/5197 [43:35:08<477:25:15, 361.53s/it]  9%|â–Š         | 444/5197 [43:41:30<485:21:03, 367.61s/it]                                                            9%|â–Š         | 444/5197 [43:41:30<485:21:03, 367.61s/it]  9%|â–Š         | 445/5197 [43:47:43<487:35:52, 369.39s/it]                                                            9%|â–Š         | 445/5197 [43:47:43<487:35:52, 369.39s/it]  9%|â–Š         | 446/5197 [43:53:35<480:23:19, 364.01s/it]                                                            9%|â–Š         | 446/5197 [43:53:35<480:23:19, 364.01s/it]  9%|â–Š         | 447/5197 [43:59:43<481:51:22, 365.20s/it]                                                            9%|â–Š         | 447/5197 [43:59:43<481:51:22, 365.20s/it]  9%|â–Š         | 448/5197 [44:05:23<471:47:10, 357.64s/it]                                                            9%|â–Š         | 448/5197 [44:05:23<471:47:10, 357.64s/it]  9%|â–Š         | 449/5197 [44:11:20<471:40:20, 357.63s/{'loss': 0.9229, 'learning_rate': 0.00019833748893207325, 'epoch': 0.09}
{'loss': 0.9229, 'learning_rate': 0.00019833748893207325, 'epoch': 0.09}
67.62s/it]  9%|â–Š         | 445/5197 [43:48:11<487:35:42, 369.39s/it]                                                            9%|â–Š         | 445/5197 [43:48:11<487:35:42, 369.39s/it]  9%|â–Š         | 446/5197 [43:54:02<480:23:03, 364.00s/it]                                                            9%|â–Š         | 446/5197 [43:54:02<480:23:03, 364.00s/it]  9%|â–Š         | 447/5197 [44:00:10<481:50:56, 365.19s/it]                                                            9%|â–Š         | 447/5197 [44:00:10<481:50:56, 365.19s/it]  9%|â–Š         | 448/5197 [44:05:50<471:46:53, 357.64s/it]                                                            9%|â–Š         | 448/5197 [44:05:50<471:46:53, 357.64s/it]  9%|â–Š         | 449/5197 [44:11:48<471:39:57, 357.62s/it]                                                            9%|â–Š         | 449/5197 [44:11:48<471:39:57, 357.62s/it]  9%|â–Š         | 450/5197 [44:17:52<474:01:32, 359.49s/it]                                          {'loss': 0.9417, 'learning_rate': 0.00019832615318704389, 'epoch': 0.09}
{'loss': 0.9417, 'learning_rate': 0.00019832615318704389, 'epoch': 0.09}
{'loss': 0.9723, 'learning_rate': 0.00019831477925326963, 'epoch': 0.09}
{'loss': 0.9723, 'learning_rate': 0.00019831477925326963, 'epoch': 0.09}
{'loss': 0.9402, 'learning_rate': 0.00019830336713516799, 'epoch': 0.09}
{'loss': 0.9402, 'learning_rate': 0.00019830336713516799, 'epoch': 0.09}
{'loss': 1.0134, 'learning_rate': 0.00019829191683717133, 'epoch': 0.09}
{'loss': 1.0134, 'learning_rate': 0.00019829191683717133, 'epoch': 0.09}
{'loss': 0.9303, 'learning_rate': 0.00019828042836372677, 'epoch': 0.09}
{'loss': 0.9303, 'learning_rate': 0.00019828042836372677, 'epoch': 0.09}
it]                                                            9%|â–Š         | 449/5197 [44:11:20<471:40:20, 357.63s/it]  9%|â–Š         | 450/5197 [44:17:24<474:01:39, 359.49s/it]                                                            9%|â–Š         | 450/5197 [44:17:24<474:01:39, 359.49s/it]  9%|â–Š         | 451/5197 [44:23:41<480:39:32, 364.60s/it]                                                            9%|â–Š         | 451/5197 [44:23:41<480:39:32, 364.60s/it]  9%|â–Š         | 452/5197 [44:29:55<484:27:35, 367.56s/it]                                                            9%|â–Š         | 452/5197 [44:29:55<484:27:35, 367.56s/it]  9%|â–Š         | 453/5197 [44:35:58<482:26:52, 366.11s/it]                                                            9%|â–Š         | 453/5197 [44:35:58<482:26:52, 366.11s/it]  9%|â–Š         | 454/5197 [44:42:07<483:34:20, 367.04s/it]                                                            9%|â–Š         | 454/5197 [44:42:07<483:34:20, 3{'loss': 0.9454, 'learning_rate': 0.00019826890171929632, 'epoch': 0.09}
{'loss': 0.9454, 'learning_rate': 0.00019826890171929632, 'epoch': 0.09}
                  9%|â–Š         | 450/5197 [44:17:52<474:01:32, 359.49s/it]  9%|â–Š         | 451/5197 [44:24:08<480:39:19, 364.59s/it]                                                            9%|â–Š         | 451/5197 [44:24:08<480:39:19, 364.59s/it]  9%|â–Š         | 452/5197 [44:30:23<484:27:59, 367.56s/it]                                                            9%|â–Š         | 452/5197 [44:30:23<484:27:59, 367.56s/it]  9%|â–Š         | 453/5197 [44:36:25<482:27:18, 366.11s/it]                                                            9%|â–Š         | 453/5197 [44:36:25<482:27:18, 366.11s/it]  9%|â–Š         | 454/5197 [44:42:35<483:34:29, 367.04s/it]                                                            9%|â–Š         | 454/5197 [44:42:35<483:34:29, 367.04s/it]  9%|â–‰         | 455/5197 [44:48:41<483:18:36, 366.92s/it]                                                            9%|â–‰         | 455/5197 [44:48:41<483:18:36, 366.92s/it]  9%|â–‰         | 456/5197 [44:54:{'loss': 0.9375, 'learning_rate': 0.00019825733690835679, 'epoch': 0.09}
{'loss': 0.9375, 'learning_rate': 0.00019825733690835679, 'epoch': 0.09}
{'loss': 0.9338, 'learning_rate': 0.00019824573393539984, 'epoch': 0.09}
{'loss': 0.9338, 'learning_rate': 0.00019824573393539984, 'epoch': 0.09}
{'loss': 0.9741, 'learning_rate': 0.0001982340928049319, 'epoch': 0.09}
{'loss': 0.9741, 'learning_rate': 0.0001982340928049319, 'epoch': 0.09}
{'loss': 0.8696, 'learning_rate': 0.00019822241352147427, 'epoch': 0.09}
{'loss': 0.8696, 'learning_rate': 0.00019822241352147427, 'epoch': 0.09}
67.04s/it]  9%|â–‰         | 455/5197 [44:48:14<483:18:18, 366.91s/it]                                                            9%|â–‰         | 455/5197 [44:48:14<483:18:18, 366.91s/it]  9%|â–‰         | 456/5197 [44:54:28<485:55:09, 368.97s/it]                                                            9%|â–‰         | 456/5197 [44:54:28<485:55:09, 368.97s/it]  9%|â–‰         | 457/5197 [45:00:12<476:04:53, 361.58s/it]                                                            9%|â–‰         | 457/5197 [45:00:12<476:04:53, 361.58s/it]  9%|â–‰         | 458/5197 [45:06:13<475:38:26, 361.32s/it]                                                            9%|â–‰         | 458/5197 [45:06:13<475:38:26, 361.32s/it]  9%|â–‰         | 459/5197 [45:12:16<476:23:30, 361.97s/it]                                                            9%|â–‰         | 459/5197 [45:12:16<476:23:30, 361.97s/it]  9%|â–‰         | 460/5197 [45:18:06<471:31:21, 358.35s/it]                                          {'loss': 1.0171, 'learning_rate': 0.00019821069608956307, 'epoch': 0.09}
{'loss': 1.0171, 'learning_rate': 0.00019821069608956307, 'epoch': 0.09}
{'loss': 0.8819, 'learning_rate': 0.00019819894051374915, 'epoch': 0.09}
{'loss': 0.8819, 'learning_rate': 0.00019819894051374915, 'epoch': 0.09}
55<485:54:59, 368.97s/it]                                                            9%|â–‰         | 456/5197 [44:54:55<485:54:59, 368.97s/it]  9%|â–‰         | 457/5197 [45:00:39<476:04:41, 361.58s/it]                                                            9%|â–‰         | 457/5197 [45:00:39<476:04:41, 361.58s/it]  9%|â–‰         | 458/5197 [45:06:40<475:37:49, 361.31s/it]                                                            9%|â–‰         | 458/5197 [45:06:40<475:37:49, 361.31s/it]  9%|â–‰         | 459/5197 [45:12:43<476:23:40, 361.97s/it]                                                            9%|â–‰         | 459/5197 [45:12:43<476:23:40, 361.97s/it]  9%|â–‰         | 460/5197 [45:18:33<471:31:29, 358.35s/it]                                                            9%|â–‰         | 460/5197 [45:18:33<471:31:29, 358.35s/it]  9%|â–‰         | 461/5197 [45:24:21<467:12:09, 355.14s/it]                                                            9%|â–‰         | 461/5197 {'loss': 1.0082, 'learning_rate': 0.0001981871467985983, 'epoch': 0.09}
{'loss': 1.0082, 'learning_rate': 0.0001981871467985983, 'epoch': 0.09}
{'loss': 0.903, 'learning_rate': 0.00019817531494869105, 'epoch': 0.09}
{'loss': 0.903, 'learning_rate': 0.00019817531494869105, 'epoch': 0.09}
{'loss': 0.9477, 'learning_rate': 0.00019816344496862272, 'epoch': 0.09}
{'loss': 0.9477, 'learning_rate': 0.00019816344496862272, 'epoch': 0.09}
[2023-11-24 07:10:55,629] [WARNING] [stage3.py:1850:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0196, 'learning_rate': 0.00019815153686300352, 'epoch': 0.09}
{'loss': 1.0196, 'learning_rate': 0.00019815153686300352, 'epoch': 0.09}
                  9%|â–‰         | 460/5197 [45:18:06<471:31:21, 358.35s/it]  9%|â–‰         | 461/5197 [45:23:54<467:12:02, 355.14s/it]                                                            9%|â–‰         | 461/5197 [45:23:54<467:12:02, 355.14s/it]  9%|â–‰         | 462/5197 [45:29:54<469:20:50, 356.84s/it]                                                            9%|â–‰         | 462/5197 [45:29:54<469:20:50, 356.84s/it]  9%|â–‰         | 463/5197 [45:35:43<466:02:55, 354.41s/it]                                                            9%|â–‰         | 463/5197 [45:35:43<466:02:55, 354.41s/it]  9%|â–‰         | 464/5197 [45:41:53<471:53:28, 358.93s/it]                                                            9%|â–‰         | 464/5197 [45:41:53<471:53:28, 358.93s/it]  9%|â–‰         | 465/5197 [45:48:01<475:28:10, 361.73s/it]                                                            9%|â–‰         | 465/5197 [45:48:01<475:28:10, 361.73s/it]  9%|â–‰         | 466/5197 [45:54:{'loss': 0.8762, 'learning_rate': 0.0001981395906364584, 'epoch': 0.09}
{'loss': 0.8762, 'learning_rate': 0.0001981395906364584, 'epoch': 0.09}
[45:24:21<467:12:09, 355.14s/it]  9%|â–‰         | 462/5197 [45:30:22<469:21:00, 356.84s/it]                                                            9%|â–‰         | 462/5197 [45:30:22<469:21:00, 356.84s/it]  9%|â–‰         | 463/5197 [45:36:11<466:03:12, 354.41s/it]                                                            9%|â–‰         | 463/5197 [45:36:11<466:03:12, 354.41s/it]  9%|â–‰         | 464/5197 [45:42:20<471:53:40, 358.93s/it]                                                            9%|â–‰         | 464/5197 [45:42:20<471:53:40, 358.93s/it]  9%|â–‰         | 465/5197 [45:48:28<475:27:58, 361.72s/it]                                                            9%|â–‰         | 465/5197 [45:48:28<475:27:58, 361.72s/it]  9%|â–‰         | 466/5197 [45:54:34<476:52:46, 362.88s/it]                                                            9%|â–‰         | 466/5197 [45:54:34<476:52:46, 362.88s/it]  9%|â–‰         | 467/5197 [46:00:30<474:18:06, 360.99s/it]                    {'loss': 0.9156, 'learning_rate': 0.00019812760629362716, 'epoch': 0.09}
{'loss': 0.9156, 'learning_rate': 0.00019812760629362716, 'epoch': 0.09}
{'loss': 0.9976, 'learning_rate': 0.0001981155838391643, 'epoch': 0.09}
{'loss': 0.9976, 'learning_rate': 0.0001981155838391643, 'epoch': 0.09}
{'loss': 0.9574, 'learning_rate': 0.00019810352327773935, 'epoch': 0.09}
{'loss': 0.9574, 'learning_rate': 0.00019810352327773935, 'epoch': 0.09}
{'loss': 0.9693, 'learning_rate': 0.00019809142461403633, 'epoch': 0.09}
{'loss': 0.9693, 'learning_rate': 0.00019809142461403633, 'epoch': 0.09}
{'loss': 0.9282, 'learning_rate': 0.00019807928785275434, 'epoch': 0.09}
{'loss': 0.9282, 'learning_rate': 0.00019807928785275434, 'epoch': 0.09}
06<476:52:35, 362.87s/it]                                                            9%|â–‰         | 466/5197 [45:54:06<476:52:35, 362.87s/it]  9%|â–‰         | 467/5197 [46:00:03<474:17:54, 360.99s/it]                                                            9%|â–‰         | 467/5197 [46:00:03<474:17:54, 360.99s/it]  9%|â–‰         | 468/5197 [46:06:03<473:56:07, 360.79s/it]                                                            9%|â–‰         | 468/5197 [46:06:03<473:56:07, 360.79s/it]  9%|â–‰         | 469/5197 [46:11:55<470:08:23, 357.97s/it]                                                            9%|â–‰         | 469/5197 [46:11:55<470:08:23, 357.97s/it]  9%|â–‰         | 470/5197 [46:17:45<467:10:19, 355.79s/it]                                                            9%|â–‰         | 470/5197 [46:17:45<467:10:19, 355.79s/it]  9%|â–‰         | 471/5197 [46:23:10<454:49:40, 346.46s/it]                                                            9%|â–‰         | 471/5197 {'loss': 1.0013, 'learning_rate': 0.0001980671129986071, 'epoch': 0.09}
{'loss': 1.0013, 'learning_rate': 0.0001980671129986071, 'epoch': 0.09}
                                        9%|â–‰         | 467/5197 [46:00:30<474:18:06, 360.99s/it]  9%|â–‰         | 468/5197 [46:06:31<473:56:29, 360.79s/it]                                                            9%|â–‰         | 468/5197 [46:06:31<473:56:29, 360.79s/it]  9%|â–‰         | 469/5197 [46:12:22<470:08:57, 357.98s/it]                                                            9%|â–‰         | 469/5197 [46:12:22<470:08:57, 357.98s/it]  9%|â–‰         | 470/5197 [46:18:13<467:11:01, 355.80s/it]                                                            9%|â–‰         | 470/5197 [46:18:13<467:11:01, 355.80s/it]  9%|â–‰         | 471/5197 [46:23:38<454:50:08, 346.47s/it]                                                            9%|â–‰         | 471/5197 [46:23:38<454:50:08, 346.47s/it]  9%|â–‰         | 472/5197 [46:28:50<441:08:29, 336.11s/it]                                                            9%|â–‰         | 472/5197 [46:28:50<441:08:29, 336.11s/it]  9%|â–‰     {'loss': 0.993, 'learning_rate': 0.0001980549000563232, 'epoch': 0.09}
{'loss': 0.993, 'learning_rate': 0.0001980549000563232, 'epoch': 0.09}
{'loss': 0.9825, 'learning_rate': 0.000198042649030646, 'epoch': 0.09}
{'loss': 0.9825, 'learning_rate': 0.000198042649030646, 'epoch': 0.09}
{'loss': 0.9654, 'learning_rate': 0.00019803035992633366, 'epoch': 0.09}
{'loss': 0.9654, 'learning_rate': 0.00019803035992633366, 'epoch': 0.09}
{'loss': 0.9677, 'learning_rate': 0.00019801803274815917, 'epoch': 0.09}
{'loss': 0.9677, 'learning_rate': 0.00019801803274815917, 'epoch': 0.09}
{'loss': 0.9213, 'learning_rate': 0.00019800566750091016, 'epoch': 0.09}
[46:23:10<454:49:40, 346.46s/it]  9%|â–‰         | 472/5197 [46:28:22<441:08:09, 336.10s/it]                                                            9%|â–‰         | 472/5197 [46:28:22<441:08:09, 336.10s/it]  9%|â–‰         | 473/5197 [46:33:40<433:51:27, 330.63s/it]                                                            9%|â–‰         | 473/5197 [46:33:40<433:51:27, 330.63s/it]  9%|â–‰         | 474/5197 [46:38:40<421:55:23, 321.60s/it]                                                            9%|â–‰         | 474/5197 [46:38:40<421:55:23, 321.60s/it]  9%|â–‰         | 475/5197 [46:43:37<412:07:11, 314.20s/it]                                                            9%|â–‰         | 475/5197 [46:43:37<412:07:11, 314.20s/it]  9%|â–‰         | 476/5197 [46:49:07<418:12:06, 318.90s/it]                                                            9%|â–‰         | 476/5197 [46:49:07<418:12:06, 318.90s/it]  9%|â–‰         | 477/5197 [46:54:35<421:32:39, 321.52s/it]                    {'loss': 0.9213, 'learning_rate': 0.00019800566750091016, 'epoch': 0.09}
WARNING: tokenization mismatch: 1 vs. 64. (ignored)
{'loss': 0.9471, 'learning_rate': 0.00019799326418938924, 'epoch': 0.09}
{'loss': 0.9471, 'learning_rate': 0.00019799326418938924, 'epoch': 0.09}
    | 473/5197 [46:34:07<433:51:43, 330.63s/it]                                                            9%|â–‰         | 473/5197 [46:34:07<433:51:43, 330.63s/it]  9%|â–‰         | 474/5197 [46:39:08<421:55:23, 321.60s/it]                                                            9%|â–‰         | 474/5197 [46:39:08<421:55:23, 321.60s/it]  9%|â–‰         | 475/5197 [46:44:05<412:07:09, 314.20s/it]                                                            9%|â–‰         | 475/5197 [46:44:05<412:07:09, 314.20s/it]  9%|â–‰         | 476/5197 [46:49:35<418:12:01, 318.90s/it]                                                            9%|â–‰         | 476/5197 [46:49:35<418:12:01, 318.90s/it]  9%|â–‰         | 477/5197 [46:55:02<421:32:34, 321.52s/it]                                                            9%|â–‰         | 477/5197 [46:55:02<421:32:34, 321.52s/it]  9%|â–‰         | 478/5197 [47:00:27<422:50:59, 322.58s/it]                                                            9%|â{'loss': 0.9704, 'learning_rate': 0.0001979808228184137, 'epoch': 0.09}
{'loss': 0.9704, 'learning_rate': 0.0001979808228184137, 'epoch': 0.09}
{'loss': 0.9288, 'learning_rate': 0.0001979683433928156, 'epoch': 0.09}
{'loss': 0.9288, 'learning_rate': 0.0001979683433928156, 'epoch': 0.09}
{'loss': 0.9617, 'learning_rate': 0.0001979558259174418, 'epoch': 0.09}
{'loss': 0.9617, 'learning_rate': 0.0001979558259174418, 'epoch': 0.09}
{'loss': 0.984, 'learning_rate': 0.00019794327039715395, 'epoch': 0.09}
{'loss': 0.984, 'learning_rate': 0.00019794327039715395, 'epoch': 0.09}
                                        9%|â–‰         | 477/5197 [46:54:35<421:32:39, 321.52s/it]  9%|â–‰         | 478/5197 [47:00:00<422:51:01, 322.58s/it]                                                            9%|â–‰         | 478/5197 [47:00:00<422:51:01, 322.58s/it]  9%|â–‰         | 479/5197 [47:05:06<416:05:23, 317.49s/it]                                                            9%|â–‰         | 479/5197 [47:05:06<416:05:23, 317.49s/it]  9%|â–‰         | 480/5197 [47:10:26<417:18:57, 318.49s/it]                                                            9%|â–‰         | 480/5197 [47:10:26<417:18:57, 318.49s/it]  9%|â–‰         | 481/5197 [47:15:32<412:05:45, 314.58s/it]                                                            9%|â–‰         | 481/5197 [47:15:32<412:05:45, 314.58s/it]  9%|â–‰         | 482/5197 [47:20:53<414:39:36, 316.60s/it]                                                            9%|â–‰         | 482/5197 [47:20:53<414:39:36, 316.60s/it]  9%|â–‰     {'loss': 0.9612, 'learning_rate': 0.0001979306768368285, 'epoch': 0.09}
{'loss': 0.9612, 'learning_rate': 0.0001979306768368285, 'epoch': 0.09}
–‰         | 478/5197 [47:00:27<422:50:59, 322.58s/it]  9%|â–‰         | 479/5197 [47:05:33<416:05:17, 317.49s/it]                                                            9%|â–‰         | 479/5197 [47:05:33<416:05:17, 317.49s/it]  9%|â–‰         | 480/5197 [47:10:54<417:18:53, 318.49s/it]                                                            9%|â–‰         | 480/5197 [47:10:54<417:18:53, 318.49s/it]  9%|â–‰         | 481/5197 [47:15:59<412:05:17, 314.57s/it]                                                            9%|â–‰         | 481/5197 [47:15:59<412:05:17, 314.57s/it]  9%|â–‰         | 482/5197 [47:21:21<414:39:10, 316.60s/it]                                                            9%|â–‰         | 482/5197 [47:21:21<414:39:10, 316.60s/it]  9%|â–‰         | 483/5197 [47:26:45<417:36:23, 318.92s/it]                                                            9%|â–‰         | 483/5197 [47:26:45<417:36:23, 318.92s/it]  9%|â–‰         | 484/5197 [47:32:19<423:17:57, 323.33s/it{'loss': 0.9705, 'learning_rate': 0.0001979180452413566, 'epoch': 0.09}
{'loss': 0.9705, 'learning_rate': 0.0001979180452413566, 'epoch': 0.09}
{'loss': 0.8332, 'learning_rate': 0.00019790537561564428, 'epoch': 0.09}
{'loss': 0.8332, 'learning_rate': 0.00019790537561564428, 'epoch': 0.09}
{'loss': 0.9172, 'learning_rate': 0.00019789266796461222, 'epoch': 0.09}
{'loss': 0.9172, 'learning_rate': 0.00019789266796461222, 'epoch': 0.09}
{'loss': 0.9696, 'learning_rate': 0.00019787992229319592, 'epoch': 0.09}
{'loss': 0.9696, 'learning_rate': 0.00019787992229319592, 'epoch': 0.09}
{'loss': 0.9495, 'learning_rate': 0.00019786713860634567, 'epoch': 0.09}
{'loss': 0.9495, 'learning_rate': 0.00019786713860634567, 'epoch': 0.09}
    | 483/5197 [47:26:17<417:36:43, 318.92s/it]                                                            9%|â–‰         | 483/5197 [47:26:17<417:36:43, 318.92s/it]  9%|â–‰         | 484/5197 [47:31:51<423:18:12, 323.34s/it]                                                            9%|â–‰         | 484/5197 [47:31:51<423:18:12, 323.34s/it]  9%|â–‰         | 485/5197 [47:37:18<424:35:16, 324.39s/it]                                                            9%|â–‰         | 485/5197 [47:37:18<424:35:16, 324.39s/it]  9%|â–‰         | 486/5197 [47:42:45<425:33:03, 325.19s/it]                                                            9%|â–‰         | 486/5197 [47:42:45<425:33:03, 325.19s/it]  9%|â–‰         | 487/5197 [47:48:06<423:58:35, 324.06s/it]                                                            9%|â–‰         | 487/5197 [47:48:06<423:58:35, 324.06s/it]  9%|â–‰         | 488/5197 [47:53:36<426:14:03, 325.85s/it]                                                            9%|â{'loss': 0.3282, 'learning_rate': 0.00019785431690902652, 'epoch': 0.09}
{'loss': 0.3282, 'learning_rate': 0.00019785431690902652, 'epoch': 0.09}
]                                                            9%|â–‰         | 484/5197 [47:32:19<423:17:57, 323.33s/it]  9%|â–‰         | 485/5197 [47:37:45<424:35:10, 324.39s/it]                                                            9%|â–‰         | 485/5197 [47:37:45<424:35:10, 324.39s/it]  9%|â–‰         | 486/5197 [47:43:12<425:33:08, 325.19s/it]                                                            9%|â–‰         | 486/5197 [47:43:12<425:33:08, 325.19s/it]  9%|â–‰         | 487/5197 [47:48:34<423:58:56, 324.06s/it]                                                            9%|â–‰         | 487/5197 [47:48:34<423:58:56, 324.06s/it]  9%|â–‰         | 488/5197 [47:54:04<426:14:29, 325.86s/it]                                                            9%|â–‰         | 488/5197 [47:54:04<426:14:29, 325.86s/it]  9%|â–‰         | 489/5197 [47:59:30<426:15:08, 325.94s/it]                                                            9%|â–‰         | 489/5197 [47:59:30<426:15:08, 325{'loss': 0.9766, 'learning_rate': 0.00019784145720621826, 'epoch': 0.09}
{'loss': 0.9766, 'learning_rate': 0.00019784145720621826, 'epoch': 0.09}
{'loss': 0.9653, 'learning_rate': 0.00019782855950291542, 'epoch': 0.09}
{'loss': 0.9653, 'learning_rate': 0.00019782855950291542, 'epoch': 0.09}
{'loss': 0.931, 'learning_rate': 0.0001978156238041274, 'epoch': 0.09}
{'loss': 0.931, 'learning_rate': 0.0001978156238041274, 'epoch': 0.09}
{'loss': 0.9814, 'learning_rate': 0.0001978026501148782, 'epoch': 0.09}
{'loss': 0.9814, 'learning_rate': 0.0001978026501148782, 'epoch': 0.09}
–‰         | 488/5197 [47:53:36<426:14:03, 325.85s/it]  9%|â–‰         | 489/5197 [47:59:03<426:14:34, 325.93s/it]                                                            9%|â–‰         | 489/5197 [47:59:03<426:14:34, 325.93s/it]  9%|â–‰         | 490/5197 [48:04:31<427:11:53, 326.73s/it]                                                            9%|â–‰         | 490/5197 [48:04:31<427:11:53, 326.73s/it]  9%|â–‰         | 491/5197 [48:10:00<427:59:36, 327.41s/it]                                                            9%|â–‰         | 491/5197 [48:10:00<427:59:36, 327.41s/it]  9%|â–‰         | 492/5197 [48:15:29<428:22:56, 327.77s/it]                                                            9%|â–‰         | 492/5197 [48:15:29<428:22:56, 327.77s/it]  9%|â–‰         | 493/5197 [48:20:46<424:10:00, 324.62s/it]                                                            9%|â–‰         | 493/5197 [48:20:46<424:10:00, 324.62s/it] 10%|â–‰         | 494/5197 [48:26:02<420:48:16, 322.11s/it{'loss': 1.0308, 'learning_rate': 0.00019778963844020665, 'epoch': 0.1}
{'loss': 1.0308, 'learning_rate': 0.00019778963844020665, 'epoch': 0.1}
.94s/it]  9%|â–‰         | 490/5197 [48:04:59<427:12:06, 326.73s/it]                                                            9%|â–‰         | 490/5197 [48:04:59<427:12:06, 326.73s/it]  9%|â–‰         | 491/5197 [48:10:28<427:59:50, 327.41s/it]                                                            9%|â–‰         | 491/5197 [48:10:28<427:59:50, 327.41s/it]  9%|â–‰         | 492/5197 [48:15:56<428:23:06, 327.78s/it]                                                            9%|â–‰         | 492/5197 [48:15:56<428:23:06, 327.78s/it]  9%|â–‰         | 493/5197 [48:21:14<424:10:12, 324.62s/it]                                                            9%|â–‰         | 493/5197 [48:21:14<424:10:12, 324.62s/it] 10%|â–‰         | 494/5197 [48:26:30<420:48:25, 322.11s/it]                                                           10%|â–‰         | 494/5197 [48:26:30<420:48:25, 322.11s/it] 10%|â–‰         | 495/5197 [48:31:46<418:31:33, 320.44s/it]                                            {'loss': 0.8904, 'learning_rate': 0.00019777658878516639, 'epoch': 0.1}
{'loss': 0.8904, 'learning_rate': 0.00019777658878516639, 'epoch': 0.1}
{'loss': 0.3049, 'learning_rate': 0.0001977635011548257, 'epoch': 0.1}
{'loss': 0.3049, 'learning_rate': 0.0001977635011548257, 'epoch': 0.1}
{'loss': 0.9383, 'learning_rate': 0.0001977503755542677, 'epoch': 0.1}
{'loss': 0.9383, 'learning_rate': 0.0001977503755542677, 'epoch': 0.1}
{'loss': 0.973, 'learning_rate': 0.00019773721198859022, 'epoch': 0.1}
{'loss': 0.973, 'learning_rate': 0.00019773721198859022, 'epoch': 0.1}
{'loss': 1.0222, 'learning_rate': 0.00019772401046290586, 'epoch': 0.1}
{'loss': 1.0222, 'learning_rate': 0.00019772401046290586, 'epoch': 0.1}
]                                                           10%|â–‰         | 494/5197 [48:26:02<420:48:16, 322.11s/it] 10%|â–‰         | 495/5197 [48:31:19<418:31:42, 320.44s/it]                                                           10%|â–‰         | 495/5197 [48:31:19<418:31:42, 320.44s/it] 10%|â–‰         | 496/5197 [48:36:44<420:07:30, 321.73s/it]                                                           10%|â–‰         | 496/5197 [48:36:44<420:07:30, 321.73s/it] 10%|â–‰         | 497/5197 [48:42:12<422:41:41, 323.77s/it]                                                           10%|â–‰         | 497/5197 [48:42:12<422:41:41, 323.77s/it] 10%|â–‰         | 498/5197 [48:47:47<426:58:33, 327.12s/it]                                                           10%|â–‰         | 498/5197 [48:47:47<426:58:33, 327.12s/it] 10%|â–‰         | 499/5197 [48:53:14<426:55:36, 327.15s/it]                                                           10%|â–‰         | 499/5197 [48:53:14<426:55:36, 327{'loss': 0.9764, 'learning_rate': 0.00019771077098234186, 'epoch': 0.1}
{'loss': 0.9764, 'learning_rate': 0.00019771077098234186, 'epoch': 0.1}
               10%|â–‰         | 495/5197 [48:31:46<418:31:33, 320.44s/it] 10%|â–‰         | 496/5197 [48:37:11<420:07:26, 321.73s/it]                                                           10%|â–‰         | 496/5197 [48:37:11<420:07:26, 321.73s/it] 10%|â–‰         | 497/5197 [48:42:40<422:41:37, 323.77s/it]                                                           10%|â–‰         | 497/5197 [48:42:40<422:41:37, 323.77s/it] 10%|â–‰         | 498/5197 [48:48:14<426:58:31, 327.11s/it]                                                           10%|â–‰         | 498/5197 [48:48:14<426:58:31, 327.11s/it] 10%|â–‰         | 499/5197 [48:53:42<426:55:33, 327.15s/it]                                                           10%|â–‰         | 499/5197 [48:53:42<426:55:33, 327.15s/it] 10%|â–‰         | 500/5197 [48:59:01<423:36:39, 324.68s/it]                                                           10%|â–‰         | 500/5197 [48:59:01<423:36:39, 324.68s/it] 10%|â–‰         | 501/5197 [49:04:28{'loss': 0.8889, 'learning_rate': 0.00019769749355204032, 'epoch': 0.1}
{'loss': 0.8889, 'learning_rate': 0.00019769749355204032, 'epoch': 0.1}
{'loss': 0.9223, 'learning_rate': 0.00019768417817715809, 'epoch': 0.1}
{'loss': 0.9223, 'learning_rate': 0.00019768417817715809, 'epoch': 0.1}
{'loss': 0.9573, 'learning_rate': 0.00019767082486286665, 'epoch': 0.1}
{'loss': 0.9573, 'learning_rate': 0.00019767082486286665, 'epoch': 0.1}
{'loss': 0.9951, 'learning_rate': 0.0001976574336143523, 'epoch': 0.1}
{'loss': 0.9951, 'learning_rate': 0.0001976574336143523, 'epoch': 0.1}
{'loss': 0.9283, 'learning_rate': 0.00019764400443681606, 'epoch': 0.1}
.15s/it] 10%|â–‰         | 500/5197 [48:58:33<423:36:42, 324.68s/it]                                                           10%|â–‰         | 500/5197 [48:58:33<423:36:42, 324.68s/it] 10%|â–‰         | 501/5197 [49:04:00<424:29:22, 325.42s/it]                                                           10%|â–‰         | 501/5197 [49:04:00<424:29:22, 325.42s/it] 10%|â–‰         | 502/5197 [49:09:29<425:48:23, 326.50s/it]                                                           10%|â–‰         | 502/5197 [49:09:29<425:48:23, 326.50s/it] 10%|â–‰         | 503/5197 [49:14:59<426:57:23, 327.45s/it]                                                           10%|â–‰         | 503/5197 [49:14:59<426:57:23, 327.45s/it] 10%|â–‰         | 504/5197 [49:20:27<427:13:26, 327.72s/it]                                                           10%|â–‰         | 504/5197 [49:20:27<427:13:26, 327.72s/it] 10%|â–‰         | 505/5197 [49:26:14<434:24:34, 333.31s/it]                                            {'loss': 0.9283, 'learning_rate': 0.00019764400443681606, 'epoch': 0.1}
{'loss': 0.9465, 'learning_rate': 0.00019763053733547366, 'epoch': 0.1}
{'loss': 0.9465, 'learning_rate': 0.00019763053733547366, 'epoch': 0.1}
<424:29:20, 325.42s/it]                                                           10%|â–‰         | 501/5197 [49:04:28<424:29:20, 325.42s/it] 10%|â–‰         | 502/5197 [49:09:57<425:48:24, 326.50s/it]                                                           10%|â–‰         | 502/5197 [49:09:57<425:48:24, 326.50s/it] 10%|â–‰         | 503/5197 [49:15:26<426:57:15, 327.45s/it]                                                           10%|â–‰         | 503/5197 [49:15:26<426:57:15, 327.45s/it] 10%|â–‰         | 504/5197 [49:20:55<427:13:32, 327.72s/it]                                                           10%|â–‰         | 504/5197 [49:20:55<427:13:32, 327.72s/it] 10%|â–‰         | 505/5197 [49:26:41<434:24:32, 333.31s/it]                                                           10%|â–‰         | 505/5197 [49:26:41<434:24:32, 333.31s/it] 10%|â–‰         | 506/5197 [49:32:17<435:12:28, 333.99s/it]                                                           10%|â–‰         | 506/5197 [4{'loss': 0.8482, 'learning_rate': 0.0001976170323155555, 'epoch': 0.1}
{'loss': 0.8482, 'learning_rate': 0.0001976170323155555, 'epoch': 0.1}
{'loss': 1.004, 'learning_rate': 0.0001976034893823069, 'epoch': 0.1}
{'loss': 1.004, 'learning_rate': 0.0001976034893823069, 'epoch': 0.1}
{'loss': 0.9754, 'learning_rate': 0.0001975899085409876, 'epoch': 0.1}
{'loss': 0.9754, 'learning_rate': 0.0001975899085409876, 'epoch': 0.1}
{'loss': 0.9673, 'learning_rate': 0.00019757628979687246, 'epoch': 0.1}
{'loss': 0.9673, 'learning_rate': 0.00019757628979687246, 'epoch': 0.1}
               10%|â–‰         | 505/5197 [49:26:14<434:24:34, 333.31s/it] 10%|â–‰         | 506/5197 [49:31:49<435:12:50, 333.99s/it]                                                           10%|â–‰         | 506/5197 [49:31:49<435:12:50, 333.99s/it] 10%|â–‰         | 507/5197 [49:37:23<434:48:33, 333.76s/it]                                                           10%|â–‰         | 507/5197 [49:37:23<434:48:33, 333.76s/it] 10%|â–‰         | 508/5197 [49:42:55<434:15:09, 333.40s/it]                                                           10%|â–‰         | 508/5197 [49:42:55<434:15:09, 333.40s/it] 10%|â–‰         | 509/5197 [49:48:17<429:30:47, 329.83s/it]                                                           10%|â–‰         | 509/5197 [49:48:17<429:30:47, 329.83s/it] 10%|â–‰         | 510/5197 [49:53:53<432:00:26, 331.82s/it]                                                           10%|â–‰         | 510/5197 [49:53:53<432:00:26, 331.82s/it] 10%|â–‰         | 511/5197 [49:59:20{'loss': 0.9344, 'learning_rate': 0.0001975626331552507, 'epoch': 0.1}
{'loss': 0.9344, 'learning_rate': 0.0001975626331552507, 'epoch': 0.1}
9:32:17<435:12:28, 333.99s/it] 10%|â–‰         | 507/5197 [49:37:50<434:47:59, 333.75s/it]                                                           10%|â–‰         | 507/5197 [49:37:50<434:47:59, 333.75s/it] 10%|â–‰         | 508/5197 [49:43:22<434:14:42, 333.39s/it]                                                           10%|â–‰         | 508/5197 [49:43:22<434:14:42, 333.39s/it] 10%|â–‰         | 509/5197 [49:48:44<429:30:25, 329.83s/it]                                                           10%|â–‰         | 509/5197 [49:48:44<429:30:25, 329.83s/it] 10%|â–‰         | 510/5197 [49:54:20<431:59:56, 331.81s/it]                                                           10%|â–‰         | 510/5197 [49:54:20<431:59:56, 331.81s/it] 10%|â–‰         | 511/5197 [49:59:48<430:08:39, 330.46s/it]                                                           10%|â–‰         | 511/5197 [49:59:48<430:08:39, 330.46s/it] 10%|â–‰         | 512/5197 [50:05:24<432:26:24, 332.29s/it]                      {'loss': 0.3068, 'learning_rate': 0.00019754893862142643, 'epoch': 0.1}
{'loss': 0.3068, 'learning_rate': 0.00019754893862142643, 'epoch': 0.1}
{'loss': 1.0068, 'learning_rate': 0.00019753520620071843, 'epoch': 0.1}
{'loss': 1.0068, 'learning_rate': 0.00019753520620071843, 'epoch': 0.1}
{'loss': 0.8863, 'learning_rate': 0.0001975214358984603, 'epoch': 0.1}
{'loss': 0.8863, 'learning_rate': 0.0001975214358984603, 'epoch': 0.1}
{'loss': 0.9428, 'learning_rate': 0.00019750762772000014, 'epoch': 0.1}
{'loss': 0.9428, 'learning_rate': 0.00019750762772000014, 'epoch': 0.1}
{'loss': 0.8862, 'learning_rate': 0.000197493781670701, 'epoch': 0.1}
{'loss': 0.8862, 'learning_rate': 0.000197493781670701, 'epoch': 0.1}
<430:08:58, 330.46s/it]                                                           10%|â–‰         | 511/5197 [49:59:20<430:08:58, 330.46s/it] 10%|â–‰         | 512/5197 [50:04:57<432:26:19, 332.29s/it]                                                           10%|â–‰         | 512/5197 [50:04:57<432:26:19, 332.29s/it] 10%|â–‰         | 513/5197 [50:10:46<438:59:58, 337.40s/it]                                                           10%|â–‰         | 513/5197 [50:10:46<438:59:58, 337.40s/it] 10%|â–‰         | 514/5197 [50:16:57<451:50:39, 347.35s/it]                                                           10%|â–‰         | 514/5197 [50:16:57<451:50:39, 347.35s/it] 10%|â–‰         | 515/5197 [50:22:54<455:38:41, 350.35s/it]                                                           10%|â–‰         | 515/5197 [50:22:54<455:38:41, 350.35s/it] 10%|â–‰         | 516/5197 [50:28:26<448:15:42, 344.74s/it]                                                           10%|â–‰         | 516/5197 [5{'loss': 0.965, 'learning_rate': 0.00019747989775594044, 'epoch': 0.1}
{'loss': 0.965, 'learning_rate': 0.00019747989775594044, 'epoch': 0.1}
                                     10%|â–‰         | 512/5197 [50:05:24<432:26:24, 332.29s/it] 10%|â–‰         | 513/5197 [50:11:14<438:59:59, 337.40s/it]                                                           10%|â–‰         | 513/5197 [50:11:14<438:59:59, 337.40s/it] 10%|â–‰         | 514/5197 [50:17:24<451:50:43, 347.35s/it]                                                           10%|â–‰         | 514/5197 [50:17:24<451:50:43, 347.35s/it] 10%|â–‰         | 515/5197 [50:23:22<455:38:47, 350.35s/it]                                                           10%|â–‰         | 515/5197 [50:23:22<455:38:47, 350.35s/it] 10%|â–‰         | 516/5197 [50:28:53<448:15:47, 344.74s/it]                                                           10%|â–‰         | 516/5197 [50:28:53<448:15:47, 344.74s/it] 10%|â–‰         | 517/5197 [50:34:27<444:01:33, 341.56s/it]                                                           10%|â–‰         | 517/5197 [50:34:27<444:01:33, 341.56s/it] 10%|â–‰       {'loss': 0.9353, 'learning_rate': 0.0001974659759811109, 'epoch': 0.1}
{'loss': 0.9353, 'learning_rate': 0.0001974659759811109, 'epoch': 0.1}
{'loss': 1.0008, 'learning_rate': 0.00019745201635161936, 'epoch': 0.1}
{'loss': 1.0008, 'learning_rate': 0.00019745201635161936, 'epoch': 0.1}
{'loss': 0.909, 'learning_rate': 0.00019743801887288763, 'epoch': 0.1}
{'loss': 0.909, 'learning_rate': 0.00019743801887288763, 'epoch': 0.1}
{'loss': 0.959, 'learning_rate': 0.0001974239835503521, 'epoch': 0.1}
{'loss': 0.959, 'learning_rate': 0.0001974239835503521, 'epoch': 0.1}
{'loss': 0.2481, 'learning_rate': 0.00019740991038946404, 'epoch': 0.1}
0:28:26<448:15:42, 344.74s/it] 10%|â–‰         | 517/5197 [50:34:00<444:01:28, 341.56s/it]                                                           10%|â–‰         | 517/5197 [50:34:00<444:01:28, 341.56s/it] 10%|â–‰         | 518/5197 [50:39:35<441:34:58, 339.75s/it]                                                           10%|â–‰         | 518/5197 [50:39:35<441:34:58, 339.75s/it] 10%|â–‰         | 519/5197 [50:45:01<436:07:33, 335.63s/it]                                                           10%|â–‰         | 519/5197 [50:45:01<436:07:33, 335.63s/it] 10%|â–ˆ         | 520/5197 [50:50:37<435:56:35, 335.56s/it]                                                           10%|â–ˆ         | 520/5197 [50:50:37<435:56:35, 335.56s/it] 10%|â–ˆ         | 521/5197 [50:56:14<436:37:28, 336.15s/it]                                                           10%|â–ˆ         | 521/5197 [50:56:14<436:37:28, 336.15s/it] 10%|â–ˆ         | 522/5197 [51:01:37<431:26:20, 332.23s/it]                      {'loss': 0.2481, 'learning_rate': 0.00019740991038946404, 'epoch': 0.1}
{'loss': 0.965, 'learning_rate': 0.0001973957993956892, 'epoch': 0.1}
  | 518/5197 [50:40:03<441:35:09, 339.75s/it]                                                           10%|â–‰         | 518/5197 [50:40:03<441:35:09, 339.75s/it] 10%|â–‰         | 519/5197 [50:45:29<436:07:44, 335.63s/it]                                                           10%|â–‰         | 519/5197 [50:45:29<436:07:44, 335.63s/it] 10%|â–ˆ         | 520/5197 [50:51:04<435:56:48, 335.56s/it]                                                           10%|â–ˆ         | 520/5197 [50:51:04<435:56:48, 335.56s/it] 10%|â–ˆ         | 521/5197 [50:56:42<436:37:34, 336.15s/it]                                                           10%|â–ˆ         | 521/5197 [50:56:42<436:37:34, 336.15s/it] 10%|â–ˆ         | 522/5197 [51:02:05<431:26:25, 332.23s/it]                                                           10%|â–ˆ         | 522/5197 [51:02:05<431:26:25, 332.23s/it] 10%|â–ˆ         | 523/5197 [51:07:44<434:12:34, 334.44s/it]                                                           10%|â–ˆ{'loss': 0.965, 'learning_rate': 0.0001973957993956892, 'epoch': 0.1}
{'loss': 0.9669, 'learning_rate': 0.00019738165057450816, 'epoch': 0.1}
{'loss': 0.9669, 'learning_rate': 0.00019738165057450816, 'epoch': 0.1}
{'loss': 0.9733, 'learning_rate': 0.00019736746393141617, 'epoch': 0.1}
{'loss': 0.9733, 'learning_rate': 0.00019736746393141617, 'epoch': 0.1}
{'loss': 0.9731, 'learning_rate': 0.00019735323947192316, 'epoch': 0.1}
{'loss': 0.9731, 'learning_rate': 0.00019735323947192316, 'epoch': 0.1}
{'loss': 0.9652, 'learning_rate': 0.00019733897720155375, 'epoch': 0.1}
{'loss': 0.9652, 'learning_rate': 0.00019733897720155375, 'epoch': 0.1}
                                     10%|â–ˆ         | 522/5197 [51:01:37<431:26:20, 332.23s/it] 10%|â–ˆ         | 523/5197 [51:07:17<434:12:31, 334.44s/it]                                                           10%|â–ˆ         | 523/5197 [51:07:17<434:12:31, 334.44s/it] 10%|â–ˆ         | 524/5197 [51:12:47<432:16:16, 333.01s/it]                                                           10%|â–ˆ         | 524/5197 [51:12:47<432:16:16, 333.01s/it] 10%|â–ˆ         | 525/5197 [51:18:25<434:09:07, 334.53s/it]                                                           10%|â–ˆ         | 525/5197 [51:18:25<434:09:07, 334.53s/it] 10%|â–ˆ         | 526/5197 [51:24:11<438:26:46, 337.92s/it]                                                           10%|â–ˆ         | 526/5197 [51:24:11<438:26:46, 337.92s/it] 10%|â–ˆ         | 527/5197 [51:29:51<439:21:16, 338.69s/it]                                                           10%|â–ˆ         | 527/5197 [51:29:51<439:21:16, 338.69s/it] 10%|â–ˆ       {'loss': 0.9431, 'learning_rate': 0.00019732467712584722, 'epoch': 0.1}
{'loss': 0.9431, 'learning_rate': 0.00019732467712584722, 'epoch': 0.1}
         | 523/5197 [51:07:44<434:12:34, 334.44s/it] 10%|â–ˆ         | 524/5197 [51:13:14<432:16:32, 333.02s/it]                                                           10%|â–ˆ         | 524/5197 [51:13:14<432:16:32, 333.02s/it] 10%|â–ˆ         | 525/5197 [51:18:52<434:09:22, 334.54s/it]                                                           10%|â–ˆ         | 525/5197 [51:18:52<434:09:22, 334.54s/it] 10%|â–ˆ         | 526/5197 [51:24:38<438:27:00, 337.92s/it]                                                           10%|â–ˆ         | 526/5197 [51:24:38<438:27:00, 337.92s/it] 10%|â–ˆ         | 527/5197 [51:30:19<439:21:23, 338.69s/it]                                                           10%|â–ˆ         | 527/5197 [51:30:19<439:21:23, 338.69s/it] 10%|â–ˆ         | 528/5197 [51:36:04<441:57:41, 340.77s/it]                                                           10%|â–ˆ         | 528/5197 [51:36:04<441:57:41, 340.77s/it] 10%|â–ˆ         | 529/5197 [51:41:39<439:34:32, 339.00s/it]{'loss': 1.0172, 'learning_rate': 0.0001973103392503576, 'epoch': 0.1}
{'loss': 1.0172, 'learning_rate': 0.0001973103392503576, 'epoch': 0.1}
{'loss': 0.8509, 'learning_rate': 0.00019729596358065345, 'epoch': 0.1}
{'loss': 0.8509, 'learning_rate': 0.00019729596358065345, 'epoch': 0.1}
{'loss': 0.9673, 'learning_rate': 0.00019728155012231825, 'epoch': 0.1}
{'loss': 0.9673, 'learning_rate': 0.00019728155012231825, 'epoch': 0.1}
{'loss': 0.935, 'learning_rate': 0.00019726709888094992, 'epoch': 0.1}
{'loss': 0.935, 'learning_rate': 0.00019726709888094992, 'epoch': 0.1}
{'loss': 0.9604, 'learning_rate': 0.0001972526098621612, 'epoch': 0.1}
{'loss': 0.9604, 'learning_rate': 0.0001972526098621612, 'epoch': 0.1}
  | 528/5197 [51:35:37<441:57:40, 340.77s/it]                                                           10%|â–ˆ         | 528/5197 [51:35:37<441:57:40, 340.77s/it] 10%|â–ˆ         | 529/5197 [51:41:12<439:34:32, 339.00s/it]                                                           10%|â–ˆ         | 529/5197 [51:41:12<439:34:32, 339.00s/it] 10%|â–ˆ         | 530/5197 [51:46:36<433:37:18, 334.48s/it]                                                           10%|â–ˆ         | 530/5197 [51:46:36<433:37:18, 334.48s/it] 10%|â–ˆ         | 531/5197 [51:52:12<434:12:58, 335.01s/it]                                                           10%|â–ˆ         | 531/5197 [51:52:12<434:12:58, 335.01s/it] 10%|â–ˆ         | 532/5197 [51:57:55<437:06:05, 337.31s/it]                                                           10%|â–ˆ         | 532/5197 [51:57:55<437:06:05, 337.31s/it] 10%|â–ˆ         | 533/5197 [52:03:33<437:25:17, 337.63s/it]                                                           10%|â–ˆ{'loss': 0.9935, 'learning_rate': 0.00019723808307157948, 'epoch': 0.1}
{'loss': 0.9935, 'learning_rate': 0.00019723808307157948, 'epoch': 0.1}
                                                           10%|â–ˆ         | 529/5197 [51:41:39<439:34:32, 339.00s/it] 10%|â–ˆ         | 530/5197 [51:47:03<433:37:11, 334.48s/it]                                                           10%|â–ˆ         | 530/5197 [51:47:03<433:37:11, 334.48s/it] 10%|â–ˆ         | 531/5197 [51:52:39<434:12:30, 335.01s/it]                                                           10%|â–ˆ         | 531/5197 [51:52:39<434:12:30, 335.01s/it] 10%|â–ˆ         | 532/5197 [51:58:22<437:05:41, 337.31s/it]                                                           10%|â–ˆ         | 532/5197 [51:58:22<437:05:41, 337.31s/it] 10%|â–ˆ         | 533/5197 [52:04:00<437:25:14, 337.63s/it]                                                           10%|â–ˆ         | 533/5197 [52:04:00<437:25:14, 337.63s/it] 10%|â–ˆ         | 534/5197 [52:09:35<436:14:51, 336.80s/it]                                                           10%|â–ˆ         | 534/5197 [52:09:35<436:14:51, 336.8{'loss': 0.9129, 'learning_rate': 0.00019722351851484676, 'epoch': 0.1}
{'loss': 0.9129, 'learning_rate': 0.00019722351851484676, 'epoch': 0.1}
{'loss': 0.9229, 'learning_rate': 0.00019720891619761974, 'epoch': 0.1}
{'loss': 0.9229, 'learning_rate': 0.00019720891619761974, 'epoch': 0.1}
{'loss': 0.9111, 'learning_rate': 0.0001971942761255698, 'epoch': 0.1}
{'loss': 0.9111, 'learning_rate': 0.0001971942761255698, 'epoch': 0.1}
{'loss': 0.9348, 'learning_rate': 0.00019717959830438302, 'epoch': 0.1}
{'loss': 0.9348, 'learning_rate': 0.00019717959830438302, 'epoch': 0.1}
{'loss': 0.9329, 'learning_rate': 0.00019716488273976003, 'epoch': 0.1}
         | 533/5197 [52:03:33<437:25:17, 337.63s/it] 10%|â–ˆ         | 534/5197 [52:09:08<436:14:50, 336.80s/it]                                                           10%|â–ˆ         | 534/5197 [52:09:08<436:14:50, 336.80s/it] 10%|â–ˆ         | 535/5197 [52:14:45<436:15:07, 336.87s/it]                                                           10%|â–ˆ         | 535/5197 [52:14:45<436:15:07, 336.87s/it] 10%|â–ˆ         | 536/5197 [52:20:15<433:41:34, 334.97s/it]                                                           10%|â–ˆ         | 536/5197 [52:20:15<433:41:34, 334.97s/it] 10%|â–ˆ         | 537/5197 [52:25:48<432:46:08, 334.33s/it]                                                           10%|â–ˆ         | 537/5197 [52:25:48<432:46:08, 334.33s/it] 10%|â–ˆ         | 538/5197 [52:31:06<426:20:01, 329.43s/it]                                                           10%|â–ˆ         | 538/5197 [52:31:06<426:20:01, 329.43s/it] 10%|â–ˆ         | 539/5197 [52:36:31<424:20:07, 327.95s/it]{'loss': 0.9329, 'learning_rate': 0.00019716488273976003, 'epoch': 0.1}
0s/it] 10%|â–ˆ         | 535/5197 [52:15:12<436:15:22, 336.88s/it]                                                           10%|â–ˆ         | 535/5197 [52:15:12<436:15:22, 336.88s/it] 10%|â–ˆ         | 536/5197 [52:20:43<433:41:47, 334.97s/it]                                                           10%|â–ˆ         | 536/5197 [52:20:43<433:41:47, 334.97s/it] 10%|â–ˆ         | 537/5197 [52:26:16<432:46:19, 334.33s/it]                                                           10%|â–ˆ         | 537/5197 [52:26:16<432:46:19, 334.33s/it] 10%|â–ˆ         | 538/5197 [52:31:34<426:20:09, 329.43s/it]                                                           10%|â–ˆ         | 538/5197 [52:31:34<426:20:09, 329.43s/it] 10%|â–ˆ         | 539/5197 [52:36:58<424:20:15, 327.96s/it]                                                           10%|â–ˆ         | 539/5197 [52:36:58<424:20:15, 327.96s/it] 10%|â–ˆ         | 540/5197 [52:42:34<427:18:29, 330.32s/it]                                              {'loss': 0.9072, 'learning_rate': 0.0001971501294374162, 'epoch': 0.1}
{'loss': 0.9072, 'learning_rate': 0.0001971501294374162, 'epoch': 0.1}
{'loss': 0.9627, 'learning_rate': 0.00019713533840308157, 'epoch': 0.1}
{'loss': 0.9627, 'learning_rate': 0.00019713533840308157, 'epoch': 0.1}
{'loss': 0.9615, 'learning_rate': 0.00019712050964250082, 'epoch': 0.1}
{'loss': 0.9615, 'learning_rate': 0.00019712050964250082, 'epoch': 0.1}
{'loss': 0.9247, 'learning_rate': 0.00019710564316143323, 'epoch': 0.1}
{'loss': 0.9247, 'learning_rate': 0.00019710564316143323, 'epoch': 0.1}
{'loss': 0.9865, 'learning_rate': 0.00019709073896565275, 'epoch': 0.1}
{'loss': 0.9865, 'learning_rate': 0.00019709073896565275, 'epoch': 0.1}
                                                           10%|â–ˆ         | 539/5197 [52:36:31<424:20:07, 327.95s/it] 10%|â–ˆ         | 540/5197 [52:42:07<427:18:31, 330.32s/it]                                                           10%|â–ˆ         | 540/5197 [52:42:07<427:18:31, 330.32s/it] 10%|â–ˆ         | 541/5197 [52:47:30<424:29:17, 328.21s/it]                                                           10%|â–ˆ         | 541/5197 [52:47:30<424:29:17, 328.21s/it] 10%|â–ˆ         | 542/5197 [52:53:20<432:51:00, 334.75s/it]                                                           10%|â–ˆ         | 542/5197 [52:53:20<432:51:00, 334.75s/it] 10%|â–ˆ         | 543/5197 [52:58:51<431:20:14, 333.65s/it]                                                           10%|â–ˆ         | 543/5197 [52:58:51<431:20:14, 333.65s/it] 10%|â–ˆ         | 544/5197 [53:04:31<433:55:42, 335.73s/it]                                                           10%|â–ˆ         | 544/5197 [53:04:31<433:55:42, 335.7{'loss': 0.9994, 'learning_rate': 0.00019707579706094807, 'epoch': 0.1}
{'loss': 0.9994, 'learning_rate': 0.00019707579706094807, 'epoch': 0.1}
             10%|â–ˆ         | 540/5197 [52:42:34<427:18:29, 330.32s/it] 10%|â–ˆ         | 541/5197 [52:47:57<424:29:12, 328.21s/it]                                                           10%|â–ˆ         | 541/5197 [52:47:57<424:29:12, 328.21s/it] 10%|â–ˆ         | 542/5197 [52:53:47<432:50:54, 334.75s/it]                                                           10%|â–ˆ         | 542/5197 [52:53:47<432:50:54, 334.75s/it] 10%|â–ˆ         | 543/5197 [52:59:18<431:20:07, 333.65s/it]                                                           10%|â–ˆ         | 543/5197 [52:59:18<431:20:07, 333.65s/it] 10%|â–ˆ         | 544/5197 [53:04:59<433:55:35, 335.73s/it]                                                           10%|â–ˆ         | 544/5197 [53:04:59<433:55:35, 335.73s/it] 10%|â–ˆ         | 545/5197 [53:10:18<427:16:43, 330.65s/it]                                                           10%|â–ˆ         | 545/5197 [53:10:18<427:16:43, 330.65s/it] 11%|â–ˆ         | 546/5197 [53:15:47<4{'loss': 0.8726, 'learning_rate': 0.0001970608174531224, 'epoch': 0.11}
{'loss': 0.8726, 'learning_rate': 0.0001970608174531224, 'epoch': 0.11}
{'loss': 0.9696, 'learning_rate': 0.0001970458001479937, 'epoch': 0.11}
{'loss': 0.9696, 'learning_rate': 0.0001970458001479937, 'epoch': 0.11}
{'loss': 0.8781, 'learning_rate': 0.00019703074515139445, 'epoch': 0.11}
{'loss': 0.8781, 'learning_rate': 0.00019703074515139445, 'epoch': 0.11}
{'loss': 0.9464, 'learning_rate': 0.00019701565246917183, 'epoch': 0.11}
{'loss': 0.9464, 'learning_rate': 0.00019701565246917183, 'epoch': 0.11}
{'loss': 0.944, 'learning_rate': 0.00019700052210718777, 'epoch': 0.11}
3s/it] 10%|â–ˆ         | 545/5197 [53:09:50<427:16:53, 330.66s/it]                                                           10%|â–ˆ         | 545/5197 [53:09:50<427:16:53, 330.66s/it] 11%|â–ˆ         | 546/5197 [53:15:19<426:33:30, 330.17s/it]                                                           11%|â–ˆ         | 546/5197 [53:15:19<426:33:30, 330.17s/it] 11%|â–ˆ         | 547/5197 [53:20:48<425:50:34, 329.68s/it]                                                           11%|â–ˆ         | 547/5197 [53:20:48<425:50:34, 329.68s/it] 11%|â–ˆ         | 548/5197 [53:26:17<425:41:31, 329.64s/it]                                                           11%|â–ˆ         | 548/5197 [53:26:17<425:41:31, 329.64s/it] 11%|â–ˆ         | 549/5197 [53:31:34<420:21:53, 325.58s/it]                                                           11%|â–ˆ         | 549/5197 [53:31:34<420:21:53, 325.58s/it] 11%|â–ˆ         | 550/5197 [53:37:11<424:47:47, 329.09s/it]                                              {'loss': 0.944, 'learning_rate': 0.00019700052210718777, 'epoch': 0.11}
{'loss': 0.8898, 'learning_rate': 0.00019698535407131862, 'epoch': 0.11}
{'loss': 0.8898, 'learning_rate': 0.00019698535407131862, 'epoch': 0.11}
26:33:24, 330.17s/it]                                                           11%|â–ˆ         | 546/5197 [53:15:47<426:33:24, 330.17s/it] 11%|â–ˆ         | 547/5197 [53:21:15<425:50:33, 329.68s/it]                                                           11%|â–ˆ         | 547/5197 [53:21:15<425:50:33, 329.68s/it] 11%|â–ˆ         | 548/5197 [53:26:45<425:41:25, 329.64s/it]                                                           11%|â–ˆ         | 548/5197 [53:26:45<425:41:25, 329.64s/it] 11%|â–ˆ         | 549/5197 [53:32:01<420:22:01, 325.59s/it]                                                           11%|â–ˆ         | 549/5197 [53:32:01<420:22:01, 325.59s/it] 11%|â–ˆ         | 550/5197 [53:37:38<424:48:01, 329.09s/it]                                                           11%|â–ˆ         | 550/5197 [53:37:38<424:48:01, 329.09s/it] 11%|â–ˆ         | 551/5197 [53:42:46<416:28:37, 322.71s/it]                                                           11%|â–ˆ         | 551/5197 [53:{'loss': 1.0015, 'learning_rate': 0.00019697014836745553, 'epoch': 0.11}
{'loss': 1.0015, 'learning_rate': 0.00019697014836745553, 'epoch': 0.11}
{'loss': 0.932, 'learning_rate': 0.00019695490500150418, 'epoch': 0.11}
{'loss': 0.932, 'learning_rate': 0.00019695490500150418, 'epoch': 0.11}
{'loss': 0.8922, 'learning_rate': 0.00019693962397938496, 'epoch': 0.11}
{'loss': 0.8922, 'learning_rate': 0.00019693962397938496, 'epoch': 0.11}
{'loss': 0.9336, 'learning_rate': 0.00019692430530703282, 'epoch': 0.11}
{'loss': 0.9336, 'learning_rate': 0.00019692430530703282, 'epoch': 0.11}
             11%|â–ˆ         | 550/5197 [53:37:11<424:47:47, 329.09s/it] 11%|â–ˆ         | 551/5197 [53:42:19<416:28:13, 322.71s/it]                                                           11%|â–ˆ         | 551/5197 [53:42:19<416:28:13, 322.71s/it] 11%|â–ˆ         | 552/5197 [53:47:35<413:46:44, 320.69s/it]                                                           11%|â–ˆ         | 552/5197 [53:47:35<413:46:44, 320.69s/it] 11%|â–ˆ         | 553/5197 [53:52:58<414:45:00, 321.51s/it]                                                           11%|â–ˆ         | 553/5197 [53:52:58<414:45:00, 321.51s/it] 11%|â–ˆ         | 554/5197 [53:58:19<414:30:52, 321.40s/it]                                                           11%|â–ˆ         | 554/5197 [53:58:19<414:30:52, 321.40s/it] 11%|â–ˆ         | 555/5197 [54:03:41<414:46:18, 321.67s/it]                                                           11%|â–ˆ         | 555/5197 [54:03:41<414:46:18, 321.67s/it] 11%|â–ˆ         | 556/5197 [54:09:07<4{'loss': 0.9764, 'learning_rate': 0.00019690894899039734, 'epoch': 0.11}
{'loss': 0.9764, 'learning_rate': 0.00019690894899039734, 'epoch': 0.11}
42:46<416:28:37, 322.71s/it] 11%|â–ˆ         | 552/5197 [53:48:02<413:46:54, 320.69s/it]                                                           11%|â–ˆ         | 552/5197 [53:48:02<413:46:54, 320.69s/it] 11%|â–ˆ         | 553/5197 [53:53:25<414:45:01, 321.51s/it]                                                           11%|â–ˆ         | 553/5197 [53:53:25<414:45:01, 321.51s/it] 11%|â–ˆ         | 554/5197 [53:58:47<414:30:57, 321.40s/it]                                                           11%|â–ˆ         | 554/5197 [53:58:47<414:30:57, 321.40s/it] 11%|â–ˆ         | 555/5197 [54:04:09<414:46:17, 321.67s/it]                                                           11%|â–ˆ         | 555/5197 [54:04:09<414:46:17, 321.67s/it] 11%|â–ˆ         | 556/5197 [54:09:34<416:12:48, 322.85s/it]                                                           11%|â–ˆ         | 556/5197 [54:09:34<416:12:48, 322.85s/it] 11%|â–ˆ         | 557/5197 [54:15:17<423:49:04, 328.82s/it]                        {'loss': 0.9975, 'learning_rate': 0.00019689355503544275, 'epoch': 0.11}
{'loss': 0.9975, 'learning_rate': 0.00019689355503544275, 'epoch': 0.11}
{'loss': 1.0179, 'learning_rate': 0.0001968781234481479, 'epoch': 0.11}
{'loss': 1.0179, 'learning_rate': 0.0001968781234481479, 'epoch': 0.11}
{'loss': 0.9185, 'learning_rate': 0.00019686265423450624, 'epoch': 0.11}
{'loss': 0.9185, 'learning_rate': 0.00019686265423450624, 'epoch': 0.11}
{'loss': 0.9705, 'learning_rate': 0.00019684714740052583, 'epoch': 0.11}
{'loss': 0.9705, 'learning_rate': 0.00019684714740052583, 'epoch': 0.11}
{'loss': 0.9069, 'learning_rate': 0.00019683160295222934, 'epoch': 0.11}
{'loss': 0.9069, 'learning_rate': 0.00019683160295222934, 'epoch': 0.11}
16:13:07, 322.86s/it]                                                           11%|â–ˆ         | 556/5197 [54:09:07<416:13:07, 322.86s/it] 11%|â–ˆ         | 557/5197 [54:14:50<423:49:27, 328.83s/it]                                                           11%|â–ˆ         | 557/5197 [54:14:50<423:49:27, 328.83s/it] 11%|â–ˆ         | 558/5197 [54:20:16<422:52:43, 328.17s/it]                                                           11%|â–ˆ         | 558/5197 [54:20:16<422:52:43, 328.17s/it] 11%|â–ˆ         | 559/5197 [54:25:31<417:41:29, 324.21s/it]                                                           11%|â–ˆ         | 559/5197 [54:25:31<417:41:29, 324.21s/it] 11%|â–ˆ         | 560/5197 [54:30:30<407:40:47, 316.51s/it]                                                           11%|â–ˆ         | 560/5197 [54:30:30<407:40:47, 316.51s/it] 11%|â–ˆ         | 561/5197 [54:35:48<408:21:43, 317.11s/it]                                                           11%|â–ˆ         | 561/5197 [54:{'loss': 0.9132, 'learning_rate': 0.00019681602089565402, 'epoch': 0.11}
{'loss': 0.9132, 'learning_rate': 0.00019681602089565402, 'epoch': 0.11}
                                   11%|â–ˆ         | 557/5197 [54:15:17<423:49:04, 328.82s/it] 11%|â–ˆ         | 558/5197 [54:20:44<422:52:28, 328.16s/it]                                                           11%|â–ˆ         | 558/5197 [54:20:44<422:52:28, 328.16s/it] 11%|â–ˆ         | 559/5197 [54:25:59<417:41:19, 324.21s/it]                                                           11%|â–ˆ         | 559/5197 [54:25:59<417:41:19, 324.21s/it] 11%|â–ˆ         | 560/5197 [54:30:57<407:40:42, 316.51s/it]                                                           11%|â–ˆ         | 560/5197 [54:30:57<407:40:42, 316.51s/it] 11%|â–ˆ         | 561/5197 [54:36:16<408:21:32, 317.10s/it]                                                           11%|â–ˆ         | 561/5197 [54:36:16<408:21:32, 317.10s/it] 11%|â–ˆ         | 562/5197 [54:41:40<410:54:34, 319.15s/it]                                                           11%|â–ˆ         | 562/5197 [54:41:40<410:54:34, 319.15s/it] 11%|â–ˆ         {'loss': 0.9005, 'learning_rate': 0.0001968004012368518, 'epoch': 0.11}
{'loss': 0.9005, 'learning_rate': 0.0001968004012368518, 'epoch': 0.11}
{'loss': 0.936, 'learning_rate': 0.0001967847439818892, 'epoch': 0.11}
{'loss': 0.936, 'learning_rate': 0.0001967847439818892, 'epoch': 0.11}
{'loss': 0.9742, 'learning_rate': 0.00019676904913684727, 'epoch': 0.11}
{'loss': 0.9742, 'learning_rate': 0.00019676904913684727, 'epoch': 0.11}
{'loss': 0.9397, 'learning_rate': 0.0001967533167078217, 'epoch': 0.11}
{'loss': 0.9397, 'learning_rate': 0.0001967533167078217, 'epoch': 0.11}
{'loss': 1.0131, 'learning_rate': 0.00019673754670092284, 'epoch': 0.11}
35:48<408:21:43, 317.11s/it] 11%|â–ˆ         | 562/5197 [54:41:12<410:54:22, 319.15s/it]                                                           11%|â–ˆ         | 562/5197 [54:41:12<410:54:22, 319.15s/it] 11%|â–ˆ         | 563/5197 [54:46:37<413:03:39, 320.89s/it]                                                           11%|â–ˆ         | 563/5197 [54:46:37<413:03:39, 320.89s/it] 11%|â–ˆ         | 564/5197 [54:52:07<416:21:08, 323.52s/it]                                                           11%|â–ˆ         | 564/5197 [54:52:07<416:21:08, 323.52s/it] 11%|â–ˆ         | 565/5197 [54:57:02<405:18:48, 315.01s/it]                                                           11%|â–ˆ         | 565/5197 [54:57:02<405:18:48, 315.01s/it] 11%|â–ˆ         | 566/5197 [55:03:26<431:39:09, 335.55s/it]                                                           11%|â–ˆ         | 566/5197 [55:03:26<431:39:09, 335.55s/it] 11%|â–ˆ         | 567/5197 [55:08:58<430:27:55, 334.70s/it]                        {'loss': 1.0131, 'learning_rate': 0.00019673754670092284, 'epoch': 0.11}
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
| 563/5197 [54:47:05<413:03:39, 320.89s/it]                                                           11%|â–ˆ         | 563/5197 [54:47:05<413:03:39, 320.89s/it] 11%|â–ˆ         | 564/5197 [54:52:34<416:21:18, 323.52s/it]                                                           11%|â–ˆ         | 564/5197 [54:52:34<416:21:18, 323.52s/it] 11%|â–ˆ         | 565/5197 [54:57:30<405:18:53, 315.01s/it]                                                           11%|â–ˆ         | 565/5197 [54:57:30<405:18:53, 315.01s/it] 11%|â–ˆ         | 566/5197 [55:03:53<431:39:01, 335.55s/it]                                                           11%|â–ˆ         | 566/5197 [55:03:53<431:39:01, 335.55s/it] 11%|â–ˆ         | 567/5197 [55:09:26<430:28:07, 334.71s/it]                                                           11%|â–ˆ         | 567/5197 [55:09:26<430:28:07, 334.71s/it]WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 10183 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 10184 closing signal SIGTERM
srun: got SIGCONT
                                   11%|â–ˆ         | 567/5197 [55:08:58<430:27:55, 334.70s/it]slurmstepd: error: *** STEP 116214.2 ON x090-5 CANCELLED AT 2023-11-24T16:35:28 ***
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 10185 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 10186 closing signal SIGTERM
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
slurmstepd: error: *** JOB 116214 ON x090-5 CANCELLED AT 2023-11-24T16:35:28 ***
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 58989 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 58990 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 58991 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 58992 closing signal SIGTERM
srun: forcing job termination
Traceback (most recent call last):
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py", line 798, in <module>
    main()
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 241, in launch_agent
    result = agent.run()
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 129, in wrapper
    result = f(*args, **kwargs)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 723, in run
    result = self._invoke_run(role)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    time.sleep(monitor_interval)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 10116 got signal: 15
Traceback (most recent call last):
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py", line 798, in <module>
    main()
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 241, in launch_agent
    result = agent.run()
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 129, in wrapper
    result = f(*args, **kwargs)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 723, in run
    result = self._invoke_run(role)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    time.sleep(monitor_interval)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 58919 got signal: 15
srun: error: x090-5: task 0: Exited with exit code 1
srun: error: x090-8: task 1: Exited with exit code 1
