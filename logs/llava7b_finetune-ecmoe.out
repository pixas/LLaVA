Node IP: 10.0.1.44 nodes_array: x090-3
x090-3-0,1,2,3,4,5,6,7
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : llava/train/train_mem.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 8
  run_id           : 29577
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.0.1.44:29577
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_peq7swpt/29577_v_pspaqa
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=x090-3
  master_port=38555
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
  role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
  global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
  role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]
  global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_peq7swpt/29577_v_pspaqa/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_peq7swpt/29577_v_pspaqa/attempt_0/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_peq7swpt/29577_v_pspaqa/attempt_0/2/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_peq7swpt/29577_v_pspaqa/attempt_0/3/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker4 reply file to: /tmp/torchelastic_peq7swpt/29577_v_pspaqa/attempt_0/4/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker5 reply file to: /tmp/torchelastic_peq7swpt/29577_v_pspaqa/attempt_0/5/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker6 reply file to: /tmp/torchelastic_peq7swpt/29577_v_pspaqa/attempt_0/6/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker7 reply file to: /tmp/torchelastic_peq7swpt/29577_v_pspaqa/attempt_0/7/error.json
[2023-11-28 23:46:07,640] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-28 23:46:07,640] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-28 23:46:07,641] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-28 23:46:07,642] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-28 23:46:07,644] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-28 23:46:07,645] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-28 23:46:07,645] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-28 23:46:07,645] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-28 23:46:22,986] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-28 23:46:22,986] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-28 23:46:22,990] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-28 23:46:22,991] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-28 23:46:22,992] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-28 23:46:22,992] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-28 23:46:22,992] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-28 23:46:22,992] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-28 23:46:22,992] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-28 23:46:22,992] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-28 23:46:22,992] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-28 23:46:22,992] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-28 23:46:22,993] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-28 23:46:22,993] [INFO] [comm.py:594:init_distributed] cdb=None
[2023-11-28 23:46:22,993] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-11-28 23:46:22,999] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-11-28 23:46:22,999] [INFO] [comm.py:594:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.
[2023-11-28 23:46:32,111] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 6.74B parameters

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:38<00:38, 38.69s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:38<00:38, 38.76s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:38<00:38, 38.79s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:38<00:38, 38.79s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:39<00:39, 39.10s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:39<00:39, 39.14s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:39<00:39, 39.18s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 20.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 23.27s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 20.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 23.27s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 20.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 23.28s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 20.75s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 23.44s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 20.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.65s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 20.92s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.66s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 20.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.68s/it]

Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.69s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 24.75s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.19s/it]
Adding LoRA adapters...
[2023-11-28 23:48:50,136] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0
[2023-11-28 23:48:50,476] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 7.04B parameters
Traceback (most recent call last):
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train_mem.py", line 13, in <module>
    train()
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train.py", line 901, in train
    model.get_model().initialize_vision_modules(
  File "/remote-home/syjiang/repo/LLaVA/llava/model/llava_arch.py", line 93, in initialize_vision_modules
    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ECSwitchLinear:
	size mismatch for experts.0.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.0.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.0.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.1.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.1.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.1.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.2.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.2.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.2.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.3.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.3.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.3.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.4.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.4.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.4.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.5.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.5.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.5.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.6.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.6.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.6.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.7.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.7.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.7.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
Traceback (most recent call last):
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train_mem.py", line 13, in <module>
Traceback (most recent call last):
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train_mem.py", line 13, in <module>
    train()
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train.py", line 901, in train
    train()
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train.py", line 901, in train
Traceback (most recent call last):
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train_mem.py", line 13, in <module>
    model.get_model().initialize_vision_modules(
  File "/remote-home/syjiang/repo/LLaVA/llava/model/llava_arch.py", line 93, in initialize_vision_modules
    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    model.get_model().initialize_vision_modules(
  File "/remote-home/syjiang/repo/LLaVA/llava/model/llava_arch.py", line 93, in initialize_vision_modules
    train()
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train.py", line 901, in train
    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    model.get_model().initialize_vision_modules(
  File "/remote-home/syjiang/repo/LLaVA/llava/model/llava_arch.py", line 93, in initialize_vision_modules
        self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(

  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
RuntimeError: Error(s) in loading state_dict for ECSwitchLinear:
	size mismatch for experts.0.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.0.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.0.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.1.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.1.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.1.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.2.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.2.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.2.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.3.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.3.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.3.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.4.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.4.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.4.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.5.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.5.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.5.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.6.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.6.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.6.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.7.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.7.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.7.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ECSwitchLinear:
	size mismatch for experts.0.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.0.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.0.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.1.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.1.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.1.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.2.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.2.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.2.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.3.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.3.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.3.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.4.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.4.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.4.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.5.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.5.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.5.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.6.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.6.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.6.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.7.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.7.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.7.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
Traceback (most recent call last):
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train_mem.py", line 13, in <module>
Traceback (most recent call last):
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train_mem.py", line 13, in <module>
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ECSwitchLinear:
	size mismatch for experts.0.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.0.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.0.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.1.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.1.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.1.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.2.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.2.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.2.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.3.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.3.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.3.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.4.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.4.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.4.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.5.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.5.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.5.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.6.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.6.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.6.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.7.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.7.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.7.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
    train()
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train.py", line 901, in train
    train()
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train.py", line 901, in train
Traceback (most recent call last):
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train_mem.py", line 13, in <module>
    model.get_model().initialize_vision_modules(
  File "/remote-home/syjiang/repo/LLaVA/llava/model/llava_arch.py", line 93, in initialize_vision_modules
    train()
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train.py", line 901, in train
    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)
      File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
model.get_model().initialize_vision_modules(
  File "/remote-home/syjiang/repo/LLaVA/llava/model/llava_arch.py", line 93, in initialize_vision_modules
    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    model.get_model().initialize_vision_modules(
  File "/remote-home/syjiang/repo/LLaVA/llava/model/llava_arch.py", line 93, in initialize_vision_modules
    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ECSwitchLinear:
	size mismatch for experts.0.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.0.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for expert's.0.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.1.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.1.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.1.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.2.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.2.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.2.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.3.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.3.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.3.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.4.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.4.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.4.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.5.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.5.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.5.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.6.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.6.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.6.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.7.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.7.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.7.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}.format(
RuntimeError: Error(s) in loading state_dict for ECSwitchLinear:
	size mismatch for experts.0.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.0.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.0.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.1.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.1.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.1.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.2.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.2.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.2.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.3.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.3.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.3.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.4.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.4.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.4.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.5.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.5.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.5.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.6.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.6.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.6.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.7.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.7.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.7.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ECSwitchLinear:
	size mismatch for experts.0.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.0.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.0.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.1.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.1.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.1.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.2.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.2.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.2.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.3.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.3.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.3.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.4.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.4.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.4.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.5.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.5.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.5.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.6.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.6.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.6.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.7.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.7.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.7.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
Traceback (most recent call last):
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train_mem.py", line 13, in <module>
    train()
  File "/remote-home/syjiang/repo/LLaVA/llava/train/train.py", line 901, in train
    model.get_model().initialize_vision_modules(
  File "/remote-home/syjiang/repo/LLaVA/llava/model/llava_arch.py", line 93, in initialize_vision_modules
    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ECSwitchLinear:
	size mismatch for experts.0.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.0.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.0.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.1.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.1.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.1.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.2.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.2.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.2.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.3.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.3.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.3.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.4.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.4.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.4.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.5.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.5.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.5.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.6.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.6.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.6.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
	size mismatch for experts.7.fn1.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for experts.7.fn1.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for experts.7.fn2.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 46504) of binary: /remote-home/syjiang/anaconda3/envs/llava/bin/python
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00042366981506347656 seconds
INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 0 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html
Traceback (most recent call last):
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py", line 798, in <module>
    main()
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/remote-home/syjiang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
llava/train/train_mem.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-11-28_23:49:08
  host      : x090-3
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 46505)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-11-28_23:49:08
  host      : x090-3
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 46506)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-11-28_23:49:08
  host      : x090-3
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 46507)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2023-11-28_23:49:08
  host      : x090-3
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 46508)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2023-11-28_23:49:08
  host      : x090-3
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 46509)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2023-11-28_23:49:08
  host      : x090-3
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 46510)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2023-11-28_23:49:08
  host      : x090-3
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 46511)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-28_23:49:08
  host      : x090-3
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 46504)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: x090-3: task 0: Exited with exit code 1
