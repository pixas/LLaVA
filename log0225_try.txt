开始监测显卡剩余显存...
检查显卡状态...
找到 7 张显卡的剩余显存大于等于 24000 MiB.
满足条件，正在选择剩余显存最大的 4 张显卡...
选定的显卡编号为：0 1 4 5
正在执行命令：CUDA_VISIBLE_DEVICES=0,1,4,5 bash scripts/v1_5/finetune_lora_decoder_molora.sh
[2024-02-25 04:11:52,744] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 04:11:52,745] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 04:11:53,704] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-02-25 04:11:53,704] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-02-25 04:11:53,704] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-25 04:11:53,760] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-02-25 04:11:53,760] [INFO] [comm.py:594:init_distributed] cdb=None
MoELlavaConfig {
  "_name_or_path": "/remote-home/yushengliao/syjiang/checkpoints/vicuna-7b-v1.5",
  "architectures": [
    "MoELLamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "is_eff_moe": false,
  "is_sparse": true,
  "lora_dropout": 0.0,
  "max_position_embeddings": 4096,
  "merge_weights": false,
  "mix_lora_alpha": 256,
  "mix_lora_r": 128,
  "model_type": "moe_llava",
  "moe_layer_index": 8,
  "num_attention_heads": 32,
  "num_experts": 4,
  "num_experts_per_token": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "use_lbl_loss": false,
  "vocab_size": 32000
}

[2024-02-25 04:11:57,537] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 19.95B parameters
MoELlavaLlamaForCausalLM(
  (model): MoELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x MoELLamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Adding LoRA adapters...
base_model.model.model.embed_tokens.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.0.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.1.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.2.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.3.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.4.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.5.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.6.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.7.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.8.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.9.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.10.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.11.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.12.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.13.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.14.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.15.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.16.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.17.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.18.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.19.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.20.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.21.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.22.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.23.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.24.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.25.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.26.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.27.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.28.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.29.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.30.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.31.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.norm.weight torch.bfloat16 False
base_model.model.lm_head.weight torch.bfloat16 False
base_model.model.model.embed_tokens.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.0.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.0.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.0.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.0.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.0.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.1.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.1.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.1.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.1.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.1.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.2.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.2.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.2.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.2.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.2.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.3.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.3.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.3.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.3.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.3.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.4.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.4.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.4.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.4.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.4.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.5.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.5.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.5.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.5.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.5.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.6.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.6.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.6.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.6.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.6.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.7.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.7.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.7.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.7.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.7.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.8.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.8.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.8.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.8.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.8.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.9.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.9.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.9.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.9.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.9.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.10.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.10.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.10.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.10.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.10.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.11.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.11.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.11.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.11.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.11.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.12.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.12.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.12.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.12.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.12.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.13.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.13.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.13.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.13.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.13.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.14.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.14.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.14.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.14.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.14.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.15.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.15.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.15.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.15.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.15.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.16.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.16.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.16.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.16.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.16.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.17.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.17.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.17.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.17.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.17.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.18.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.18.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.18.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.18.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.18.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.19.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.19.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.19.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.19.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.19.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.20.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.20.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.20.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.20.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.20.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.21.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.21.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.21.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.21.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.21.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.22.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.22.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.22.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.22.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.22.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.23.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.23.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.23.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.23.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.23.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.24.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.24.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.24.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.24.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.24.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.25.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.25.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.25.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.25.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.25.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.26.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.26.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.26.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.26.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.26.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.27.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.27.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.27.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.27.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.27.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.28.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.28.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.28.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.28.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.28.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.29.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.29.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.29.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.29.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.29.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.30.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.30.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.30.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.30.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.30.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.31.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.31.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.31.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.31.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.31.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.norm.weight torch.bfloat16 False
base_model.model.lm_head.weight torch.bfloat16 False
[2024-02-25 04:14:37,196] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0
[2024-02-25 04:14:37,406] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 20.26B parameters
Formatting inputs...Skip in lazy mode
Parameter Offload: Total persistent parameters: 2483584 in 568 params
{'loss': 1.3201, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
{'loss': 1.3145, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
{'loss': 1.3267, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.0}
{'loss': 1.4058, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.0}
{'loss': 1.333, 'learning_rate': 6.41025641025641e-06, 'epoch': 0.0}
{'loss': 1.272, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.0}
{'loss': 1.312, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.0}
{'loss': 1.2656, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.0}
{'loss': 1.2466, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.0}
{'loss': 1.3501, 'learning_rate': 1.282051282051282e-05, 'epoch': 0.0}
{'loss': 1.251, 'learning_rate': 1.4102564102564104e-05, 'epoch': 0.0}
{'loss': 1.1816, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.0}
{'loss': 1.249, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.0}
{'loss': 1.1365, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.0}
{'loss': 1.176, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.0}
{'loss': 1.1697, 'learning_rate': 2.0512820512820512e-05, 'epoch': 0.0}
{'loss': 1.1953, 'learning_rate': 2.1794871794871795e-05, 'epoch': 0.0}
{'loss': 1.0923, 'learning_rate': 2.307692307692308e-05, 'epoch': 0.0}
{'loss': 1.1108, 'learning_rate': 2.435897435897436e-05, 'epoch': 0.0}
{'loss': 1.0422, 'learning_rate': 2.564102564102564e-05, 'epoch': 0.0}
{'loss': 1.0732, 'learning_rate': 2.6923076923076923e-05, 'epoch': 0.0}
{'loss': 1.0479, 'learning_rate': 2.8205128205128207e-05, 'epoch': 0.0}
{'loss': 1.0964, 'learning_rate': 2.948717948717949e-05, 'epoch': 0.0}
{'loss': 1.0305, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.0}
{'loss': 1.1309, 'learning_rate': 3.205128205128206e-05, 'epoch': 0.0}
{'loss': 1.0942, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}
{'loss': 1.1455, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.01}
{'loss': 1.0461, 'learning_rate': 3.58974358974359e-05, 'epoch': 0.01}
{'loss': 1.0403, 'learning_rate': 3.717948717948718e-05, 'epoch': 0.01}
{'loss': 1.0803, 'learning_rate': 3.846153846153846e-05, 'epoch': 0.01}
{'loss': 1.0413, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.01}
{'loss': 1.0432, 'learning_rate': 4.1025641025641023e-05, 'epoch': 0.01}
{'loss': 1.0176, 'learning_rate': 4.230769230769231e-05, 'epoch': 0.01}
{'loss': 1.0449, 'learning_rate': 4.358974358974359e-05, 'epoch': 0.01}
{'loss': 1.0366, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.01}
{'loss': 1.0129, 'learning_rate': 4.615384615384616e-05, 'epoch': 0.01}
{'loss': 0.351, 'learning_rate': 4.7435897435897435e-05, 'epoch': 0.01}
{'loss': 1.0757, 'learning_rate': 4.871794871794872e-05, 'epoch': 0.01}
{'loss': 1.0081, 'learning_rate': 5e-05, 'epoch': 0.01}
{'loss': 1.0002, 'learning_rate': 5.128205128205128e-05, 'epoch': 0.01}
{'loss': 1.0029, 'learning_rate': 5.256410256410257e-05, 'epoch': 0.01}
{'loss': 1.0291, 'learning_rate': 5.384615384615385e-05, 'epoch': 0.01}
{'loss': 0.9868, 'learning_rate': 5.512820512820514e-05, 'epoch': 0.01}
{'loss': 1.0649, 'learning_rate': 5.6410256410256414e-05, 'epoch': 0.01}
{'loss': 1.0435, 'learning_rate': 5.769230769230769e-05, 'epoch': 0.01}
{'loss': 0.9609, 'learning_rate': 5.897435897435898e-05, 'epoch': 0.01}
{'loss': 0.9385, 'learning_rate': 6.025641025641026e-05, 'epoch': 0.01}
{'loss': 0.9236, 'learning_rate': 6.153846153846155e-05, 'epoch': 0.01}
{'loss': 1.002, 'learning_rate': 6.282051282051282e-05, 'epoch': 0.01}
{'loss': 1.0115, 'learning_rate': 6.410256410256412e-05, 'epoch': 0.01}
{'loss': 0.9534, 'learning_rate': 6.538461538461539e-05, 'epoch': 0.01}
{'loss': 1.0046, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.01}
{'loss': 0.9934, 'learning_rate': 6.794871794871795e-05, 'epoch': 0.01}
{'loss': 0.9844, 'learning_rate': 6.923076923076924e-05, 'epoch': 0.01}
{'loss': 0.967, 'learning_rate': 7.051282051282052e-05, 'epoch': 0.01}
{'loss': 0.3151, 'learning_rate': 7.17948717948718e-05, 'epoch': 0.01}
{'loss': 0.9829, 'learning_rate': 7.307692307692307e-05, 'epoch': 0.01}
{'loss': 0.9565, 'learning_rate': 7.435897435897436e-05, 'epoch': 0.01}
{'loss': 1.0337, 'learning_rate': 7.564102564102564e-05, 'epoch': 0.01}
{'loss': 0.9807, 'learning_rate': 7.692307692307693e-05, 'epoch': 0.01}
{'loss': 0.949, 'learning_rate': 7.820512820512821e-05, 'epoch': 0.01}
{'loss': 1.0759, 'learning_rate': 7.948717948717948e-05, 'epoch': 0.01}
{'loss': 0.9812, 'learning_rate': 8.076923076923078e-05, 'epoch': 0.01}
{'loss': 1.0391, 'learning_rate': 8.205128205128205e-05, 'epoch': 0.01}
{'loss': 0.9873, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.01}
{'loss': 0.8729, 'learning_rate': 8.461538461538461e-05, 'epoch': 0.01}
{'loss': 0.9922, 'learning_rate': 8.58974358974359e-05, 'epoch': 0.01}
{'loss': 0.9269, 'learning_rate': 8.717948717948718e-05, 'epoch': 0.01}
{'loss': 0.938, 'learning_rate': 8.846153846153847e-05, 'epoch': 0.01}
{'loss': 0.9746, 'learning_rate': 8.974358974358975e-05, 'epoch': 0.01}
{'loss': 0.926, 'learning_rate': 9.102564102564103e-05, 'epoch': 0.01}
{'loss': 0.9316, 'learning_rate': 9.230769230769232e-05, 'epoch': 0.01}
{'loss': 0.9275, 'learning_rate': 9.35897435897436e-05, 'epoch': 0.01}
{'loss': 0.9192, 'learning_rate': 9.487179487179487e-05, 'epoch': 0.01}
{'loss': 0.9471, 'learning_rate': 9.615384615384617e-05, 'epoch': 0.01}
{'loss': 0.9226, 'learning_rate': 9.743589743589744e-05, 'epoch': 0.01}
{'loss': 0.9888, 'learning_rate': 9.871794871794872e-05, 'epoch': 0.01}
{'loss': 1.0085, 'learning_rate': 0.0001, 'epoch': 0.02}
{'loss': 0.9287, 'learning_rate': 0.00010128205128205129, 'epoch': 0.02}
{'loss': 0.998, 'learning_rate': 0.00010256410256410256, 'epoch': 0.02}
{'loss': 0.9663, 'learning_rate': 0.00010384615384615386, 'epoch': 0.02}
{'loss': 0.9485, 'learning_rate': 0.00010512820512820514, 'epoch': 0.02}
{'loss': 0.9583, 'learning_rate': 0.00010641025641025641, 'epoch': 0.02}
{'loss': 0.9185, 'learning_rate': 0.0001076923076923077, 'epoch': 0.02}
{'loss': 0.9875, 'learning_rate': 0.00010897435897435896, 'epoch': 0.02}
{'loss': 0.9126, 'learning_rate': 0.00011025641025641027, 'epoch': 0.02}
{'loss': 1.033, 'learning_rate': 0.00011153846153846154, 'epoch': 0.02}
{'loss': 0.9819, 'learning_rate': 0.00011282051282051283, 'epoch': 0.02}
{'loss': 0.2987, 'learning_rate': 0.0001141025641025641, 'epoch': 0.02}
{'loss': 0.9675, 'learning_rate': 0.00011538461538461538, 'epoch': 0.02}
{'loss': 0.9734, 'learning_rate': 0.00011666666666666668, 'epoch': 0.02}
{'loss': 0.9795, 'learning_rate': 0.00011794871794871796, 'epoch': 0.02}
{'loss': 0.9509, 'learning_rate': 0.00011923076923076923, 'epoch': 0.02}
{'loss': 0.9323, 'learning_rate': 0.00012051282051282052, 'epoch': 0.02}
{'loss': 0.2638, 'learning_rate': 0.00012179487179487179, 'epoch': 0.02}
{'loss': 0.8879, 'learning_rate': 0.0001230769230769231, 'epoch': 0.02}
{'loss': 0.9438, 'learning_rate': 0.00012435897435897437, 'epoch': 0.02}
{'loss': 0.9316, 'learning_rate': 0.00012564102564102564, 'epoch': 0.02}
{'loss': 0.9517, 'learning_rate': 0.00012692307692307693, 'epoch': 0.02}
{'loss': 0.2956, 'learning_rate': 0.00012820512820512823, 'epoch': 0.02}
{'loss': 0.9561, 'learning_rate': 0.0001294871794871795, 'epoch': 0.02}
{'loss': 0.9338, 'learning_rate': 0.00013076923076923077, 'epoch': 0.02}
{'loss': 0.9348, 'learning_rate': 0.00013205128205128204, 'epoch': 0.02}
{'loss': 0.927, 'learning_rate': 0.00013333333333333334, 'epoch': 0.02}
{'loss': 0.2958, 'learning_rate': 0.00013461538461538464, 'epoch': 0.02}
{'loss': 0.9634, 'learning_rate': 0.0001358974358974359, 'epoch': 0.02}
{'loss': 0.9678, 'learning_rate': 0.00013717948717948718, 'epoch': 0.02}
{'loss': 0.9397, 'learning_rate': 0.00013846153846153847, 'epoch': 0.02}
{'loss': 0.9431, 'learning_rate': 0.00013974358974358974, 'epoch': 0.02}
{'loss': 1.0164, 'learning_rate': 0.00014102564102564104, 'epoch': 0.02}
{'loss': 0.9915, 'learning_rate': 0.0001423076923076923, 'epoch': 0.02}
{'loss': 0.9397, 'learning_rate': 0.0001435897435897436, 'epoch': 0.02}
{'loss': 0.9434, 'learning_rate': 0.00014487179487179488, 'epoch': 0.02}
{'loss': 0.9761, 'learning_rate': 0.00014615384615384615, 'epoch': 0.02}
{'loss': 0.8848, 'learning_rate': 0.00014743589743589745, 'epoch': 0.02}
{'loss': 0.3042, 'learning_rate': 0.00014871794871794872, 'epoch': 0.02}
{'loss': 0.958, 'learning_rate': 0.00015000000000000001, 'epoch': 0.02}
{'loss': 0.9194, 'learning_rate': 0.00015128205128205128, 'epoch': 0.02}
{'loss': 0.9053, 'learning_rate': 0.00015256410256410255, 'epoch': 0.02}
{'loss': 0.9165, 'learning_rate': 0.00015384615384615385, 'epoch': 0.02}
{'loss': 0.9731, 'learning_rate': 0.00015512820512820515, 'epoch': 0.02}
{'loss': 0.9856, 'learning_rate': 0.00015641025641025642, 'epoch': 0.02}
{'loss': 0.9028, 'learning_rate': 0.0001576923076923077, 'epoch': 0.02}
{'loss': 0.9092, 'learning_rate': 0.00015897435897435896, 'epoch': 0.02}
{'loss': 0.9673, 'learning_rate': 0.00016025641025641028, 'epoch': 0.02}
{'loss': 0.9844, 'learning_rate': 0.00016153846153846155, 'epoch': 0.02}
{'loss': 0.9854, 'learning_rate': 0.00016282051282051282, 'epoch': 0.02}
{'loss': 0.9807, 'learning_rate': 0.0001641025641025641, 'epoch': 0.02}
{'loss': 0.9287, 'learning_rate': 0.0001653846153846154, 'epoch': 0.02}
{'loss': 0.9695, 'learning_rate': 0.0001666666666666667, 'epoch': 0.03}
{'loss': 0.9456, 'learning_rate': 0.00016794871794871796, 'epoch': 0.03}
{'loss': 0.8809, 'learning_rate': 0.00016923076923076923, 'epoch': 0.03}
{'loss': 0.8887, 'learning_rate': 0.00017051282051282053, 'epoch': 0.03}
{'loss': 0.967, 'learning_rate': 0.0001717948717948718, 'epoch': 0.03}
{'loss': 1.0005, 'learning_rate': 0.0001730769230769231, 'epoch': 0.03}
{'loss': 0.9448, 'learning_rate': 0.00017435897435897436, 'epoch': 0.03}
{'loss': 0.9766, 'learning_rate': 0.00017564102564102566, 'epoch': 0.03}
{'loss': 0.9543, 'learning_rate': 0.00017692307692307693, 'epoch': 0.03}
{'loss': 0.9646, 'learning_rate': 0.00017820512820512823, 'epoch': 0.03}
{'loss': 0.9333, 'learning_rate': 0.0001794871794871795, 'epoch': 0.03}
{'loss': 0.8806, 'learning_rate': 0.00018076923076923077, 'epoch': 0.03}
{'loss': 0.8926, 'learning_rate': 0.00018205128205128207, 'epoch': 0.03}
{'loss': 0.9619, 'learning_rate': 0.00018333333333333334, 'epoch': 0.03}
{'loss': 0.9856, 'learning_rate': 0.00018461538461538463, 'epoch': 0.03}
{'loss': 0.8361, 'learning_rate': 0.0001858974358974359, 'epoch': 0.03}
{'loss': 0.9751, 'learning_rate': 0.0001871794871794872, 'epoch': 0.03}
{'loss': 0.9829, 'learning_rate': 0.00018846153846153847, 'epoch': 0.03}
{'loss': 0.9294, 'learning_rate': 0.00018974358974358974, 'epoch': 0.03}
{'loss': 0.9277, 'learning_rate': 0.00019102564102564104, 'epoch': 0.03}
{'loss': 0.95, 'learning_rate': 0.00019230769230769233, 'epoch': 0.03}
WARNING: tokenization mismatch: 1 vs. 789. (ignored)
{'loss': 0.95, 'learning_rate': 0.0001935897435897436, 'epoch': 0.03}
{'loss': 0.3184, 'learning_rate': 0.00019487179487179487, 'epoch': 0.03}
{'loss': 0.9746, 'learning_rate': 0.00019615384615384615, 'epoch': 0.03}
{'loss': 0.8857, 'learning_rate': 0.00019743589743589744, 'epoch': 0.03}
{'loss': 0.9485, 'learning_rate': 0.00019871794871794874, 'epoch': 0.03}
{'loss': 0.9569, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.9331, 'learning_rate': 0.00019999998058057615, 'epoch': 0.03}
{'loss': 0.9224, 'learning_rate': 0.00019999992232231216, 'epoch': 0.03}
{'loss': 0.9404, 'learning_rate': 0.0001999998252252306, 'epoch': 0.03}
{'loss': 0.9463, 'learning_rate': 0.00019999968928936926, 'epoch': 0.03}
{'loss': 0.9165, 'learning_rate': 0.00019999951451478087, 'epoch': 0.03}
{'loss': 0.9539, 'learning_rate': 0.00019999930090153334, 'epoch': 0.03}
{'loss': 0.8617, 'learning_rate': 0.00019999904844970962, 'epoch': 0.03}
{'loss': 0.2985, 'learning_rate': 0.00019999875715940782, 'epoch': 0.03}
{'loss': 1.0139, 'learning_rate': 0.000199998427030741, 'epoch': 0.03}
{'loss': 0.946, 'learning_rate': 0.00019999805806383738, 'epoch': 0.03}
{'loss': 0.9907, 'learning_rate': 0.0001999976502588403, 'epoch': 0.03}
{'loss': 0.9353, 'learning_rate': 0.0001999972036159081, 'epoch': 0.03}
{'loss': 0.8906, 'learning_rate': 0.00019999671813521435, 'epoch': 0.03}
{'loss': 0.9045, 'learning_rate': 0.0001999961938169475, 'epoch': 0.03}
{'loss': 0.9089, 'learning_rate': 0.00019999563066131124, 'epoch': 0.03}
{'loss': 0.937, 'learning_rate': 0.00019999502866852425, 'epoch': 0.03}
{'loss': 0.9707, 'learning_rate': 0.0001999943878388204, 'epoch': 0.03}
{'loss': 0.8768, 'learning_rate': 0.00019999370817244853, 'epoch': 0.03}
{'loss': 0.9143, 'learning_rate': 0.00019999298966967265, 'epoch': 0.03}
{'loss': 0.8933, 'learning_rate': 0.00019999223233077177, 'epoch': 0.03}
{'loss': 0.8579, 'learning_rate': 0.0001999914361560401, 'epoch': 0.03}
{'loss': 0.9163, 'learning_rate': 0.00019999060114578684, 'epoch': 0.03}
{'loss': 0.9265, 'learning_rate': 0.00019998972730033622, 'epoch': 0.03}
{'loss': 0.2885, 'learning_rate': 0.00019998881462002778, 'epoch': 0.03}
{'loss': 0.925, 'learning_rate': 0.00019998786310521585, 'epoch': 0.03}
