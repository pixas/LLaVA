开始监测显卡剩余显存...
检查显卡状态...
找到 7 张显卡的剩余显存大于等于 24000 MiB.
满足条件，正在选择剩余显存最大的 4 张显卡...
选定的显卡编号为：0 1 4 5
正在执行命令：CUDA_VISIBLE_DEVICES=0,1,4,5 bash scripts/v1_5/finetune_lora_decoder_molora.sh
[2024-02-25 04:11:52,744] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 04:11:52,745] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 04:11:53,704] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-02-25 04:11:53,704] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-02-25 04:11:53,704] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-25 04:11:53,760] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-02-25 04:11:53,760] [INFO] [comm.py:594:init_distributed] cdb=None
MoELlavaConfig {
  "_name_or_path": "/remote-home/yushengliao/syjiang/checkpoints/vicuna-7b-v1.5",
  "architectures": [
    "MoELLamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "is_eff_moe": false,
  "is_sparse": true,
  "lora_dropout": 0.0,
  "max_position_embeddings": 4096,
  "merge_weights": false,
  "mix_lora_alpha": 256,
  "mix_lora_r": 128,
  "model_type": "moe_llava",
  "moe_layer_index": 8,
  "num_attention_heads": 32,
  "num_experts": 4,
  "num_experts_per_token": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "use_lbl_loss": false,
  "vocab_size": 32000
}

[2024-02-25 04:11:57,537] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 19.95B parameters
MoELlavaLlamaForCausalLM(
  (model): MoELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x MoELLamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Adding LoRA adapters...
base_model.model.model.embed_tokens.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.0.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.1.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.2.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.3.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.4.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.5.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.6.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.7.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.8.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.9.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.10.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.11.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.12.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.13.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.14.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.15.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.16.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.17.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.18.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.19.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.20.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.21.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.22.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.23.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.24.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.25.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.26.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.27.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.28.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.29.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.30.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.31.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.norm.weight torch.bfloat16 False
base_model.model.lm_head.weight torch.bfloat16 False
base_model.model.model.embed_tokens.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.0.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.0.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.0.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.0.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.0.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.0.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.0.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.0.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.0.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.1.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.1.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.1.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.1.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.1.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.1.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.1.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.1.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.1.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.2.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.2.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.2.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.2.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.2.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.2.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.2.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.2.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.2.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.3.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.3.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.3.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.3.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.3.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.3.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.3.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.3.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.3.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.4.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.4.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.4.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.4.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.4.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.4.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.4.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.4.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.4.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.5.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.5.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.5.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.5.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.5.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.5.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.5.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.5.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.5.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.6.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.6.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.6.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.6.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.6.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.6.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.6.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.6.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.6.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.7.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.7.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.7.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.7.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.7.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.7.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.7.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.7.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.7.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.8.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.8.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.8.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.8.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.8.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.8.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.8.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.8.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.8.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.9.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.9.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.9.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.9.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.9.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.9.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.9.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.9.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.9.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.10.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.10.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.10.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.10.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.10.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.10.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.10.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.10.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.10.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.11.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.11.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.11.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.11.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.11.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.11.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.11.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.11.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.11.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.12.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.12.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.12.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.12.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.12.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.12.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.12.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.12.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.12.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.13.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.13.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.13.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.13.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.13.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.13.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.13.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.13.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.13.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.14.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.14.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.14.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.14.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.14.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.14.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.14.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.14.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.14.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.15.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.15.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.15.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.15.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.15.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.15.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.15.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.15.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.15.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.16.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.16.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.16.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.16.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.16.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.16.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.16.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.16.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.16.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.17.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.17.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.17.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.17.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.17.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.17.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.17.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.17.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.17.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.18.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.18.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.18.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.18.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.18.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.18.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.18.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.18.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.18.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.19.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.19.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.19.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.19.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.19.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.19.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.19.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.19.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.19.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.20.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.20.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.20.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.20.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.20.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.20.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.20.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.20.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.20.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.21.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.21.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.21.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.21.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.21.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.21.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.21.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.21.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.21.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.22.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.22.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.22.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.22.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.22.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.22.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.22.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.22.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.22.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.23.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.23.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.23.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.23.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.23.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.23.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.23.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.23.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.23.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.24.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.24.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.24.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.24.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.24.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.24.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.24.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.24.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.24.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.25.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.25.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.25.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.25.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.25.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.25.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.25.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.25.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.25.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.26.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.26.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.26.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.26.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.26.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.26.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.26.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.26.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.26.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.27.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.27.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.27.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.27.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.27.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.27.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.27.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.27.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.27.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.28.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.28.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.28.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.28.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.28.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.28.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.28.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.28.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.28.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.29.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.29.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.29.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.29.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.29.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.29.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.29.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.29.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.29.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.30.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.30.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.30.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.30.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.30.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.30.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.30.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.30.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.30.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.q_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.k_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.v_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.o_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.float32 True
base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.mlp.gate_proj.bias torch.float32 False
base_model.model.model.layers.31.mlp.gate_proj.switch.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.switch.bias torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.31.mlp.gate_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.mlp.up_proj.bias torch.float32 False
base_model.model.model.layers.31.mlp.up_proj.switch.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.switch.bias torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.31.mlp.up_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.weight torch.bfloat16 False
base_model.model.model.layers.31.mlp.down_proj.bias torch.float32 False
base_model.model.model.layers.31.mlp.down_proj.switch.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.switch.bias torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.0.lora_A_0.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.0.lora_B_0.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.1.lora_A_1.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.1.lora_B_1.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.2.lora_A_2.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.2.lora_B_2.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.3.lora_A_3.weight torch.float32 True
base_model.model.model.layers.31.mlp.down_proj.experts.3.lora_B_3.weight torch.float32 True
base_model.model.model.layers.31.input_layernorm.weight torch.bfloat16 False
base_model.model.model.layers.31.post_attention_layernorm.weight torch.bfloat16 False
base_model.model.model.norm.weight torch.bfloat16 False
base_model.model.lm_head.weight torch.bfloat16 False
[2024-02-25 04:14:37,196] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0
[2024-02-25 04:14:37,406] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 20.26B parameters
Formatting inputs...Skip in lazy mode
Parameter Offload: Total persistent parameters: 2483584 in 568 params
{'loss': 1.3201, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
{'loss': 1.3145, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
{'loss': 1.3267, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.0}
{'loss': 1.4058, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.0}
{'loss': 1.333, 'learning_rate': 6.41025641025641e-06, 'epoch': 0.0}
{'loss': 1.272, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.0}
{'loss': 1.312, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.0}
{'loss': 1.2656, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.0}
{'loss': 1.2466, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.0}
{'loss': 1.3501, 'learning_rate': 1.282051282051282e-05, 'epoch': 0.0}
{'loss': 1.251, 'learning_rate': 1.4102564102564104e-05, 'epoch': 0.0}
{'loss': 1.1816, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.0}
{'loss': 1.249, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.0}
{'loss': 1.1365, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.0}
{'loss': 1.176, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.0}
{'loss': 1.1697, 'learning_rate': 2.0512820512820512e-05, 'epoch': 0.0}
{'loss': 1.1953, 'learning_rate': 2.1794871794871795e-05, 'epoch': 0.0}
{'loss': 1.0923, 'learning_rate': 2.307692307692308e-05, 'epoch': 0.0}
{'loss': 1.1108, 'learning_rate': 2.435897435897436e-05, 'epoch': 0.0}
{'loss': 1.0422, 'learning_rate': 2.564102564102564e-05, 'epoch': 0.0}
{'loss': 1.0732, 'learning_rate': 2.6923076923076923e-05, 'epoch': 0.0}
{'loss': 1.0479, 'learning_rate': 2.8205128205128207e-05, 'epoch': 0.0}
{'loss': 1.0964, 'learning_rate': 2.948717948717949e-05, 'epoch': 0.0}
{'loss': 1.0305, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.0}
{'loss': 1.1309, 'learning_rate': 3.205128205128206e-05, 'epoch': 0.0}
{'loss': 1.0942, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}
{'loss': 1.1455, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.01}
{'loss': 1.0461, 'learning_rate': 3.58974358974359e-05, 'epoch': 0.01}
{'loss': 1.0403, 'learning_rate': 3.717948717948718e-05, 'epoch': 0.01}
{'loss': 1.0803, 'learning_rate': 3.846153846153846e-05, 'epoch': 0.01}
{'loss': 1.0413, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.01}
{'loss': 1.0432, 'learning_rate': 4.1025641025641023e-05, 'epoch': 0.01}
{'loss': 1.0176, 'learning_rate': 4.230769230769231e-05, 'epoch': 0.01}
{'loss': 1.0449, 'learning_rate': 4.358974358974359e-05, 'epoch': 0.01}
{'loss': 1.0366, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.01}
{'loss': 1.0129, 'learning_rate': 4.615384615384616e-05, 'epoch': 0.01}
{'loss': 0.351, 'learning_rate': 4.7435897435897435e-05, 'epoch': 0.01}
{'loss': 1.0757, 'learning_rate': 4.871794871794872e-05, 'epoch': 0.01}
{'loss': 1.0081, 'learning_rate': 5e-05, 'epoch': 0.01}
{'loss': 1.0002, 'learning_rate': 5.128205128205128e-05, 'epoch': 0.01}
{'loss': 1.0029, 'learning_rate': 5.256410256410257e-05, 'epoch': 0.01}
{'loss': 1.0291, 'learning_rate': 5.384615384615385e-05, 'epoch': 0.01}
{'loss': 0.9868, 'learning_rate': 5.512820512820514e-05, 'epoch': 0.01}
{'loss': 1.0649, 'learning_rate': 5.6410256410256414e-05, 'epoch': 0.01}
{'loss': 1.0435, 'learning_rate': 5.769230769230769e-05, 'epoch': 0.01}
{'loss': 0.9609, 'learning_rate': 5.897435897435898e-05, 'epoch': 0.01}
{'loss': 0.9385, 'learning_rate': 6.025641025641026e-05, 'epoch': 0.01}
{'loss': 0.9236, 'learning_rate': 6.153846153846155e-05, 'epoch': 0.01}
{'loss': 1.002, 'learning_rate': 6.282051282051282e-05, 'epoch': 0.01}
{'loss': 1.0115, 'learning_rate': 6.410256410256412e-05, 'epoch': 0.01}
{'loss': 0.9534, 'learning_rate': 6.538461538461539e-05, 'epoch': 0.01}
{'loss': 1.0046, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.01}
{'loss': 0.9934, 'learning_rate': 6.794871794871795e-05, 'epoch': 0.01}
{'loss': 0.9844, 'learning_rate': 6.923076923076924e-05, 'epoch': 0.01}
{'loss': 0.967, 'learning_rate': 7.051282051282052e-05, 'epoch': 0.01}
{'loss': 0.3151, 'learning_rate': 7.17948717948718e-05, 'epoch': 0.01}
{'loss': 0.9829, 'learning_rate': 7.307692307692307e-05, 'epoch': 0.01}
{'loss': 0.9565, 'learning_rate': 7.435897435897436e-05, 'epoch': 0.01}
{'loss': 1.0337, 'learning_rate': 7.564102564102564e-05, 'epoch': 0.01}
{'loss': 0.9807, 'learning_rate': 7.692307692307693e-05, 'epoch': 0.01}
{'loss': 0.949, 'learning_rate': 7.820512820512821e-05, 'epoch': 0.01}
{'loss': 1.0759, 'learning_rate': 7.948717948717948e-05, 'epoch': 0.01}
{'loss': 0.9812, 'learning_rate': 8.076923076923078e-05, 'epoch': 0.01}
{'loss': 1.0391, 'learning_rate': 8.205128205128205e-05, 'epoch': 0.01}
{'loss': 0.9873, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.01}
{'loss': 0.8729, 'learning_rate': 8.461538461538461e-05, 'epoch': 0.01}
{'loss': 0.9922, 'learning_rate': 8.58974358974359e-05, 'epoch': 0.01}
{'loss': 0.9269, 'learning_rate': 8.717948717948718e-05, 'epoch': 0.01}
{'loss': 0.938, 'learning_rate': 8.846153846153847e-05, 'epoch': 0.01}
{'loss': 0.9746, 'learning_rate': 8.974358974358975e-05, 'epoch': 0.01}
{'loss': 0.926, 'learning_rate': 9.102564102564103e-05, 'epoch': 0.01}
{'loss': 0.9316, 'learning_rate': 9.230769230769232e-05, 'epoch': 0.01}
{'loss': 0.9275, 'learning_rate': 9.35897435897436e-05, 'epoch': 0.01}
{'loss': 0.9192, 'learning_rate': 9.487179487179487e-05, 'epoch': 0.01}
{'loss': 0.9471, 'learning_rate': 9.615384615384617e-05, 'epoch': 0.01}
{'loss': 0.9226, 'learning_rate': 9.743589743589744e-05, 'epoch': 0.01}
{'loss': 0.9888, 'learning_rate': 9.871794871794872e-05, 'epoch': 0.01}
{'loss': 1.0085, 'learning_rate': 0.0001, 'epoch': 0.02}
{'loss': 0.9287, 'learning_rate': 0.00010128205128205129, 'epoch': 0.02}
{'loss': 0.998, 'learning_rate': 0.00010256410256410256, 'epoch': 0.02}
{'loss': 0.9663, 'learning_rate': 0.00010384615384615386, 'epoch': 0.02}
{'loss': 0.9485, 'learning_rate': 0.00010512820512820514, 'epoch': 0.02}
{'loss': 0.9583, 'learning_rate': 0.00010641025641025641, 'epoch': 0.02}
{'loss': 0.9185, 'learning_rate': 0.0001076923076923077, 'epoch': 0.02}
{'loss': 0.9875, 'learning_rate': 0.00010897435897435896, 'epoch': 0.02}
{'loss': 0.9126, 'learning_rate': 0.00011025641025641027, 'epoch': 0.02}
{'loss': 1.033, 'learning_rate': 0.00011153846153846154, 'epoch': 0.02}
{'loss': 0.9819, 'learning_rate': 0.00011282051282051283, 'epoch': 0.02}
{'loss': 0.2987, 'learning_rate': 0.0001141025641025641, 'epoch': 0.02}
{'loss': 0.9675, 'learning_rate': 0.00011538461538461538, 'epoch': 0.02}
{'loss': 0.9734, 'learning_rate': 0.00011666666666666668, 'epoch': 0.02}
{'loss': 0.9795, 'learning_rate': 0.00011794871794871796, 'epoch': 0.02}
{'loss': 0.9509, 'learning_rate': 0.00011923076923076923, 'epoch': 0.02}
{'loss': 0.9323, 'learning_rate': 0.00012051282051282052, 'epoch': 0.02}
{'loss': 0.2638, 'learning_rate': 0.00012179487179487179, 'epoch': 0.02}
{'loss': 0.8879, 'learning_rate': 0.0001230769230769231, 'epoch': 0.02}
{'loss': 0.9438, 'learning_rate': 0.00012435897435897437, 'epoch': 0.02}
{'loss': 0.9316, 'learning_rate': 0.00012564102564102564, 'epoch': 0.02}
{'loss': 0.9517, 'learning_rate': 0.00012692307692307693, 'epoch': 0.02}
{'loss': 0.2956, 'learning_rate': 0.00012820512820512823, 'epoch': 0.02}
{'loss': 0.9561, 'learning_rate': 0.0001294871794871795, 'epoch': 0.02}
{'loss': 0.9338, 'learning_rate': 0.00013076923076923077, 'epoch': 0.02}
{'loss': 0.9348, 'learning_rate': 0.00013205128205128204, 'epoch': 0.02}
{'loss': 0.927, 'learning_rate': 0.00013333333333333334, 'epoch': 0.02}
{'loss': 0.2958, 'learning_rate': 0.00013461538461538464, 'epoch': 0.02}
{'loss': 0.9634, 'learning_rate': 0.0001358974358974359, 'epoch': 0.02}
{'loss': 0.9678, 'learning_rate': 0.00013717948717948718, 'epoch': 0.02}
{'loss': 0.9397, 'learning_rate': 0.00013846153846153847, 'epoch': 0.02}
{'loss': 0.9431, 'learning_rate': 0.00013974358974358974, 'epoch': 0.02}
{'loss': 1.0164, 'learning_rate': 0.00014102564102564104, 'epoch': 0.02}
{'loss': 0.9915, 'learning_rate': 0.0001423076923076923, 'epoch': 0.02}
{'loss': 0.9397, 'learning_rate': 0.0001435897435897436, 'epoch': 0.02}
{'loss': 0.9434, 'learning_rate': 0.00014487179487179488, 'epoch': 0.02}
{'loss': 0.9761, 'learning_rate': 0.00014615384615384615, 'epoch': 0.02}
{'loss': 0.8848, 'learning_rate': 0.00014743589743589745, 'epoch': 0.02}
{'loss': 0.3042, 'learning_rate': 0.00014871794871794872, 'epoch': 0.02}
{'loss': 0.958, 'learning_rate': 0.00015000000000000001, 'epoch': 0.02}
{'loss': 0.9194, 'learning_rate': 0.00015128205128205128, 'epoch': 0.02}
{'loss': 0.9053, 'learning_rate': 0.00015256410256410255, 'epoch': 0.02}
{'loss': 0.9165, 'learning_rate': 0.00015384615384615385, 'epoch': 0.02}
{'loss': 0.9731, 'learning_rate': 0.00015512820512820515, 'epoch': 0.02}
{'loss': 0.9856, 'learning_rate': 0.00015641025641025642, 'epoch': 0.02}
{'loss': 0.9028, 'learning_rate': 0.0001576923076923077, 'epoch': 0.02}
{'loss': 0.9092, 'learning_rate': 0.00015897435897435896, 'epoch': 0.02}
{'loss': 0.9673, 'learning_rate': 0.00016025641025641028, 'epoch': 0.02}
{'loss': 0.9844, 'learning_rate': 0.00016153846153846155, 'epoch': 0.02}
{'loss': 0.9854, 'learning_rate': 0.00016282051282051282, 'epoch': 0.02}
{'loss': 0.9807, 'learning_rate': 0.0001641025641025641, 'epoch': 0.02}
{'loss': 0.9287, 'learning_rate': 0.0001653846153846154, 'epoch': 0.02}
{'loss': 0.9695, 'learning_rate': 0.0001666666666666667, 'epoch': 0.03}
{'loss': 0.9456, 'learning_rate': 0.00016794871794871796, 'epoch': 0.03}
{'loss': 0.8809, 'learning_rate': 0.00016923076923076923, 'epoch': 0.03}
{'loss': 0.8887, 'learning_rate': 0.00017051282051282053, 'epoch': 0.03}
{'loss': 0.967, 'learning_rate': 0.0001717948717948718, 'epoch': 0.03}
{'loss': 1.0005, 'learning_rate': 0.0001730769230769231, 'epoch': 0.03}
{'loss': 0.9448, 'learning_rate': 0.00017435897435897436, 'epoch': 0.03}
{'loss': 0.9766, 'learning_rate': 0.00017564102564102566, 'epoch': 0.03}
{'loss': 0.9543, 'learning_rate': 0.00017692307692307693, 'epoch': 0.03}
{'loss': 0.9646, 'learning_rate': 0.00017820512820512823, 'epoch': 0.03}
{'loss': 0.9333, 'learning_rate': 0.0001794871794871795, 'epoch': 0.03}
{'loss': 0.8806, 'learning_rate': 0.00018076923076923077, 'epoch': 0.03}
{'loss': 0.8926, 'learning_rate': 0.00018205128205128207, 'epoch': 0.03}
{'loss': 0.9619, 'learning_rate': 0.00018333333333333334, 'epoch': 0.03}
{'loss': 0.9856, 'learning_rate': 0.00018461538461538463, 'epoch': 0.03}
{'loss': 0.8361, 'learning_rate': 0.0001858974358974359, 'epoch': 0.03}
{'loss': 0.9751, 'learning_rate': 0.0001871794871794872, 'epoch': 0.03}
{'loss': 0.9829, 'learning_rate': 0.00018846153846153847, 'epoch': 0.03}
{'loss': 0.9294, 'learning_rate': 0.00018974358974358974, 'epoch': 0.03}
{'loss': 0.9277, 'learning_rate': 0.00019102564102564104, 'epoch': 0.03}
{'loss': 0.95, 'learning_rate': 0.00019230769230769233, 'epoch': 0.03}
WARNING: tokenization mismatch: 1 vs. 789. (ignored)
{'loss': 0.95, 'learning_rate': 0.0001935897435897436, 'epoch': 0.03}
{'loss': 0.3184, 'learning_rate': 0.00019487179487179487, 'epoch': 0.03}
{'loss': 0.9746, 'learning_rate': 0.00019615384615384615, 'epoch': 0.03}
{'loss': 0.8857, 'learning_rate': 0.00019743589743589744, 'epoch': 0.03}
{'loss': 0.9485, 'learning_rate': 0.00019871794871794874, 'epoch': 0.03}
{'loss': 0.9569, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.9331, 'learning_rate': 0.00019999998058057615, 'epoch': 0.03}
{'loss': 0.9224, 'learning_rate': 0.00019999992232231216, 'epoch': 0.03}
{'loss': 0.9404, 'learning_rate': 0.0001999998252252306, 'epoch': 0.03}
{'loss': 0.9463, 'learning_rate': 0.00019999968928936926, 'epoch': 0.03}
{'loss': 0.9165, 'learning_rate': 0.00019999951451478087, 'epoch': 0.03}
{'loss': 0.9539, 'learning_rate': 0.00019999930090153334, 'epoch': 0.03}
{'loss': 0.8617, 'learning_rate': 0.00019999904844970962, 'epoch': 0.03}
{'loss': 0.2985, 'learning_rate': 0.00019999875715940782, 'epoch': 0.03}
{'loss': 1.0139, 'learning_rate': 0.000199998427030741, 'epoch': 0.03}
{'loss': 0.946, 'learning_rate': 0.00019999805806383738, 'epoch': 0.03}
{'loss': 0.9907, 'learning_rate': 0.0001999976502588403, 'epoch': 0.03}
{'loss': 0.9353, 'learning_rate': 0.0001999972036159081, 'epoch': 0.03}
{'loss': 0.8906, 'learning_rate': 0.00019999671813521435, 'epoch': 0.03}
{'loss': 0.9045, 'learning_rate': 0.0001999961938169475, 'epoch': 0.03}
{'loss': 0.9089, 'learning_rate': 0.00019999563066131124, 'epoch': 0.03}
{'loss': 0.937, 'learning_rate': 0.00019999502866852425, 'epoch': 0.03}
{'loss': 0.9707, 'learning_rate': 0.0001999943878388204, 'epoch': 0.03}
{'loss': 0.8768, 'learning_rate': 0.00019999370817244853, 'epoch': 0.03}
{'loss': 0.9143, 'learning_rate': 0.00019999298966967265, 'epoch': 0.03}
{'loss': 0.8933, 'learning_rate': 0.00019999223233077177, 'epoch': 0.03}
{'loss': 0.8579, 'learning_rate': 0.0001999914361560401, 'epoch': 0.03}
{'loss': 0.9163, 'learning_rate': 0.00019999060114578684, 'epoch': 0.03}
{'loss': 0.9265, 'learning_rate': 0.00019998972730033622, 'epoch': 0.03}
{'loss': 0.2885, 'learning_rate': 0.00019998881462002778, 'epoch': 0.03}
{'loss': 0.925, 'learning_rate': 0.00019998786310521585, 'epoch': 0.03}
{'loss': 0.9209, 'learning_rate': 0.00019998687275627006, 'epoch': 0.04}
{'loss': 0.9556, 'learning_rate': 0.00019998584357357502, 'epoch': 0.04}
{'loss': 0.9731, 'learning_rate': 0.00019998477555753055, 'epoch': 0.04}
{'loss': 0.9717, 'learning_rate': 0.00019998366870855133, 'epoch': 0.04}
{'loss': 0.9639, 'learning_rate': 0.0001999825230270673, 'epoch': 0.04}
{'loss': 0.3088, 'learning_rate': 0.0001999813385135234, 'epoch': 0.04}
{'loss': 0.9619, 'learning_rate': 0.00019998011516837974, 'epoch': 0.04}
{'loss': 0.9602, 'learning_rate': 0.0001999788529921114, 'epoch': 0.04}
{'loss': 0.2802, 'learning_rate': 0.0001999775519852086, 'epoch': 0.04}
{'loss': 0.9504, 'learning_rate': 0.00019997621214817667, 'epoch': 0.04}
{'loss': 0.8931, 'learning_rate': 0.000199974833481536, 'epoch': 0.04}
{'loss': 0.957, 'learning_rate': 0.00019997341598582195, 'epoch': 0.04}
{'loss': 0.9585, 'learning_rate': 0.00019997195966158518, 'epoch': 0.04}
{'loss': 0.9485, 'learning_rate': 0.0001999704645093912, 'epoch': 0.04}
{'loss': 0.9614, 'learning_rate': 0.0001999689305298208, 'epoch': 0.04}
{'loss': 0.8765, 'learning_rate': 0.00019996735772346972, 'epoch': 0.04}
{'loss': 0.8201, 'learning_rate': 0.00019996574609094884, 'epoch': 0.04}
{'loss': 0.9451, 'learning_rate': 0.00019996409563288406, 'epoch': 0.04}
{'loss': 0.8931, 'learning_rate': 0.00019996240634991642, 'epoch': 0.04}
{'loss': 0.8748, 'learning_rate': 0.00019996067824270205, 'epoch': 0.04}
{'loss': 0.9617, 'learning_rate': 0.00019995891131191205, 'epoch': 0.04}
{'loss': 0.8643, 'learning_rate': 0.00019995710555823276, 'epoch': 0.04}
{'loss': 0.9775, 'learning_rate': 0.00019995526098236547, 'epoch': 0.04}
{'loss': 0.284, 'learning_rate': 0.0001999533775850266, 'epoch': 0.04}
{'loss': 0.9128, 'learning_rate': 0.00019995145536694762, 'epoch': 0.04}
{'loss': 0.9187, 'learning_rate': 0.00019994949432887514, 'epoch': 0.04}
{'loss': 0.936, 'learning_rate': 0.00019994749447157077, 'epoch': 0.04}
{'loss': 0.9246, 'learning_rate': 0.00019994545579581123, 'epoch': 0.04}
{'loss': 0.9355, 'learning_rate': 0.00019994337830238834, 'epoch': 0.04}
{'loss': 0.9187, 'learning_rate': 0.00019994126199210897, 'epoch': 0.04}
{'loss': 0.8481, 'learning_rate': 0.00019993910686579507, 'epoch': 0.04}
{'loss': 0.8833, 'learning_rate': 0.00019993691292428365, 'epoch': 0.04}
{'loss': 0.9189, 'learning_rate': 0.00019993468016842682, 'epoch': 0.04}
{'loss': 0.8824, 'learning_rate': 0.00019993240859909176, 'epoch': 0.04}
{'loss': 0.9297, 'learning_rate': 0.00019993009821716074, 'epoch': 0.04}
{'loss': 0.3096, 'learning_rate': 0.00019992774902353105, 'epoch': 0.04}
{'loss': 0.3087, 'learning_rate': 0.0001999253610191151, 'epoch': 0.04}
{'loss': 0.9275, 'learning_rate': 0.00019992293420484039, 'epoch': 0.04}
{'loss': 0.3259, 'learning_rate': 0.00019992046858164944, 'epoch': 0.04}
{'loss': 1.0107, 'learning_rate': 0.0001999179641504999, 'epoch': 0.04}
{'loss': 0.9651, 'learning_rate': 0.00019991542091236437, 'epoch': 0.04}
{'loss': 0.9358, 'learning_rate': 0.00019991283886823074, 'epoch': 0.04}
{'loss': 0.95, 'learning_rate': 0.00019991021801910177, 'epoch': 0.04}
{'loss': 0.9104, 'learning_rate': 0.00019990755836599538, 'epoch': 0.04}
{'loss': 0.29, 'learning_rate': 0.0001999048599099446, 'epoch': 0.04}
{'loss': 0.9363, 'learning_rate': 0.00019990212265199738, 'epoch': 0.04}
{'loss': 0.9373, 'learning_rate': 0.0001998993465932169, 'epoch': 0.04}
{'loss': 0.9214, 'learning_rate': 0.00019989653173468135, 'epoch': 0.04}
{'loss': 0.949, 'learning_rate': 0.000199893678077484, 'epoch': 0.04}
{'loss': 0.8945, 'learning_rate': 0.00019989078562273314, 'epoch': 0.04}
{'loss': 0.9373, 'learning_rate': 0.0001998878543715522, 'epoch': 0.04}
{'loss': 0.9355, 'learning_rate': 0.0001998848843250796, 'epoch': 0.04}
{'loss': 0.8748, 'learning_rate': 0.00019988187548446894, 'epoch': 0.05}
{'loss': 0.8923, 'learning_rate': 0.0001998788278508888, 'epoch': 0.05}
{'loss': 0.8435, 'learning_rate': 0.00019987574142552275, 'epoch': 0.05}
{'loss': 0.8574, 'learning_rate': 0.00019987261620956964, 'epoch': 0.05}
{'loss': 0.9287, 'learning_rate': 0.00019986945220424324, 'epoch': 0.05}
{'loss': 0.9236, 'learning_rate': 0.00019986624941077238, 'epoch': 0.05}
{'loss': 1.0125, 'learning_rate': 0.000199863007830401, 'epoch': 0.05}
{'loss': 1.0103, 'learning_rate': 0.0001998597274643881, 'epoch': 0.05}
{'loss': 0.936, 'learning_rate': 0.00019985640831400776, 'epoch': 0.05}
{'loss': 0.8994, 'learning_rate': 0.0001998530503805491, 'epoch': 0.05}
{'loss': 0.9067, 'learning_rate': 0.00019984965366531623, 'epoch': 0.05}
{'loss': 0.9226, 'learning_rate': 0.00019984621816962844, 'epoch': 0.05}
{'loss': 0.9253, 'learning_rate': 0.00019984274389482005, 'epoch': 0.05}
{'loss': 0.9578, 'learning_rate': 0.00019983923084224045, 'epoch': 0.05}
{'loss': 0.9138, 'learning_rate': 0.00019983567901325403, 'epoch': 0.05}
{'loss': 0.9519, 'learning_rate': 0.00019983208840924026, 'epoch': 0.05}
{'loss': 0.9338, 'learning_rate': 0.0001998284590315937, 'epoch': 0.05}
{'loss': 0.3222, 'learning_rate': 0.00019982479088172405, 'epoch': 0.05}
{'loss': 0.9553, 'learning_rate': 0.00019982108396105584, 'epoch': 0.05}
{'loss': 0.9445, 'learning_rate': 0.00019981733827102884, 'epoch': 0.05}
{'loss': 0.9258, 'learning_rate': 0.00019981355381309789, 'epoch': 0.05}
{'loss': 0.8726, 'learning_rate': 0.0001998097305887328, 'epoch': 0.05}
{'loss': 0.905, 'learning_rate': 0.00019980586859941847, 'epoch': 0.05}
{'loss': 0.9221, 'learning_rate': 0.00019980196784665478, 'epoch': 0.05}
{'loss': 0.8999, 'learning_rate': 0.00019979802833195682, 'epoch': 0.05}
{'loss': 0.8582, 'learning_rate': 0.00019979405005685465, 'epoch': 0.05}
{'loss': 0.9182, 'learning_rate': 0.00019979003302289335, 'epoch': 0.05}
{'loss': 0.9482, 'learning_rate': 0.0001997859772316331, 'epoch': 0.05}
{'loss': 0.8979, 'learning_rate': 0.00019978188268464912, 'epoch': 0.05}
{'loss': 0.9253, 'learning_rate': 0.0001997777493835317, 'epoch': 0.05}
{'loss': 0.8931, 'learning_rate': 0.00019977357732988614, 'epoch': 0.05}
{'loss': 0.9336, 'learning_rate': 0.0001997693665253329, 'epoch': 0.05}
{'loss': 0.9722, 'learning_rate': 0.0001997651169715073, 'epoch': 0.05}
{'loss': 0.9683, 'learning_rate': 0.00019976082867005984, 'epoch': 0.05}
{'loss': 0.9355, 'learning_rate': 0.00019975650162265608, 'epoch': 0.05}
{'loss': 0.861, 'learning_rate': 0.0001997521358309766, 'epoch': 0.05}
{'loss': 0.8916, 'learning_rate': 0.00019974773129671701, 'epoch': 0.05}
{'loss': 0.8999, 'learning_rate': 0.00019974328802158797, 'epoch': 0.05}
{'loss': 0.9026, 'learning_rate': 0.0001997388060073152, 'epoch': 0.05}
{'loss': 0.8762, 'learning_rate': 0.00019973428525563947, 'epoch': 0.05}
{'loss': 0.9453, 'learning_rate': 0.00019972972576831656, 'epoch': 0.05}
{'loss': 0.874, 'learning_rate': 0.0001997251275471174, 'epoch': 0.05}
{'loss': 0.9182, 'learning_rate': 0.00019972049059382782, 'epoch': 0.05}
{'loss': 0.9124, 'learning_rate': 0.00019971581491024873, 'epoch': 0.05}
{'loss': 0.9468, 'learning_rate': 0.0001997111004981962, 'epoch': 0.05}
{'loss': 0.876, 'learning_rate': 0.00019970634735950115, 'epoch': 0.05}
{'loss': 0.9543, 'learning_rate': 0.00019970155549600978, 'epoch': 0.05}
{'loss': 0.928, 'learning_rate': 0.00019969672490958304, 'epoch': 0.05}
{'loss': 0.9612, 'learning_rate': 0.0001996918556020972, 'epoch': 0.05}
{'loss': 0.9336, 'learning_rate': 0.0001996869475754434, 'epoch': 0.05}
{'loss': 0.8745, 'learning_rate': 0.00019968200083152782, 'epoch': 0.05}
{'loss': 0.8489, 'learning_rate': 0.00019967701537227175, 'epoch': 0.05}
{'loss': 0.8901, 'learning_rate': 0.00019967199119961152, 'epoch': 0.06}
{'loss': 0.9211, 'learning_rate': 0.0001996669283154984, 'epoch': 0.06}
{'loss': 0.9207, 'learning_rate': 0.0001996618267218988, 'epoch': 0.06}
{'loss': 0.8992, 'learning_rate': 0.00019965668642079408, 'epoch': 0.06}
{'loss': 0.8723, 'learning_rate': 0.00019965150741418073, 'epoch': 0.06}
{'loss': 0.8733, 'learning_rate': 0.0001996462897040702, 'epoch': 0.06}
{'loss': 0.8462, 'learning_rate': 0.0001996410332924889, 'epoch': 0.06}
{'loss': 0.9866, 'learning_rate': 0.0001996357381814785, 'epoch': 0.06}
{'loss': 0.9307, 'learning_rate': 0.00019963040437309549, 'epoch': 0.06}
{'loss': 0.9529, 'learning_rate': 0.00019962503186941142, 'epoch': 0.06}
{'loss': 0.8784, 'learning_rate': 0.00019961962067251298, 'epoch': 0.06}
{'loss': 0.3315, 'learning_rate': 0.00019961417078450178, 'epoch': 0.06}
{'loss': 0.9031, 'learning_rate': 0.00019960868220749448, 'epoch': 0.06}
{'loss': 0.9226, 'learning_rate': 0.00019960315494362284, 'epoch': 0.06}
{'loss': 0.8516, 'learning_rate': 0.00019959758899503353, 'epoch': 0.06}
{'loss': 0.9021, 'learning_rate': 0.0001995919843638883, 'epoch': 0.06}
{'loss': 0.925, 'learning_rate': 0.00019958634105236395, 'epoch': 0.06}
{'loss': 0.8899, 'learning_rate': 0.00019958065906265228, 'epoch': 0.06}
{'loss': 0.9685, 'learning_rate': 0.0001995749383969601, 'epoch': 0.06}
{'loss': 0.8278, 'learning_rate': 0.00019956917905750924, 'epoch': 0.06}
{'loss': 0.9199, 'learning_rate': 0.00019956338104653657, 'epoch': 0.06}
{'loss': 0.907, 'learning_rate': 0.00019955754436629399, 'epoch': 0.06}
{'loss': 0.9004, 'learning_rate': 0.00019955166901904837, 'epoch': 0.06}
{'loss': 0.9473, 'learning_rate': 0.00019954575500708162, 'epoch': 0.06}
{'loss': 0.989, 'learning_rate': 0.0001995398023326907, 'epoch': 0.06}
{'loss': 0.9221, 'learning_rate': 0.00019953381099818755, 'epoch': 0.06}
{'loss': 0.9202, 'learning_rate': 0.00019952778100589913, 'epoch': 0.06}
{'loss': 0.9294, 'learning_rate': 0.00019952171235816747, 'epoch': 0.06}
{'loss': 0.9058, 'learning_rate': 0.00019951560505734945, 'epoch': 0.06}
{'loss': 0.9067, 'learning_rate': 0.00019950945910581717, 'epoch': 0.06}
{'loss': 0.9092, 'learning_rate': 0.00019950327450595764, 'epoch': 0.06}
{'loss': 0.9128, 'learning_rate': 0.00019949705126017287, 'epoch': 0.06}
{'loss': 0.9675, 'learning_rate': 0.00019949078937087986, 'epoch': 0.06}
{'loss': 0.9001, 'learning_rate': 0.0001994844888405107, 'epoch': 0.06}
{'loss': 0.9247, 'learning_rate': 0.00019947814967151244, 'epoch': 0.06}
{'loss': 0.9045, 'learning_rate': 0.00019947177186634715, 'epoch': 0.06}
{'loss': 0.3507, 'learning_rate': 0.00019946535542749184, 'epoch': 0.06}
{'loss': 0.9397, 'learning_rate': 0.00019945890035743866, 'epoch': 0.06}
{'loss': 0.9399, 'learning_rate': 0.00019945240665869465, 'epoch': 0.06}
{'loss': 0.8708, 'learning_rate': 0.00019944587433378186, 'epoch': 0.06}
{'loss': 0.8521, 'learning_rate': 0.0001994393033852374, 'epoch': 0.06}
{'loss': 0.9795, 'learning_rate': 0.00019943269381561334, 'epoch': 0.06}
{'loss': 0.8896, 'learning_rate': 0.00019942604562747678, 'epoch': 0.06}
{'loss': 0.8833, 'learning_rate': 0.00019941935882340976, 'epoch': 0.06}
{'loss': 0.8196, 'learning_rate': 0.00019941263340600939, 'epoch': 0.06}
{'loss': 0.8691, 'learning_rate': 0.00019940586937788776, 'epoch': 0.06}
{'loss': 0.8901, 'learning_rate': 0.0001993990667416719, 'epoch': 0.06}
{'loss': 0.9414, 'learning_rate': 0.0001993922255000039, 'epoch': 0.06}
{'loss': 0.9014, 'learning_rate': 0.0001993853456555408, 'epoch': 0.06}
{'loss': 0.8671, 'learning_rate': 0.00019937842721095468, 'epoch': 0.06}
{'loss': 0.9219, 'learning_rate': 0.00019937147016893257, 'epoch': 0.06}
{'loss': 0.9233, 'learning_rate': 0.00019936447453217646, 'epoch': 0.06}
{'loss': 0.8789, 'learning_rate': 0.00019935744030340346, 'epoch': 0.07}
{'loss': 0.9084, 'learning_rate': 0.00019935036748534553, 'epoch': 0.07}
{'loss': 0.8624, 'learning_rate': 0.0001993432560807497, 'epoch': 0.07}
{'loss': 0.9312, 'learning_rate': 0.00019933610609237793, 'epoch': 0.07}
{'loss': 0.8567, 'learning_rate': 0.00019932891752300717, 'epoch': 0.07}
{'loss': 0.9763, 'learning_rate': 0.00019932169037542946, 'epoch': 0.07}
{'loss': 0.9211, 'learning_rate': 0.00019931442465245165, 'epoch': 0.07}
{'loss': 0.895, 'learning_rate': 0.00019930712035689575, 'epoch': 0.07}
{'loss': 0.9099, 'learning_rate': 0.00019929977749159859, 'epoch': 0.07}
{'loss': 0.894, 'learning_rate': 0.00019929239605941208, 'epoch': 0.07}
{'loss': 0.8926, 'learning_rate': 0.0001992849760632031, 'epoch': 0.07}
{'loss': 0.939, 'learning_rate': 0.00019927751750585347, 'epoch': 0.07}
{'loss': 0.9006, 'learning_rate': 0.00019927002039026002, 'epoch': 0.07}
{'loss': 0.9666, 'learning_rate': 0.00019926248471933454, 'epoch': 0.07}
{'loss': 0.3365, 'learning_rate': 0.0001992549104960038, 'epoch': 0.07}
{'loss': 0.8638, 'learning_rate': 0.0001992472977232095, 'epoch': 0.07}
{'loss': 0.9158, 'learning_rate': 0.00019923964640390843, 'epoch': 0.07}
{'loss': 0.8989, 'learning_rate': 0.00019923195654107225, 'epoch': 0.07}
{'loss': 0.9307, 'learning_rate': 0.00019922422813768758, 'epoch': 0.07}
{'loss': 0.9424, 'learning_rate': 0.00019921646119675605, 'epoch': 0.07}
{'loss': 0.9009, 'learning_rate': 0.00019920865572129425, 'epoch': 0.07}
{'loss': 0.9475, 'learning_rate': 0.00019920081171433379, 'epoch': 0.07}
{'loss': 0.9468, 'learning_rate': 0.00019919292917892112, 'epoch': 0.07}
{'loss': 0.9087, 'learning_rate': 0.00019918500811811778, 'epoch': 0.07}
{'loss': 0.9475, 'learning_rate': 0.00019917704853500016, 'epoch': 0.07}
{'loss': 0.8689, 'learning_rate': 0.00019916905043265972, 'epoch': 0.07}
{'loss': 1.0037, 'learning_rate': 0.00019916101381420285, 'epoch': 0.07}
{'loss': 0.8599, 'learning_rate': 0.00019915293868275083, 'epoch': 0.07}
{'loss': 0.8762, 'learning_rate': 0.00019914482504143995, 'epoch': 0.07}
{'loss': 0.9089, 'learning_rate': 0.00019913667289342147, 'epoch': 0.07}
{'loss': 0.937, 'learning_rate': 0.0001991284822418616, 'epoch': 0.07}
{'loss': 0.9359, 'learning_rate': 0.00019912025308994148, 'epoch': 0.07}
{'loss': 0.8574, 'learning_rate': 0.00019911198544085722, 'epoch': 0.07}
{'loss': 0.9246, 'learning_rate': 0.00019910367929781988, 'epoch': 0.07}
{'loss': 0.8926, 'learning_rate': 0.00019909533466405546, 'epoch': 0.07}
{'loss': 0.9182, 'learning_rate': 0.00019908695154280496, 'epoch': 0.07}
{'loss': 0.9376, 'learning_rate': 0.00019907852993732424, 'epoch': 0.07}
{'loss': 0.958, 'learning_rate': 0.0001990700698508842, 'epoch': 0.07}
{'loss': 0.8777, 'learning_rate': 0.0001990615712867706, 'epoch': 0.07}
{'loss': 0.9368, 'learning_rate': 0.00019905303424828417, 'epoch': 0.07}
{'loss': 0.907, 'learning_rate': 0.00019904445873874068, 'epoch': 0.07}
{'loss': 0.9004, 'learning_rate': 0.00019903584476147065, 'epoch': 0.07}
{'loss': 0.8679, 'learning_rate': 0.00019902719231981974, 'epoch': 0.07}
{'loss': 0.9607, 'learning_rate': 0.00019901850141714841, 'epoch': 0.07}
{'loss': 0.8894, 'learning_rate': 0.0001990097720568321, 'epoch': 0.07}
{'loss': 0.9092, 'learning_rate': 0.00019900100424226125, 'epoch': 0.07}
{'loss': 0.9102, 'learning_rate': 0.00019899219797684113, 'epoch': 0.07}
{'loss': 0.8672, 'learning_rate': 0.000198983353263992, 'epoch': 0.07}
{'loss': 0.8889, 'learning_rate': 0.00019897447010714905, 'epoch': 0.07}
{'loss': 0.8743, 'learning_rate': 0.00019896554850976238, 'epoch': 0.07}
{'loss': 1.0056, 'learning_rate': 0.00019895658847529708, 'epoch': 0.07}
{'loss': 0.9153, 'learning_rate': 0.00019894759000723306, 'epoch': 0.07}
{'loss': 0.8662, 'learning_rate': 0.00019893855310906526, 'epoch': 0.08}
{'loss': 0.9695, 'learning_rate': 0.0001989294777843035, 'epoch': 0.08}
{'loss': 0.8965, 'learning_rate': 0.00019892036403647254, 'epoch': 0.08}
{'loss': 0.8552, 'learning_rate': 0.00019891121186911206, 'epoch': 0.08}
{'loss': 0.9138, 'learning_rate': 0.00019890202128577662, 'epoch': 0.08}
{'loss': 0.9451, 'learning_rate': 0.00019889279229003576, 'epoch': 0.08}
{'loss': 0.939, 'learning_rate': 0.00019888352488547394, 'epoch': 0.08}
{'loss': 0.8894, 'learning_rate': 0.00019887421907569048, 'epoch': 0.08}
{'loss': 0.9219, 'learning_rate': 0.00019886487486429964, 'epoch': 0.08}
{'loss': 0.9602, 'learning_rate': 0.00019885549225493064, 'epoch': 0.08}
{'loss': 0.9199, 'learning_rate': 0.00019884607125122755, 'epoch': 0.08}
{'loss': 0.8752, 'learning_rate': 0.0001988366118568494, 'epoch': 0.08}
{'loss': 0.8635, 'learning_rate': 0.0001988271140754701, 'epoch': 0.08}
{'loss': 0.8516, 'learning_rate': 0.00019881757791077845, 'epoch': 0.08}
{'loss': 0.8865, 'learning_rate': 0.00019880800336647824, 'epoch': 0.08}
{'loss': 0.9172, 'learning_rate': 0.0001987983904462881, 'epoch': 0.08}
{'loss': 0.8911, 'learning_rate': 0.00019878873915394154, 'epoch': 0.08}
{'loss': 0.8767, 'learning_rate': 0.00019877904949318703, 'epoch': 0.08}
{'loss': 0.8796, 'learning_rate': 0.00019876932146778794, 'epoch': 0.08}
{'loss': 0.3301, 'learning_rate': 0.00019875955508152253, 'epoch': 0.08}
{'loss': 0.3086, 'learning_rate': 0.0001987497503381839, 'epoch': 0.08}
{'loss': 0.8655, 'learning_rate': 0.00019873990724158014, 'epoch': 0.08}
{'loss': 0.9229, 'learning_rate': 0.00019873002579553418, 'epoch': 0.08}
{'loss': 0.9148, 'learning_rate': 0.00019872010600388392, 'epoch': 0.08}
{'loss': 0.8884, 'learning_rate': 0.00019871014787048197, 'epoch': 0.08}
{'loss': 0.9253, 'learning_rate': 0.00019870015139919606, 'epoch': 0.08}
{'loss': 0.9246, 'learning_rate': 0.00019869011659390866, 'epoch': 0.08}
{'loss': 0.9417, 'learning_rate': 0.00019868004345851716, 'epoch': 0.08}
{'loss': 0.886, 'learning_rate': 0.00019866993199693392, 'epoch': 0.08}
{'loss': 0.8945, 'learning_rate': 0.000198659782213086, 'epoch': 0.08}
{'loss': 0.885, 'learning_rate': 0.00019864959411091556, 'epoch': 0.08}
{'loss': 0.9497, 'learning_rate': 0.00019863936769437955, 'epoch': 0.08}
{'loss': 0.8772, 'learning_rate': 0.00019862910296744967, 'epoch': 0.08}
{'loss': 0.8877, 'learning_rate': 0.00019861879993411275, 'epoch': 0.08}
{'loss': 0.8804, 'learning_rate': 0.00019860845859837032, 'epoch': 0.08}
{'loss': 0.9155, 'learning_rate': 0.00019859807896423882, 'epoch': 0.08}
{'loss': 0.9529, 'learning_rate': 0.0001985876610357496, 'epoch': 0.08}
{'loss': 0.8657, 'learning_rate': 0.00019857720481694885, 'epoch': 0.08}
{'loss': 0.875, 'learning_rate': 0.00019856671031189766, 'epoch': 0.08}
{'loss': 0.8699, 'learning_rate': 0.000198556177524672, 'epoch': 0.08}
{'loss': 0.894, 'learning_rate': 0.0001985456064593626, 'epoch': 0.08}
{'loss': 0.9568, 'learning_rate': 0.00019853499712007522, 'epoch': 0.08}
{'loss': 0.9172, 'learning_rate': 0.00019852434951093034, 'epoch': 0.08}
{'loss': 0.8516, 'learning_rate': 0.00019851366363606346, 'epoch': 0.08}
{'loss': 0.3937, 'learning_rate': 0.00019850293949962478, 'epoch': 0.08}
{'loss': 0.3121, 'learning_rate': 0.00019849217710577946, 'epoch': 0.08}
{'loss': 0.9429, 'learning_rate': 0.00019848137645870747, 'epoch': 0.08}
{'loss': 0.8811, 'learning_rate': 0.0001984705375626036, 'epoch': 0.08}
{'loss': 0.8508, 'learning_rate': 0.0001984596604216777, 'epoch': 0.08}
{'loss': 0.8628, 'learning_rate': 0.0001984487450401542, 'epoch': 0.08}
{'loss': 0.9043, 'learning_rate': 0.00019843779142227256, 'epoch': 0.08}
{'loss': 0.8936, 'learning_rate': 0.00019842679957228704, 'epoch': 0.08}
{'loss': 0.8705, 'learning_rate': 0.00019841576949446675, 'epoch': 0.09}
{'loss': 0.8936, 'learning_rate': 0.0001984047011930956, 'epoch': 0.09}
{'loss': 0.9167, 'learning_rate': 0.00019839359467247242, 'epoch': 0.09}
{'loss': 0.9146, 'learning_rate': 0.0001983824499369109, 'epoch': 0.09}
{'loss': 0.9382, 'learning_rate': 0.00019837126699073947, 'epoch': 0.09}
{'loss': 0.8772, 'learning_rate': 0.00019836004583830146, 'epoch': 0.09}
{'loss': 0.946, 'learning_rate': 0.00019834878648395505, 'epoch': 0.09}
{'loss': 0.8931, 'learning_rate': 0.00019833748893207325, 'epoch': 0.09}
{'loss': 0.9095, 'learning_rate': 0.00019832615318704389, 'epoch': 0.09}
{'loss': 0.9299, 'learning_rate': 0.00019831477925326963, 'epoch': 0.09}
{'loss': 0.8527, 'learning_rate': 0.00019830336713516799, 'epoch': 0.09}
{'loss': 0.9451, 'learning_rate': 0.00019829191683717133, 'epoch': 0.09}
{'loss': 0.8872, 'learning_rate': 0.00019828042836372677, 'epoch': 0.09}
{'loss': 0.8831, 'learning_rate': 0.00019826890171929632, 'epoch': 0.09}
{'loss': 0.8647, 'learning_rate': 0.00019825733690835679, 'epoch': 0.09}
{'loss': 0.9146, 'learning_rate': 0.00019824573393539984, 'epoch': 0.09}
{'loss': 0.9309, 'learning_rate': 0.0001982340928049319, 'epoch': 0.09}
{'loss': 0.8354, 'learning_rate': 0.00019822241352147427, 'epoch': 0.09}
{'loss': 0.9487, 'learning_rate': 0.00019821069608956307, 'epoch': 0.09}
{'loss': 0.8318, 'learning_rate': 0.00019819894051374915, 'epoch': 0.09}
{'loss': 0.9458, 'learning_rate': 0.0001981871467985983, 'epoch': 0.09}
{'loss': 0.8796, 'learning_rate': 0.00019817531494869105, 'epoch': 0.09}
{'loss': 0.8599, 'learning_rate': 0.00019816344496862272, 'epoch': 0.09}
{'loss': 0.9177, 'learning_rate': 0.00019815153686300352, 'epoch': 0.09}
{'loss': 0.8467, 'learning_rate': 0.0001981395906364584, 'epoch': 0.09}
{'loss': 0.8168, 'learning_rate': 0.00019812760629362716, 'epoch': 0.09}
{'loss': 0.9265, 'learning_rate': 0.0001981155838391643, 'epoch': 0.09}
{'loss': 0.8821, 'learning_rate': 0.00019810352327773935, 'epoch': 0.09}
{'loss': 0.8994, 'learning_rate': 0.00019809142461403633, 'epoch': 0.09}
{'loss': 0.8744, 'learning_rate': 0.00019807928785275434, 'epoch': 0.09}
{'loss': 0.9692, 'learning_rate': 0.0001980671129986071, 'epoch': 0.09}
{'loss': 0.9434, 'learning_rate': 0.0001980549000563232, 'epoch': 0.09}
{'loss': 0.918, 'learning_rate': 0.000198042649030646, 'epoch': 0.09}
{'loss': 0.9192, 'learning_rate': 0.00019803035992633366, 'epoch': 0.09}
{'loss': 0.9304, 'learning_rate': 0.00019801803274815917, 'epoch': 0.09}
{'loss': 0.8936, 'learning_rate': 0.00019800566750091016, 'epoch': 0.09}
WARNING: tokenization mismatch: 1 vs. 64. (ignored)
{'loss': 0.897, 'learning_rate': 0.00019799326418938924, 'epoch': 0.09}
{'loss': 0.906, 'learning_rate': 0.0001979808228184137, 'epoch': 0.09}
{'loss': 0.8926, 'learning_rate': 0.0001979683433928156, 'epoch': 0.09}
{'loss': 0.9111, 'learning_rate': 0.0001979558259174418, 'epoch': 0.09}
{'loss': 0.9348, 'learning_rate': 0.00019794327039715395, 'epoch': 0.09}
{'loss': 0.9272, 'learning_rate': 0.0001979306768368285, 'epoch': 0.09}
{'loss': 0.8909, 'learning_rate': 0.0001979180452413566, 'epoch': 0.09}
{'loss': 0.7616, 'learning_rate': 0.00019790537561564428, 'epoch': 0.09}
{'loss': 0.8914, 'learning_rate': 0.00019789266796461222, 'epoch': 0.09}
{'loss': 0.897, 'learning_rate': 0.00019787992229319592, 'epoch': 0.09}
{'loss': 0.936, 'learning_rate': 0.00019786713860634567, 'epoch': 0.09}
{'loss': 0.406, 'learning_rate': 0.00019785431690902652, 'epoch': 0.09}
{'loss': 0.8942, 'learning_rate': 0.00019784145720621826, 'epoch': 0.09}
{'loss': 0.9172, 'learning_rate': 0.00019782855950291542, 'epoch': 0.09}
{'loss': 0.8848, 'learning_rate': 0.0001978156238041274, 'epoch': 0.09}
{'loss': 0.8931, 'learning_rate': 0.0001978026501148782, 'epoch': 0.09}
{'loss': 0.96, 'learning_rate': 0.00019778963844020665, 'epoch': 0.1}
{'loss': 0.8403, 'learning_rate': 0.00019777658878516639, 'epoch': 0.1}
{'loss': 0.321, 'learning_rate': 0.0001977635011548257, 'epoch': 0.1}
{'loss': 0.8953, 'learning_rate': 0.0001977503755542677, 'epoch': 0.1}
{'loss': 0.9194, 'learning_rate': 0.00019773721198859022, 'epoch': 0.1}
{'loss': 0.876, 'learning_rate': 0.00019772401046290586, 'epoch': 0.1}
{'loss': 0.9294, 'learning_rate': 0.00019771077098234186, 'epoch': 0.1}
{'loss': 0.843, 'learning_rate': 0.00019769749355204032, 'epoch': 0.1}
{'loss': 0.8357, 'learning_rate': 0.00019768417817715809, 'epoch': 0.1}
{'loss': 0.8999, 'learning_rate': 0.00019767082486286665, 'epoch': 0.1}
{'loss': 0.928, 'learning_rate': 0.0001976574336143523, 'epoch': 0.1}
{'loss': 0.8669, 'learning_rate': 0.00019764400443681606, 'epoch': 0.1}
{'loss': 0.87, 'learning_rate': 0.00019763053733547366, 'epoch': 0.1}
{'loss': 0.8611, 'learning_rate': 0.0001976170323155555, 'epoch': 0.1}
{'loss': 0.9478, 'learning_rate': 0.0001976034893823069, 'epoch': 0.1}
{'loss': 0.8899, 'learning_rate': 0.0001975899085409876, 'epoch': 0.1}
{'loss': 0.928, 'learning_rate': 0.00019757628979687246, 'epoch': 0.1}
{'loss': 0.8564, 'learning_rate': 0.0001975626331552507, 'epoch': 0.1}
{'loss': 0.4009, 'learning_rate': 0.00019754893862142643, 'epoch': 0.1}
{'loss': 0.8821, 'learning_rate': 0.00019753520620071843, 'epoch': 0.1}
{'loss': 0.8804, 'learning_rate': 0.0001975214358984603, 'epoch': 0.1}
{'loss': 0.8796, 'learning_rate': 0.00019750762772000014, 'epoch': 0.1}
{'loss': 0.8263, 'learning_rate': 0.000197493781670701, 'epoch': 0.1}
{'loss': 0.9067, 'learning_rate': 0.00019747989775594044, 'epoch': 0.1}
{'loss': 0.8821, 'learning_rate': 0.0001974659759811109, 'epoch': 0.1}
{'loss': 0.8992, 'learning_rate': 0.00019745201635161936, 'epoch': 0.1}
{'loss': 0.8694, 'learning_rate': 0.00019743801887288763, 'epoch': 0.1}
{'loss': 0.8857, 'learning_rate': 0.0001974239835503521, 'epoch': 0.1}
{'loss': 0.3453, 'learning_rate': 0.00019740991038946404, 'epoch': 0.1}
{'loss': 0.8972, 'learning_rate': 0.0001973957993956892, 'epoch': 0.1}
{'loss': 0.9431, 'learning_rate': 0.00019738165057450816, 'epoch': 0.1}
{'loss': 0.9089, 'learning_rate': 0.00019736746393141617, 'epoch': 0.1}
{'loss': 0.9473, 'learning_rate': 0.00019735323947192316, 'epoch': 0.1}
{'loss': 0.9114, 'learning_rate': 0.00019733897720155375, 'epoch': 0.1}
{'loss': 0.8831, 'learning_rate': 0.00019732467712584722, 'epoch': 0.1}
{'loss': 0.9197, 'learning_rate': 0.0001973103392503576, 'epoch': 0.1}
{'loss': 0.8401, 'learning_rate': 0.00019729596358065345, 'epoch': 0.1}
{'loss': 0.9248, 'learning_rate': 0.00019728155012231825, 'epoch': 0.1}
{'loss': 0.8936, 'learning_rate': 0.00019726709888094992, 'epoch': 0.1}
{'loss': 0.8582, 'learning_rate': 0.0001972526098621612, 'epoch': 0.1}
{'loss': 0.9204, 'learning_rate': 0.00019723808307157948, 'epoch': 0.1}
{'loss': 0.895, 'learning_rate': 0.00019722351851484676, 'epoch': 0.1}
{'loss': 0.8752, 'learning_rate': 0.00019720891619761974, 'epoch': 0.1}
{'loss': 0.886, 'learning_rate': 0.0001971942761255698, 'epoch': 0.1}
{'loss': 0.8423, 'learning_rate': 0.00019717959830438302, 'epoch': 0.1}
{'loss': 0.8774, 'learning_rate': 0.00019716488273976003, 'epoch': 0.1}
{'loss': 0.9075, 'learning_rate': 0.0001971501294374162, 'epoch': 0.1}
{'loss': 0.9102, 'learning_rate': 0.00019713533840308157, 'epoch': 0.1}
{'loss': 0.8645, 'learning_rate': 0.00019712050964250082, 'epoch': 0.1}
{'loss': 0.8572, 'learning_rate': 0.00019710564316143323, 'epoch': 0.1}
{'loss': 0.9077, 'learning_rate': 0.00019709073896565275, 'epoch': 0.1}
{'loss': 0.9001, 'learning_rate': 0.00019707579706094807, 'epoch': 0.1}
{'loss': 0.8508, 'learning_rate': 0.0001970608174531224, 'epoch': 0.11}
{'loss': 0.8901, 'learning_rate': 0.0001970458001479937, 'epoch': 0.11}
{'loss': 0.8601, 'learning_rate': 0.00019703074515139445, 'epoch': 0.11}
{'loss': 0.9026, 'learning_rate': 0.00019701565246917183, 'epoch': 0.11}
{'loss': 0.8877, 'learning_rate': 0.00019700052210718777, 'epoch': 0.11}
{'loss': 0.7808, 'learning_rate': 0.00019698535407131862, 'epoch': 0.11}
{'loss': 0.9465, 'learning_rate': 0.00019697014836745553, 'epoch': 0.11}
{'loss': 0.8921, 'learning_rate': 0.00019695490500150418, 'epoch': 0.11}
{'loss': 0.8477, 'learning_rate': 0.00019693962397938496, 'epoch': 0.11}
{'loss': 0.8335, 'learning_rate': 0.00019692430530703282, 'epoch': 0.11}
{'loss': 0.9009, 'learning_rate': 0.00019690894899039734, 'epoch': 0.11}
{'loss': 0.9387, 'learning_rate': 0.00019689355503544275, 'epoch': 0.11}
{'loss': 0.9233, 'learning_rate': 0.0001968781234481479, 'epoch': 0.11}
{'loss': 0.8862, 'learning_rate': 0.00019686265423450624, 'epoch': 0.11}
{'loss': 0.9031, 'learning_rate': 0.00019684714740052583, 'epoch': 0.11}
{'loss': 0.8503, 'learning_rate': 0.00019683160295222934, 'epoch': 0.11}
{'loss': 0.8745, 'learning_rate': 0.00019681602089565402, 'epoch': 0.11}
{'loss': 0.8586, 'learning_rate': 0.0001968004012368518, 'epoch': 0.11}
{'loss': 0.8921, 'learning_rate': 0.0001967847439818892, 'epoch': 0.11}
{'loss': 0.9409, 'learning_rate': 0.00019676904913684727, 'epoch': 0.11}
{'loss': 0.906, 'learning_rate': 0.0001967533167078217, 'epoch': 0.11}
{'loss': 0.9656, 'learning_rate': 0.00019673754670092284, 'epoch': 0.11}
{'loss': 0.936, 'learning_rate': 0.00019672173912227553, 'epoch': 0.11}
{'loss': 0.822, 'learning_rate': 0.0001967058939780193, 'epoch': 0.11}
{'loss': 0.8171, 'learning_rate': 0.00019669001127430816, 'epoch': 0.11}
{'loss': 0.8887, 'learning_rate': 0.00019667409101731083, 'epoch': 0.11}
{'loss': 0.9182, 'learning_rate': 0.0001966581332132105, 'epoch': 0.11}
{'loss': 0.8386, 'learning_rate': 0.00019664213786820502, 'epoch': 0.11}
{'loss': 0.8909, 'learning_rate': 0.00019662610498850683, 'epoch': 0.11}
{'loss': 0.8987, 'learning_rate': 0.00019661003458034285, 'epoch': 0.11}
{'loss': 0.3705, 'learning_rate': 0.0001965939266499547, 'epoch': 0.11}
{'loss': 0.3401, 'learning_rate': 0.00019657778120359847, 'epoch': 0.11}
{'loss': 0.9412, 'learning_rate': 0.0001965615982475449, 'epoch': 0.11}
{'loss': 0.9185, 'learning_rate': 0.00019654537778807923, 'epoch': 0.11}
{'loss': 0.8987, 'learning_rate': 0.00019652911983150136, 'epoch': 0.11}
{'loss': 0.8831, 'learning_rate': 0.0001965128243841256, 'epoch': 0.11}
{'loss': 0.8586, 'learning_rate': 0.00019649649145228102, 'epoch': 0.11}
{'loss': 0.8557, 'learning_rate': 0.00019648012104231106, 'epoch': 0.11}
{'loss': 0.9031, 'learning_rate': 0.00019646371316057383, 'epoch': 0.11}
{'loss': 0.876, 'learning_rate': 0.00019644726781344195, 'epoch': 0.11}
{'loss': 0.8967, 'learning_rate': 0.0001964307850073026, 'epoch': 0.11}
{'loss': 0.969, 'learning_rate': 0.00019641426474855758, 'epoch': 0.11}
{'loss': 0.8862, 'learning_rate': 0.00019639770704362307, 'epoch': 0.11}
{'loss': 0.8867, 'learning_rate': 0.00019638111189892993, 'epoch': 0.11}
{'loss': 0.8713, 'learning_rate': 0.00019636447932092353, 'epoch': 0.11}
{'loss': 0.3438, 'learning_rate': 0.0001963478093160638, 'epoch': 0.11}
{'loss': 0.9136, 'learning_rate': 0.00019633110189082512, 'epoch': 0.11}
{'loss': 0.9551, 'learning_rate': 0.0001963143570516965, 'epoch': 0.11}
{'loss': 0.8621, 'learning_rate': 0.00019629757480518143, 'epoch': 0.11}
{'loss': 0.8696, 'learning_rate': 0.00019628075515779796, 'epoch': 0.11}
{'loss': 0.905, 'learning_rate': 0.0001962638981160786, 'epoch': 0.11}
{'loss': 0.8794, 'learning_rate': 0.00019624700368657045, 'epoch': 0.11}
{'loss': 0.8892, 'learning_rate': 0.00019623007187583515, 'epoch': 0.12}
{'loss': 0.8787, 'learning_rate': 0.0001962131026904488, 'epoch': 0.12}
{'loss': 0.9011, 'learning_rate': 0.000196196096137002, 'epoch': 0.12}
{'loss': 0.8481, 'learning_rate': 0.00019617905222209996, 'epoch': 0.12}
{'loss': 0.8538, 'learning_rate': 0.00019616197095236227, 'epoch': 0.12}
{'loss': 0.8926, 'learning_rate': 0.00019614485233442316, 'epoch': 0.12}
{'loss': 0.9143, 'learning_rate': 0.00019612769637493126, 'epoch': 0.12}
{'loss': 0.8762, 'learning_rate': 0.0001961105030805498, 'epoch': 0.12}
{'loss': 0.9148, 'learning_rate': 0.00019609327245795642, 'epoch': 0.12}
{'loss': 0.918, 'learning_rate': 0.00019607600451384326, 'epoch': 0.12}
{'loss': 0.3785, 'learning_rate': 0.00019605869925491706, 'epoch': 0.12}
{'loss': 0.3456, 'learning_rate': 0.00019604135668789896, 'epoch': 0.12}
{'loss': 0.947, 'learning_rate': 0.00019602397681952463, 'epoch': 0.12}
{'loss': 0.8677, 'learning_rate': 0.00019600655965654412, 'epoch': 0.12}
{'loss': 0.8965, 'learning_rate': 0.00019598910520572218, 'epoch': 0.12}
{'loss': 0.9443, 'learning_rate': 0.00019597161347383784, 'epoch': 0.12}
{'loss': 0.9768, 'learning_rate': 0.00019595408446768472, 'epoch': 0.12}
{'loss': 0.8958, 'learning_rate': 0.00019593651819407084, 'epoch': 0.12}
{'loss': 0.9456, 'learning_rate': 0.0001959189146598188, 'epoch': 0.12}
{'loss': 0.9124, 'learning_rate': 0.00019590127387176555, 'epoch': 0.12}
{'loss': 0.8857, 'learning_rate': 0.00019588359583676263, 'epoch': 0.12}
{'loss': 0.9094, 'learning_rate': 0.00019586588056167594, 'epoch': 0.12}
{'loss': 0.8931, 'learning_rate': 0.0001958481280533859, 'epoch': 0.12}
{'loss': 0.8352, 'learning_rate': 0.0001958303383187874, 'epoch': 0.12}
{'loss': 0.9104, 'learning_rate': 0.00019581251136478972, 'epoch': 0.12}
{'loss': 0.9199, 'learning_rate': 0.00019579464719831667, 'epoch': 0.12}
{'loss': 0.8735, 'learning_rate': 0.00019577674582630652, 'epoch': 0.12}
{'loss': 0.8459, 'learning_rate': 0.0001957588072557119, 'epoch': 0.12}
{'loss': 0.9312, 'learning_rate': 0.0001957408314935, 'epoch': 0.12}
{'loss': 0.9353, 'learning_rate': 0.00019572281854665234, 'epoch': 0.12}
{'loss': 0.8362, 'learning_rate': 0.00019570476842216498, 'epoch': 0.12}
{'loss': 0.9155, 'learning_rate': 0.00019568668112704838, 'epoch': 0.12}
{'loss': 0.8408, 'learning_rate': 0.00019566855666832743, 'epoch': 0.12}
{'loss': 0.8745, 'learning_rate': 0.00019565039505304145, 'epoch': 0.12}
{'loss': 0.8488, 'learning_rate': 0.0001956321962882442, 'epoch': 0.12}
{'loss': 0.8887, 'learning_rate': 0.0001956139603810039, 'epoch': 0.12}
{'loss': 0.8208, 'learning_rate': 0.00019559568733840314, 'epoch': 0.12}
{'loss': 0.8954, 'learning_rate': 0.00019557737716753896, 'epoch': 0.12}
{'loss': 0.8828, 'learning_rate': 0.00019555902987552283, 'epoch': 0.12}
{'loss': 0.8723, 'learning_rate': 0.00019554064546948064, 'epoch': 0.12}
{'loss': 0.8282, 'learning_rate': 0.0001955222239565526, 'epoch': 0.12}
{'loss': 0.8884, 'learning_rate': 0.00019550376534389357, 'epoch': 0.12}
{'loss': 0.9719, 'learning_rate': 0.00019548526963867252, 'epoch': 0.12}
{'loss': 0.9231, 'learning_rate': 0.00019546673684807302, 'epoch': 0.12}
{'loss': 0.8479, 'learning_rate': 0.000195448166979293, 'epoch': 0.12}
{'loss': 0.8806, 'learning_rate': 0.00019542956003954477, 'epoch': 0.12}
{'loss': 0.8621, 'learning_rate': 0.00019541091603605506, 'epoch': 0.12}
{'loss': 0.8752, 'learning_rate': 0.000195392234976065, 'epoch': 0.12}
{'loss': 0.9473, 'learning_rate': 0.00019537351686683003, 'epoch': 0.12}
{'loss': 0.9124, 'learning_rate': 0.00019535476171562012, 'epoch': 0.12}
{'loss': 0.8831, 'learning_rate': 0.00019533596952971954, 'epoch': 0.12}
{'loss': 0.8766, 'learning_rate': 0.00019531714031642696, 'epoch': 0.12}
{'loss': 0.8337, 'learning_rate': 0.0001952982740830554, 'epoch': 0.13}
{'loss': 0.8052, 'learning_rate': 0.00019527937083693231, 'epoch': 0.13}
{'loss': 0.8318, 'learning_rate': 0.0001952604305853995, 'epoch': 0.13}
{'loss': 0.8767, 'learning_rate': 0.00019524145333581317, 'epoch': 0.13}
{'loss': 0.8594, 'learning_rate': 0.00019522243909554377, 'epoch': 0.13}
{'loss': 0.917, 'learning_rate': 0.00019520338787197629, 'epoch': 0.13}
{'loss': 0.3423, 'learning_rate': 0.00019518429967251, 'epoch': 0.13}
{'loss': 0.8984, 'learning_rate': 0.00019516517450455853, 'epoch': 0.13}
{'loss': 0.8899, 'learning_rate': 0.00019514601237554988, 'epoch': 0.13}
{'loss': 0.8196, 'learning_rate': 0.00019512681329292636, 'epoch': 0.13}
{'loss': 0.856, 'learning_rate': 0.00019510757726414472, 'epoch': 0.13}
{'loss': 0.8567, 'learning_rate': 0.000195088304296676, 'epoch': 0.13}
{'loss': 0.8219, 'learning_rate': 0.00019506899439800557, 'epoch': 0.13}
{'loss': 0.8889, 'learning_rate': 0.0001950496475756332, 'epoch': 0.13}
{'loss': 0.9055, 'learning_rate': 0.000195030263837073, 'epoch': 0.13}
{'loss': 0.8792, 'learning_rate': 0.00019501084318985335, 'epoch': 0.13}
{'loss': 0.9314, 'learning_rate': 0.000194991385641517, 'epoch': 0.13}
{'loss': 0.9033, 'learning_rate': 0.00019497189119962105, 'epoch': 0.13}
{'loss': 0.9238, 'learning_rate': 0.00019495235987173693, 'epoch': 0.13}
{'loss': 0.8542, 'learning_rate': 0.00019493279166545038, 'epoch': 0.13}
{'loss': 0.8526, 'learning_rate': 0.00019491318658836142, 'epoch': 0.13}
{'loss': 0.8596, 'learning_rate': 0.0001948935446480845, 'epoch': 0.13}
{'loss': 0.8833, 'learning_rate': 0.0001948738658522483, 'epoch': 0.13}
{'loss': 0.3041, 'learning_rate': 0.00019485415020849582, 'epoch': 0.13}
{'loss': 0.8245, 'learning_rate': 0.0001948343977244844, 'epoch': 0.13}
{'loss': 0.8323, 'learning_rate': 0.00019481460840788573, 'epoch': 0.13}
{'loss': 0.3412, 'learning_rate': 0.00019479478226638565, 'epoch': 0.13}
{'loss': 0.8936, 'learning_rate': 0.0001947749193076845, 'epoch': 0.13}
{'loss': 0.9153, 'learning_rate': 0.00019475501953949672, 'epoch': 0.13}
{'loss': 0.3104, 'learning_rate': 0.00019473508296955126, 'epoch': 0.13}
{'loss': 0.8721, 'learning_rate': 0.0001947151096055912, 'epoch': 0.13}
{'loss': 0.9077, 'learning_rate': 0.00019469509945537397, 'epoch': 0.13}
{'loss': 0.9189, 'learning_rate': 0.00019467505252667127, 'epoch': 0.13}
{'loss': 0.7893, 'learning_rate': 0.0001946549688272691, 'epoch': 0.13}
{'loss': 0.95, 'learning_rate': 0.0001946348483649678, 'epoch': 0.13}
{'loss': 0.8203, 'learning_rate': 0.0001946146911475818, 'epoch': 0.13}
{'loss': 0.939, 'learning_rate': 0.00019459449718294008, 'epoch': 0.13}
{'loss': 0.9124, 'learning_rate': 0.00019457426647888563, 'epoch': 0.13}
{'loss': 0.9014, 'learning_rate': 0.00019455399904327585, 'epoch': 0.13}
{'loss': 0.8398, 'learning_rate': 0.00019453369488398238, 'epoch': 0.13}
{'loss': 0.9202, 'learning_rate': 0.00019451335400889116, 'epoch': 0.13}
{'loss': 0.3192, 'learning_rate': 0.00019449297642590228, 'epoch': 0.13}
{'loss': 0.8672, 'learning_rate': 0.00019447256214293025, 'epoch': 0.13}
{'loss': 0.8286, 'learning_rate': 0.00019445211116790362, 'epoch': 0.13}
{'loss': 0.8965, 'learning_rate': 0.00019443162350876546, 'epoch': 0.13}
{'loss': 0.8643, 'learning_rate': 0.0001944110991734728, 'epoch': 0.13}
{'loss': 0.8564, 'learning_rate': 0.00019439053816999716, 'epoch': 0.13}
{'loss': 0.8865, 'learning_rate': 0.00019436994050632414, 'epoch': 0.13}
{'loss': 0.8696, 'learning_rate': 0.00019434930619045368, 'epoch': 0.13}
{'loss': 0.877, 'learning_rate': 0.00019432863523039987, 'epoch': 0.13}
{'loss': 0.885, 'learning_rate': 0.00019430792763419107, 'epoch': 0.13}
{'loss': 0.8632, 'learning_rate': 0.00019428718340986988, 'epoch': 0.13}
{'loss': 0.2939, 'learning_rate': 0.00019426640256549313, 'epoch': 0.14}
{'loss': 0.9053, 'learning_rate': 0.00019424558510913186, 'epoch': 0.14}
{'loss': 0.8743, 'learning_rate': 0.00019422473104887134, 'epoch': 0.14}
{'loss': 0.8705, 'learning_rate': 0.000194203840392811, 'epoch': 0.14}
{'loss': 0.896, 'learning_rate': 0.00019418291314906457, 'epoch': 0.14}
{'loss': 0.9475, 'learning_rate': 0.00019416194932576, 'epoch': 0.14}
{'loss': 0.8337, 'learning_rate': 0.00019414094893103928, 'epoch': 0.14}
{'loss': 0.8181, 'learning_rate': 0.00019411991197305879, 'epoch': 0.14}
{'loss': 0.8513, 'learning_rate': 0.00019409883845998904, 'epoch': 0.14}
{'loss': 0.906, 'learning_rate': 0.00019407772840001472, 'epoch': 0.14}
{'loss': 0.9219, 'learning_rate': 0.00019405658180133477, 'epoch': 0.14}
{'loss': 0.9385, 'learning_rate': 0.00019403539867216224, 'epoch': 0.14}
{'loss': 0.8217, 'learning_rate': 0.00019401417902072446, 'epoch': 0.14}
{'loss': 0.8391, 'learning_rate': 0.00019399292285526284, 'epoch': 0.14}
{'loss': 0.9341, 'learning_rate': 0.00019397163018403308, 'epoch': 0.14}
{'loss': 0.9739, 'learning_rate': 0.00019395030101530502, 'epoch': 0.14}
{'loss': 0.9001, 'learning_rate': 0.0001939289353573626, 'epoch': 0.14}
{'loss': 0.9214, 'learning_rate': 0.00019390753321850404, 'epoch': 0.14}
{'loss': 0.9031, 'learning_rate': 0.00019388609460704168, 'epoch': 0.14}
{'loss': 0.8674, 'learning_rate': 0.000193864619531302, 'epoch': 0.14}
{'loss': 0.9136, 'learning_rate': 0.00019384310799962573, 'epoch': 0.14}
{'loss': 0.9084, 'learning_rate': 0.00019382156002036764, 'epoch': 0.14}
{'loss': 0.8562, 'learning_rate': 0.00019379997560189675, 'epoch': 0.14}
{'loss': 0.9331, 'learning_rate': 0.0001937783547525962, 'epoch': 0.14}
{'loss': 0.9158, 'learning_rate': 0.00019375669748086327, 'epoch': 0.14}
{'loss': 0.8567, 'learning_rate': 0.00019373500379510938, 'epoch': 0.14}
{'loss': 0.8792, 'learning_rate': 0.00019371327370376016, 'epoch': 0.14}
{'loss': 0.887, 'learning_rate': 0.00019369150721525527, 'epoch': 0.14}
{'loss': 0.3359, 'learning_rate': 0.0001936697043380486, 'epoch': 0.14}
{'loss': 0.9141, 'learning_rate': 0.00019364786508060808, 'epoch': 0.14}
{'loss': 0.938, 'learning_rate': 0.0001936259894514159, 'epoch': 0.14}
{'loss': 0.8356, 'learning_rate': 0.00019360407745896827, 'epoch': 0.14}
{'loss': 0.8977, 'learning_rate': 0.00019358212911177556, 'epoch': 0.14}
{'loss': 0.9114, 'learning_rate': 0.0001935601444183622, 'epoch': 0.14}
{'loss': 0.8843, 'learning_rate': 0.0001935381233872669, 'epoch': 0.14}
{'loss': 0.8848, 'learning_rate': 0.00019351606602704228, 'epoch': 0.14}
{'loss': 0.8486, 'learning_rate': 0.0001934939723462552, 'epoch': 0.14}
{'loss': 0.8875, 'learning_rate': 0.00019347184235348662, 'epoch': 0.14}
{'loss': 0.9636, 'learning_rate': 0.00019344967605733153, 'epoch': 0.14}
{'loss': 0.8534, 'learning_rate': 0.0001934274734663991, 'epoch': 0.14}
{'loss': 0.8989, 'learning_rate': 0.00019340523458931253, 'epoch': 0.14}
{'loss': 0.8398, 'learning_rate': 0.00019338295943470914, 'epoch': 0.14}
{'loss': 0.8875, 'learning_rate': 0.00019336064801124035, 'epoch': 0.14}
{'loss': 0.8635, 'learning_rate': 0.0001933383003275717, 'epoch': 0.14}
{'loss': 0.887, 'learning_rate': 0.0001933159163923827, 'epoch': 0.14}
{'loss': 0.9146, 'learning_rate': 0.00019329349621436708, 'epoch': 0.14}
{'loss': 0.3305, 'learning_rate': 0.00019327103980223254, 'epoch': 0.14}
{'loss': 0.3239, 'learning_rate': 0.0001932485471647009, 'epoch': 0.14}
{'loss': 0.9539, 'learning_rate': 0.00019322601831050804, 'epoch': 0.14}
{'loss': 0.8997, 'learning_rate': 0.00019320345324840395, 'epoch': 0.14}
{'loss': 0.8599, 'learning_rate': 0.00019318085198715256, 'epoch': 0.14}
{'loss': 0.8287, 'learning_rate': 0.000193158214535532, 'epoch': 0.14}
{'loss': 0.8528, 'learning_rate': 0.00019313554090233436, 'epoch': 0.15}
{'loss': 0.8657, 'learning_rate': 0.00019311283109636583, 'epoch': 0.15}
{'loss': 0.8276, 'learning_rate': 0.00019309008512644667, 'epoch': 0.15}
{'loss': 0.8813, 'learning_rate': 0.0001930673030014111, 'epoch': 0.15}
{'loss': 0.9297, 'learning_rate': 0.00019304448473010747, 'epoch': 0.15}
{'loss': 0.8774, 'learning_rate': 0.00019302163032139814, 'epoch': 0.15}
{'loss': 0.887, 'learning_rate': 0.00019299873978415947, 'epoch': 0.15}
{'loss': 0.8584, 'learning_rate': 0.00019297581312728186, 'epoch': 0.15}
{'loss': 0.9458, 'learning_rate': 0.0001929528503596698, 'epoch': 0.15}
{'loss': 0.8525, 'learning_rate': 0.00019292985149024178, 'epoch': 0.15}
{'loss': 0.9373, 'learning_rate': 0.00019290681652793027, 'epoch': 0.15}
{'loss': 0.9312, 'learning_rate': 0.0001928837454816818, 'epoch': 0.15}
{'loss': 0.8429, 'learning_rate': 0.00019286063836045685, 'epoch': 0.15}
{'loss': 0.8376, 'learning_rate': 0.00019283749517322999, 'epoch': 0.15}
{'loss': 0.9172, 'learning_rate': 0.00019281431592898978, 'epoch': 0.15}
{'loss': 0.9326, 'learning_rate': 0.0001927911006367388, 'epoch': 0.15}
{'loss': 0.9111, 'learning_rate': 0.0001927678493054935, 'epoch': 0.15}
{'loss': 0.8295, 'learning_rate': 0.00019274456194428454, 'epoch': 0.15}
{'loss': 0.7554, 'learning_rate': 0.0001927212385621564, 'epoch': 0.15}
{'loss': 0.8789, 'learning_rate': 0.00019269787916816763, 'epoch': 0.15}
{'loss': 0.9312, 'learning_rate': 0.00019267448377139075, 'epoch': 0.15}
{'loss': 0.8511, 'learning_rate': 0.00019265105238091228, 'epoch': 0.15}
{'loss': 0.9009, 'learning_rate': 0.00019262758500583263, 'epoch': 0.15}
{'loss': 0.7917, 'learning_rate': 0.00019260408165526637, 'epoch': 0.15}
{'loss': 0.8335, 'learning_rate': 0.00019258054233834183, 'epoch': 0.15}
{'loss': 0.9011, 'learning_rate': 0.00019255696706420148, 'epoch': 0.15}
{'loss': 0.7896, 'learning_rate': 0.00019253335584200164, 'epoch': 0.15}
{'loss': 0.8704, 'learning_rate': 0.00019250970868091267, 'epoch': 0.15}
{'loss': 0.8328, 'learning_rate': 0.00019248602559011882, 'epoch': 0.15}
{'loss': 0.907, 'learning_rate': 0.00019246230657881834, 'epoch': 0.15}
{'loss': 0.8333, 'learning_rate': 0.00019243855165622343, 'epoch': 0.15}
{'loss': 0.9287, 'learning_rate': 0.00019241476083156026, 'epoch': 0.15}
{'loss': 0.9124, 'learning_rate': 0.00019239093411406886, 'epoch': 0.15}
{'loss': 0.8439, 'learning_rate': 0.00019236707151300327, 'epoch': 0.15}
{'loss': 0.7888, 'learning_rate': 0.00019234317303763144, 'epoch': 0.15}
{'loss': 0.2839, 'learning_rate': 0.00019231923869723528, 'epoch': 0.15}
{'loss': 0.9204, 'learning_rate': 0.0001922952685011106, 'epoch': 0.15}
{'loss': 0.368, 'learning_rate': 0.00019227126245856716, 'epoch': 0.15}
{'loss': 0.9211, 'learning_rate': 0.00019224722057892862, 'epoch': 0.15}
{'loss': 0.3774, 'learning_rate': 0.00019222314287153255, 'epoch': 0.15}
{'loss': 0.866, 'learning_rate': 0.00019219902934573048, 'epoch': 0.15}
{'loss': 0.9199, 'learning_rate': 0.00019217488001088784, 'epoch': 0.15}
{'loss': 0.9075, 'learning_rate': 0.00019215069487638395, 'epoch': 0.15}
{'loss': 0.9119, 'learning_rate': 0.000192126473951612, 'epoch': 0.15}
{'loss': 0.896, 'learning_rate': 0.0001921022172459791, 'epoch': 0.15}
{'loss': 0.8838, 'learning_rate': 0.00019207792476890641, 'epoch': 0.15}
{'loss': 0.8376, 'learning_rate': 0.00019205359652982868, 'epoch': 0.15}
{'loss': 0.7889, 'learning_rate': 0.0001920292325381948, 'epoch': 0.15}
{'loss': 0.8918, 'learning_rate': 0.00019200483280346748, 'epoch': 0.15}
{'loss': 0.8916, 'learning_rate': 0.00019198039733512326, 'epoch': 0.15}
{'loss': 0.8494, 'learning_rate': 0.00019195592614265261, 'epoch': 0.15}
{'loss': 0.8715, 'learning_rate': 0.00019193141923555984, 'epoch': 0.15}
{'loss': 0.8711, 'learning_rate': 0.00019190687662336316, 'epoch': 0.16}
{'loss': 0.7977, 'learning_rate': 0.00019188229831559466, 'epoch': 0.16}
{'loss': 0.8401, 'learning_rate': 0.00019185768432180024, 'epoch': 0.16}
{'loss': 0.8809, 'learning_rate': 0.00019183303465153973, 'epoch': 0.16}
{'loss': 0.8845, 'learning_rate': 0.00019180834931438673, 'epoch': 0.16}
{'loss': 0.8804, 'learning_rate': 0.00019178362831992878, 'epoch': 0.16}
{'loss': 0.854, 'learning_rate': 0.00019175887167776717, 'epoch': 0.16}
{'loss': 0.8488, 'learning_rate': 0.0001917340793975172, 'epoch': 0.16}
{'loss': 0.9253, 'learning_rate': 0.0001917092514888078, 'epoch': 0.16}
{'loss': 0.8528, 'learning_rate': 0.00019168438796128193, 'epoch': 0.16}
{'loss': 0.8516, 'learning_rate': 0.0001916594888245962, 'epoch': 0.16}
{'loss': 0.9089, 'learning_rate': 0.00019163455408842124, 'epoch': 0.16}
{'loss': 0.8153, 'learning_rate': 0.00019160958376244136, 'epoch': 0.16}
{'loss': 0.8867, 'learning_rate': 0.00019158457785635477, 'epoch': 0.16}
{'loss': 0.8794, 'learning_rate': 0.00019155953637987346, 'epoch': 0.16}
{'loss': 0.9314, 'learning_rate': 0.0001915344593427233, 'epoch': 0.16}
{'loss': 0.8279, 'learning_rate': 0.00019150934675464382, 'epoch': 0.16}
{'loss': 0.8499, 'learning_rate': 0.00019148419862538858, 'epoch': 0.16}
{'loss': 0.9272, 'learning_rate': 0.00019145901496472472, 'epoch': 0.16}
{'loss': 0.9084, 'learning_rate': 0.00019143379578243335, 'epoch': 0.16}
{'loss': 0.8462, 'learning_rate': 0.0001914085410883093, 'epoch': 0.16}
{'loss': 0.9209, 'learning_rate': 0.00019138325089216118, 'epoch': 0.16}
{'loss': 0.8469, 'learning_rate': 0.00019135792520381142, 'epoch': 0.16}
{'loss': 0.8875, 'learning_rate': 0.00019133256403309625, 'epoch': 0.16}
{'loss': 0.7777, 'learning_rate': 0.0001913071673898656, 'epoch': 0.16}
{'loss': 0.8635, 'learning_rate': 0.0001912817352839833, 'epoch': 0.16}
{'loss': 0.8596, 'learning_rate': 0.00019125626772532683, 'epoch': 0.16}
{'loss': 0.8857, 'learning_rate': 0.00019123076472378752, 'epoch': 0.16}
{'loss': 0.8669, 'learning_rate': 0.00019120522628927045, 'epoch': 0.16}
{'loss': 0.8582, 'learning_rate': 0.00019117965243169445, 'epoch': 0.16}
{'loss': 0.8416, 'learning_rate': 0.00019115404316099213, 'epoch': 0.16}
{'loss': 0.8711, 'learning_rate': 0.00019112839848710978, 'epoch': 0.16}
{'loss': 0.3237, 'learning_rate': 0.00019110271842000755, 'epoch': 0.16}
{'loss': 0.8838, 'learning_rate': 0.00019107700296965927, 'epoch': 0.16}
{'loss': 0.8552, 'learning_rate': 0.0001910512521460525, 'epoch': 0.16}
{'loss': 0.9265, 'learning_rate': 0.00019102546595918857, 'epoch': 0.16}
{'loss': 0.9104, 'learning_rate': 0.00019099964441908257, 'epoch': 0.16}
{'loss': 0.8459, 'learning_rate': 0.00019097378753576324, 'epoch': 0.16}
{'loss': 0.9136, 'learning_rate': 0.00019094789531927316, 'epoch': 0.16}
{'loss': 0.8997, 'learning_rate': 0.00019092196777966848, 'epoch': 0.16}
{'loss': 0.3452, 'learning_rate': 0.00019089600492701924, 'epoch': 0.16}
{'loss': 0.3616, 'learning_rate': 0.00019087000677140908, 'epoch': 0.16}
{'loss': 0.8442, 'learning_rate': 0.00019084397332293538, 'epoch': 0.16}
{'loss': 0.8689, 'learning_rate': 0.00019081790459170926, 'epoch': 0.16}
{'loss': 0.9028, 'learning_rate': 0.00019079180058785547, 'epoch': 0.16}
{'loss': 0.8716, 'learning_rate': 0.00019076566132151252, 'epoch': 0.16}
{'loss': 0.8635, 'learning_rate': 0.0001907394868028326, 'epoch': 0.16}
{'loss': 0.8774, 'learning_rate': 0.0001907132770419816, 'epoch': 0.16}
{'loss': 0.8313, 'learning_rate': 0.0001906870320491391, 'epoch': 0.16}
{'loss': 0.9353, 'learning_rate': 0.00019066075183449835, 'epoch': 0.16}
{'loss': 0.8328, 'learning_rate': 0.00019063443640826623, 'epoch': 0.16}
{'loss': 0.8984, 'learning_rate': 0.0001906080857806634, 'epoch': 0.16}
{'loss': 0.8848, 'learning_rate': 0.00019058169996192417, 'epoch': 0.17}
{'loss': 0.8918, 'learning_rate': 0.00019055527896229643, 'epoch': 0.17}
{'loss': 0.958, 'learning_rate': 0.00019052882279204178, 'epoch': 0.17}
{'loss': 0.9147, 'learning_rate': 0.00019050233146143556, 'epoch': 0.17}
{'loss': 0.8938, 'learning_rate': 0.00019047580498076662, 'epoch': 0.17}
{'loss': 0.8511, 'learning_rate': 0.00019044924336033758, 'epoch': 0.17}
{'loss': 0.8239, 'learning_rate': 0.0001904226466104647, 'epoch': 0.17}
{'loss': 0.8672, 'learning_rate': 0.0001903960147414778, 'epoch': 0.17}
{'loss': 0.3109, 'learning_rate': 0.0001903693477637204, 'epoch': 0.17}
{'loss': 0.8333, 'learning_rate': 0.00019034264568754968, 'epoch': 0.17}
{'loss': 0.8574, 'learning_rate': 0.00019031590852333635, 'epoch': 0.17}
{'loss': 0.946, 'learning_rate': 0.00019028913628146488, 'epoch': 0.17}
{'loss': 0.8909, 'learning_rate': 0.00019026232897233328, 'epoch': 0.17}
{'loss': 0.9241, 'learning_rate': 0.0001902354866063532, 'epoch': 0.17}
{'loss': 0.8535, 'learning_rate': 0.0001902086091939499, 'epoch': 0.17}
{'loss': 0.8726, 'learning_rate': 0.00019018169674556227, 'epoch': 0.17}
{'loss': 0.9319, 'learning_rate': 0.0001901547492716428, 'epoch': 0.17}
{'loss': 0.8691, 'learning_rate': 0.00019012776678265756, 'epoch': 0.17}
{'loss': 0.8555, 'learning_rate': 0.00019010074928908622, 'epoch': 0.17}
{'loss': 0.8604, 'learning_rate': 0.0001900736968014221, 'epoch': 0.17}
{'loss': 0.8254, 'learning_rate': 0.0001900466093301721, 'epoch': 0.17}
{'loss': 0.9126, 'learning_rate': 0.0001900194868858566, 'epoch': 0.17}
{'loss': 0.8843, 'learning_rate': 0.0001899923294790097, 'epoch': 0.17}
{'loss': 0.3387, 'learning_rate': 0.00018996513712017898, 'epoch': 0.17}
{'loss': 0.8235, 'learning_rate': 0.0001899379098199257, 'epoch': 0.17}
{'loss': 0.9158, 'learning_rate': 0.0001899106475888246, 'epoch': 0.17}
{'loss': 0.9895, 'learning_rate': 0.00018988335043746403, 'epoch': 0.17}
{'loss': 0.8816, 'learning_rate': 0.00018985601837644587, 'epoch': 0.17}
{'loss': 0.9241, 'learning_rate': 0.00018982865141638558, 'epoch': 0.17}
{'loss': 0.8188, 'learning_rate': 0.00018980124956791215, 'epoch': 0.17}
{'loss': 0.8887, 'learning_rate': 0.00018977381284166817, 'epoch': 0.17}
{'loss': 0.8342, 'learning_rate': 0.00018974634124830977, 'epoch': 0.17}
{'loss': 0.9585, 'learning_rate': 0.0001897188347985066, 'epoch': 0.17}
{'loss': 0.8643, 'learning_rate': 0.00018969129350294178, 'epoch': 0.17}
{'loss': 0.9006, 'learning_rate': 0.00018966371737231207, 'epoch': 0.17}
{'loss': 0.9187, 'learning_rate': 0.00018963610641732777, 'epoch': 0.17}
{'loss': 0.8279, 'learning_rate': 0.00018960846064871258, 'epoch': 0.17}
{'loss': 0.9265, 'learning_rate': 0.00018958078007720385, 'epoch': 0.17}
{'loss': 0.9082, 'learning_rate': 0.00018955306471355235, 'epoch': 0.17}
{'loss': 0.3342, 'learning_rate': 0.00018952531456852247, 'epoch': 0.17}
{'loss': 0.8362, 'learning_rate': 0.00018949752965289197, 'epoch': 0.17}
{'loss': 0.8474, 'learning_rate': 0.0001894697099774523, 'epoch': 0.17}
{'loss': 0.8245, 'learning_rate': 0.0001894418555530082, 'epoch': 0.17}
{'loss': 0.8665, 'learning_rate': 0.000189413966390378, 'epoch': 0.17}
{'loss': 0.8922, 'learning_rate': 0.00018938604250039363, 'epoch': 0.17}
{'loss': 0.8486, 'learning_rate': 0.00018935808389390033, 'epoch': 0.17}
{'loss': 0.9004, 'learning_rate': 0.0001893300905817569, 'epoch': 0.17}
{'loss': 0.9346, 'learning_rate': 0.00018930206257483564, 'epoch': 0.17}
{'loss': 0.876, 'learning_rate': 0.00018927399988402232, 'epoch': 0.17}
{'loss': 0.8994, 'learning_rate': 0.00018924590252021614, 'epoch': 0.17}
{'loss': 0.3099, 'learning_rate': 0.00018921777049432984, 'epoch': 0.17}
{'loss': 0.8644, 'learning_rate': 0.00018918960381728947, 'epoch': 0.17}
{'loss': 0.9094, 'learning_rate': 0.00018916140250003474, 'epoch': 0.18}
{'loss': 0.8251, 'learning_rate': 0.0001891331665535187, 'epoch': 0.18}
{'loss': 0.8789, 'learning_rate': 0.00018910489598870784, 'epoch': 0.18}
{'loss': 0.8875, 'learning_rate': 0.00018907659081658214, 'epoch': 0.18}
{'loss': 0.8492, 'learning_rate': 0.00018904825104813498, 'epoch': 0.18}
{'loss': 0.8896, 'learning_rate': 0.0001890198766943732, 'epoch': 0.18}
{'loss': 0.8689, 'learning_rate': 0.00018899146776631712, 'epoch': 0.18}
{'loss': 0.8933, 'learning_rate': 0.0001889630242750004, 'epoch': 0.18}
{'loss': 0.8647, 'learning_rate': 0.0001889345462314702, 'epoch': 0.18}
{'loss': 0.8777, 'learning_rate': 0.000188906033646787, 'epoch': 0.18}
{'loss': 0.8662, 'learning_rate': 0.00018887748653202477, 'epoch': 0.18}
{'loss': 0.8521, 'learning_rate': 0.00018884890489827096, 'epoch': 0.18}
{'loss': 0.9163, 'learning_rate': 0.00018882028875662627, 'epoch': 0.18}
{'loss': 0.8921, 'learning_rate': 0.0001887916381182049, 'epoch': 0.18}
{'loss': 0.8822, 'learning_rate': 0.00018876295299413443, 'epoch': 0.18}
{'loss': 0.9443, 'learning_rate': 0.00018873423339555584, 'epoch': 0.18}
{'loss': 0.3435, 'learning_rate': 0.0001887054793336235, 'epoch': 0.18}
{'loss': 0.8486, 'learning_rate': 0.0001886766908195051, 'epoch': 0.18}
{'loss': 0.9075, 'learning_rate': 0.00018864786786438185, 'epoch': 0.18}
{'loss': 0.8359, 'learning_rate': 0.00018861901047944818, 'epoch': 0.18}
{'loss': 0.9006, 'learning_rate': 0.00018859011867591202, 'epoch': 0.18}
{'loss': 0.8853, 'learning_rate': 0.00018856119246499458, 'epoch': 0.18}
{'loss': 0.8442, 'learning_rate': 0.00018853223185793047, 'epoch': 0.18}
{'loss': 0.9583, 'learning_rate': 0.00018850323686596764, 'epoch': 0.18}
{'loss': 0.8218, 'learning_rate': 0.00018847420750036747, 'epoch': 0.18}
{'loss': 0.8477, 'learning_rate': 0.00018844514377240457, 'epoch': 0.18}
{'loss': 0.8977, 'learning_rate': 0.000188416045693367, 'epoch': 0.18}
{'loss': 0.9216, 'learning_rate': 0.0001883869132745561, 'epoch': 0.18}
{'loss': 0.8904, 'learning_rate': 0.0001883577465272866, 'epoch': 0.18}
{'loss': 0.8777, 'learning_rate': 0.00018832854546288642, 'epoch': 0.18}
{'loss': 0.7712, 'learning_rate': 0.00018829931009269705, 'epoch': 0.18}
{'loss': 0.8723, 'learning_rate': 0.0001882700404280731, 'epoch': 0.18}
{'loss': 0.782, 'learning_rate': 0.00018824073648038257, 'epoch': 0.18}
{'loss': 0.9146, 'learning_rate': 0.0001882113982610068, 'epoch': 0.18}
{'loss': 0.8557, 'learning_rate': 0.00018818202578134042, 'epoch': 0.18}
{'loss': 0.8989, 'learning_rate': 0.0001881526190527913, 'epoch': 0.18}
{'loss': 0.3385, 'learning_rate': 0.00018812317808678076, 'epoch': 0.18}
{'loss': 0.8738, 'learning_rate': 0.0001880937028947433, 'epoch': 0.18}
{'loss': 0.9246, 'learning_rate': 0.00018806419348812672, 'epoch': 0.18}
{'loss': 0.8491, 'learning_rate': 0.00018803464987839216, 'epoch': 0.18}
{'loss': 0.8826, 'learning_rate': 0.000188005072077014, 'epoch': 0.18}
{'loss': 0.8538, 'learning_rate': 0.00018797546009547995, 'epoch': 0.18}
{'loss': 0.887, 'learning_rate': 0.0001879458139452909, 'epoch': 0.18}
{'loss': 0.8309, 'learning_rate': 0.00018791613363796118, 'epoch': 0.18}
{'loss': 0.8999, 'learning_rate': 0.00018788641918501814, 'epoch': 0.18}
{'loss': 0.7882, 'learning_rate': 0.00018785667059800264, 'epoch': 0.18}
{'loss': 0.8875, 'learning_rate': 0.00018782688788846865, 'epoch': 0.18}
{'loss': 0.8838, 'learning_rate': 0.0001877970710679834, 'epoch': 0.18}
{'loss': 0.8264, 'learning_rate': 0.0001877672201481275, 'epoch': 0.18}
{'loss': 0.8623, 'learning_rate': 0.0001877373351404946, 'epoch': 0.18}
{'loss': 0.8649, 'learning_rate': 0.00018770741605669171, 'epoch': 0.18}
{'loss': 0.9363, 'learning_rate': 0.0001876774629083391, 'epoch': 0.18}
{'loss': 0.8364, 'learning_rate': 0.00018764747570707019, 'epoch': 0.19}
{'loss': 0.9036, 'learning_rate': 0.00018761745446453168, 'epoch': 0.19}
{'loss': 0.9167, 'learning_rate': 0.00018758739919238349, 'epoch': 0.19}
{'loss': 0.832, 'learning_rate': 0.00018755730990229868, 'epoch': 0.19}
{'loss': 0.8711, 'learning_rate': 0.00018752718660596367, 'epoch': 0.19}
{'loss': 0.8616, 'learning_rate': 0.00018749702931507796, 'epoch': 0.19}
{'loss': 0.9109, 'learning_rate': 0.00018746683804135429, 'epoch': 0.19}
{'loss': 0.8672, 'learning_rate': 0.00018743661279651855, 'epoch': 0.19}
{'loss': 0.8984, 'learning_rate': 0.00018740635359231, 'epoch': 0.19}
{'loss': 0.8408, 'learning_rate': 0.00018737606044048086, 'epoch': 0.19}
{'loss': 0.9246, 'learning_rate': 0.00018734573335279668, 'epoch': 0.19}
{'loss': 0.307, 'learning_rate': 0.00018731537234103617, 'epoch': 0.19}
{'loss': 0.8904, 'learning_rate': 0.00018728497741699117, 'epoch': 0.19}
{'loss': 0.8351, 'learning_rate': 0.0001872545485924667, 'epoch': 0.19}
{'loss': 0.8723, 'learning_rate': 0.00018722408587928103, 'epoch': 0.19}
{'loss': 0.9211, 'learning_rate': 0.00018719358928926546, 'epoch': 0.19}
{'loss': 0.8252, 'learning_rate': 0.00018716305883426454, 'epoch': 0.19}
{'loss': 0.7988, 'learning_rate': 0.00018713249452613598, 'epoch': 0.19}
{'loss': 0.8179, 'learning_rate': 0.00018710189637675056, 'epoch': 0.19}
{'loss': 0.9116, 'learning_rate': 0.00018707126439799224, 'epoch': 0.19}
{'loss': 0.8792, 'learning_rate': 0.00018704059860175817, 'epoch': 0.19}
{'loss': 0.9043, 'learning_rate': 0.00018700989899995857, 'epoch': 0.19}
{'loss': 0.8706, 'learning_rate': 0.00018697916560451682, 'epoch': 0.19}
{'loss': 0.9185, 'learning_rate': 0.0001869483984273694, 'epoch': 0.19}
{'loss': 0.8125, 'learning_rate': 0.00018691759748046594, 'epoch': 0.19}
{'loss': 0.8357, 'learning_rate': 0.00018688676277576916, 'epoch': 0.19}
{'loss': 0.8721, 'learning_rate': 0.00018685589432525492, 'epoch': 0.19}
{'loss': 0.8892, 'learning_rate': 0.0001868249921409122, 'epoch': 0.19}
{'loss': 0.8376, 'learning_rate': 0.00018679405623474296, 'epoch': 0.19}
{'loss': 0.876, 'learning_rate': 0.00018676308661876243, 'epoch': 0.19}
{'loss': 0.8464, 'learning_rate': 0.00018673208330499883, 'epoch': 0.19}
{'loss': 0.8657, 'learning_rate': 0.00018670104630549348, 'epoch': 0.19}
{'loss': 0.875, 'learning_rate': 0.0001866699756323008, 'epoch': 0.19}
{'loss': 0.9443, 'learning_rate': 0.0001866388712974883, 'epoch': 0.19}
{'loss': 0.896, 'learning_rate': 0.0001866077333131365, 'epoch': 0.19}
{'loss': 0.8853, 'learning_rate': 0.00018657656169133907, 'epoch': 0.19}
{'loss': 0.8042, 'learning_rate': 0.00018654535644420275, 'epoch': 0.19}
{'loss': 0.8228, 'learning_rate': 0.00018651411758384718, 'epoch': 0.19}
{'loss': 0.8857, 'learning_rate': 0.0001864828451224053, 'epoch': 0.19}
{'loss': 0.8695, 'learning_rate': 0.00018645153907202285, 'epoch': 0.19}
{'loss': 0.3285, 'learning_rate': 0.00018642019944485884, 'epoch': 0.19}
{'loss': 0.9375, 'learning_rate': 0.00018638882625308522, 'epoch': 0.19}
{'loss': 0.8794, 'learning_rate': 0.0001863574195088869, 'epoch': 0.19}
{'loss': 0.8604, 'learning_rate': 0.00018632597922446195, 'epoch': 0.19}
{'loss': 0.9448, 'learning_rate': 0.0001862945054120214, 'epoch': 0.19}
{'loss': 0.8367, 'learning_rate': 0.00018626299808378934, 'epoch': 0.19}
{'loss': 0.8347, 'learning_rate': 0.00018623145725200278, 'epoch': 0.19}
{'loss': 0.8943, 'learning_rate': 0.0001861998829289119, 'epoch': 0.19}
{'loss': 0.8772, 'learning_rate': 0.0001861682751267798, 'epoch': 0.19}
{'loss': 0.8965, 'learning_rate': 0.00018613663385788252, 'epoch': 0.19}
{'loss': 0.8458, 'learning_rate': 0.0001861049591345092, 'epoch': 0.19}
{'loss': 0.3556, 'learning_rate': 0.00018607325096896195, 'epoch': 0.19}
{'loss': 0.8826, 'learning_rate': 0.00018604150937355586, 'epoch': 0.2}
{'loss': 0.9204, 'learning_rate': 0.00018600973436061897, 'epoch': 0.2}
{'loss': 0.8627, 'learning_rate': 0.00018597792594249234, 'epoch': 0.2}
{'loss': 0.8921, 'learning_rate': 0.00018594608413153001, 'epoch': 0.2}
{'loss': 0.8291, 'learning_rate': 0.00018591420894009897, 'epoch': 0.2}
{'loss': 0.8041, 'learning_rate': 0.00018588230038057914, 'epoch': 0.2}
{'loss': 0.8384, 'learning_rate': 0.00018585035846536348, 'epoch': 0.2}
{'loss': 0.832, 'learning_rate': 0.00018581838320685782, 'epoch': 0.2}
{'loss': 0.8433, 'learning_rate': 0.00018578637461748106, 'epoch': 0.2}
{'loss': 0.9138, 'learning_rate': 0.00018575433270966486, 'epoch': 0.2}
{'loss': 0.8838, 'learning_rate': 0.00018572225749585402, 'epoch': 0.2}
{'loss': 0.8407, 'learning_rate': 0.00018569014898850612, 'epoch': 0.2}
{'loss': 0.8085, 'learning_rate': 0.0001856580072000918, 'epoch': 0.2}
{'loss': 0.8743, 'learning_rate': 0.00018562583214309445, 'epoch': 0.2}
{'loss': 0.8425, 'learning_rate': 0.0001855936238300106, 'epoch': 0.2}
{'loss': 0.8806, 'learning_rate': 0.00018556138227334955, 'epoch': 0.2}
{'loss': 0.9021, 'learning_rate': 0.00018552910748563357, 'epoch': 0.2}
{'loss': 0.8665, 'learning_rate': 0.00018549679947939778, 'epoch': 0.2}
{'loss': 0.9199, 'learning_rate': 0.00018546445826719024, 'epoch': 0.2}
{'loss': 0.8074, 'learning_rate': 0.00018543208386157193, 'epoch': 0.2}
{'loss': 0.8546, 'learning_rate': 0.00018539967627511668, 'epoch': 0.2}
{'loss': 0.8887, 'learning_rate': 0.00018536723552041123, 'epoch': 0.2}
{'loss': 0.8982, 'learning_rate': 0.00018533476161005518, 'epoch': 0.2}
{'loss': 0.8474, 'learning_rate': 0.00018530225455666102, 'epoch': 0.2}
{'loss': 0.8997, 'learning_rate': 0.00018526971437285417, 'epoch': 0.2}
{'loss': 0.8748, 'learning_rate': 0.00018523714107127276, 'epoch': 0.2}
{'loss': 0.8411, 'learning_rate': 0.00018520453466456797, 'epoch': 0.2}
{'loss': 0.9426, 'learning_rate': 0.00018517189516540375, 'epoch': 0.2}
{'loss': 0.8909, 'learning_rate': 0.00018513922258645685, 'epoch': 0.2}
{'loss': 0.9167, 'learning_rate': 0.000185106516940417, 'epoch': 0.2}
{'loss': 0.8065, 'learning_rate': 0.00018507377823998665, 'epoch': 0.2}
{'loss': 0.842, 'learning_rate': 0.0001850410064978811, 'epoch': 0.2}
{'loss': 0.9004, 'learning_rate': 0.00018500820172682856, 'epoch': 0.2}
{'loss': 0.3212, 'learning_rate': 0.00018497536393957003, 'epoch': 0.2}
{'loss': 0.8486, 'learning_rate': 0.0001849424931488593, 'epoch': 0.2}
{'loss': 0.8811, 'learning_rate': 0.00018490958936746306, 'epoch': 0.2}
{'loss': 0.917, 'learning_rate': 0.0001848766526081607, 'epoch': 0.2}
{'loss': 0.8335, 'learning_rate': 0.0001848436828837445, 'epoch': 0.2}
{'loss': 0.8882, 'learning_rate': 0.00018481068020701952, 'epoch': 0.2}
{'loss': 0.8376, 'learning_rate': 0.00018477764459080362, 'epoch': 0.2}
{'loss': 0.3288, 'learning_rate': 0.00018474457604792746, 'epoch': 0.2}
{'loss': 0.8364, 'learning_rate': 0.00018471147459123448, 'epoch': 0.2}
{'loss': 0.8931, 'learning_rate': 0.00018467834023358086, 'epoch': 0.2}
{'loss': 0.8701, 'learning_rate': 0.0001846451729878357, 'epoch': 0.2}
{'loss': 0.8414, 'learning_rate': 0.00018461197286688067, 'epoch': 0.2}
{'loss': 0.9077, 'learning_rate': 0.00018457873988361037, 'epoch': 0.2}
{'loss': 0.8691, 'learning_rate': 0.00018454547405093212, 'epoch': 0.2}
{'loss': 0.8821, 'learning_rate': 0.00018451217538176598, 'epoch': 0.2}
{'loss': 0.8269, 'learning_rate': 0.0001844788438890447, 'epoch': 0.2}
{'loss': 0.3341, 'learning_rate': 0.00018444547958571396, 'epoch': 0.2}
{'loss': 0.3273, 'learning_rate': 0.00018441208248473197, 'epoch': 0.2}
{'loss': 0.8647, 'learning_rate': 0.00018437865259906985, 'epoch': 0.2}
{'loss': 0.9241, 'learning_rate': 0.00018434518994171135, 'epoch': 0.21}
{'loss': 0.8564, 'learning_rate': 0.000184311694525653, 'epoch': 0.21}
{'loss': 0.8003, 'learning_rate': 0.00018427816636390398, 'epoch': 0.21}
{'loss': 0.8936, 'learning_rate': 0.0001842446054694863, 'epoch': 0.21}
{'loss': 0.8828, 'learning_rate': 0.00018421101185543465, 'epoch': 0.21}
{'loss': 0.8132, 'learning_rate': 0.0001841773855347963, 'epoch': 0.21}
{'loss': 0.7944, 'learning_rate': 0.0001841437265206314, 'epoch': 0.21}
{'loss': 0.8572, 'learning_rate': 0.0001841100348260127, 'epoch': 0.21}
{'loss': 0.9209, 'learning_rate': 0.0001840763104640257, 'epoch': 0.21}
{'loss': 0.885, 'learning_rate': 0.0001840425534477685, 'epoch': 0.21}
{'loss': 0.9072, 'learning_rate': 0.000184008763790352, 'epoch': 0.21}
{'loss': 0.8721, 'learning_rate': 0.00018397494150489965, 'epoch': 0.21}
{'loss': 0.8301, 'learning_rate': 0.00018394108660454766, 'epoch': 0.21}
{'loss': 0.8716, 'learning_rate': 0.00018390719910244487, 'epoch': 0.21}
{'loss': 0.885, 'learning_rate': 0.00018387327901175284, 'epoch': 0.21}
{'loss': 0.9019, 'learning_rate': 0.00018383932634564572, 'epoch': 0.21}
{'loss': 0.823, 'learning_rate': 0.00018380534111731028, 'epoch': 0.21}
{'loss': 0.8936, 'learning_rate': 0.00018377132333994607, 'epoch': 0.21}
{'loss': 0.9341, 'learning_rate': 0.0001837372730267652, 'epoch': 0.21}
{'loss': 0.8186, 'learning_rate': 0.00018370319019099234, 'epoch': 0.21}
{'loss': 0.8867, 'learning_rate': 0.00018366907484586496, 'epoch': 0.21}
{'loss': 0.8156, 'learning_rate': 0.000183634927004633, 'epoch': 0.21}
{'loss': 0.7891, 'learning_rate': 0.00018360074668055914, 'epoch': 0.21}
{'loss': 0.8926, 'learning_rate': 0.00018356653388691859, 'epoch': 0.21}
{'loss': 0.8889, 'learning_rate': 0.00018353228863699923, 'epoch': 0.21}
{'loss': 0.8547, 'learning_rate': 0.00018349801094410148, 'epoch': 0.21}
{'loss': 0.8323, 'learning_rate': 0.00018346370082153844, 'epoch': 0.21}
{'loss': 0.804, 'learning_rate': 0.00018342935828263575, 'epoch': 0.21}
{'loss': 0.8425, 'learning_rate': 0.00018339498334073168, 'epoch': 0.21}
{'loss': 0.8652, 'learning_rate': 0.000183360576009177, 'epoch': 0.21}
{'loss': 0.8489, 'learning_rate': 0.00018332613630133517, 'epoch': 0.21}
{'loss': 0.8765, 'learning_rate': 0.0001832916642305822, 'epoch': 0.21}
{'loss': 0.8589, 'learning_rate': 0.00018325715981030657, 'epoch': 0.21}
{'loss': 0.96, 'learning_rate': 0.00018322262305390947, 'epoch': 0.21}
{'loss': 0.8923, 'learning_rate': 0.00018318805397480453, 'epoch': 0.21}
{'loss': 0.8516, 'learning_rate': 0.00018315345258641802, 'epoch': 0.21}
{'loss': 0.8346, 'learning_rate': 0.00018311881890218873, 'epoch': 0.21}
{'loss': 0.7936, 'learning_rate': 0.0001830841529355679, 'epoch': 0.21}
{'loss': 0.8667, 'learning_rate': 0.0001830494547000195, 'epoch': 0.21}
{'loss': 0.8334, 'learning_rate': 0.00018301472420901983, 'epoch': 0.21}
{'loss': 0.8787, 'learning_rate': 0.00018297996147605787, 'epoch': 0.21}
{'loss': 0.8745, 'learning_rate': 0.00018294516651463507, 'epoch': 0.21}
{'loss': 0.9478, 'learning_rate': 0.00018291033933826533, 'epoch': 0.21}
{'loss': 0.8396, 'learning_rate': 0.0001828754799604752, 'epoch': 0.21}
{'loss': 0.9111, 'learning_rate': 0.00018284058839480361, 'epoch': 0.21}
{'loss': 0.9185, 'learning_rate': 0.00018280566465480206, 'epoch': 0.21}
{'loss': 0.311, 'learning_rate': 0.00018277070875403456, 'epoch': 0.21}
{'loss': 0.886, 'learning_rate': 0.00018273572070607753, 'epoch': 0.21}
{'loss': 0.8523, 'learning_rate': 0.00018270070052451993, 'epoch': 0.21}
{'loss': 0.8835, 'learning_rate': 0.00018266564822296322, 'epoch': 0.21}
{'loss': 0.335, 'learning_rate': 0.0001826305638150213, 'epoch': 0.21}
{'loss': 0.324, 'learning_rate': 0.0001825954473143205, 'epoch': 0.21}
{'loss': 0.8401, 'learning_rate': 0.00018256029873449974, 'epoch': 0.22}
{'loss': 0.3174, 'learning_rate': 0.00018252511808921032, 'epoch': 0.22}
{'loss': 0.896, 'learning_rate': 0.00018248990539211594, 'epoch': 0.22}
{'loss': 0.8203, 'learning_rate': 0.00018245466065689282, 'epoch': 0.22}
{'loss': 0.9111, 'learning_rate': 0.00018241938389722967, 'epoch': 0.22}
{'loss': 0.8848, 'learning_rate': 0.0001823840751268275, 'epoch': 0.22}
{'loss': 0.8674, 'learning_rate': 0.00018234873435939988, 'epoch': 0.22}
{'loss': 0.9053, 'learning_rate': 0.00018231336160867275, 'epoch': 0.22}
{'loss': 0.8162, 'learning_rate': 0.00018227795688838445, 'epoch': 0.22}
{'loss': 0.863, 'learning_rate': 0.00018224252021228579, 'epoch': 0.22}
{'loss': 0.813, 'learning_rate': 0.00018220705159413994, 'epoch': 0.22}
{'loss': 0.8743, 'learning_rate': 0.00018217155104772258, 'epoch': 0.22}
{'loss': 0.3602, 'learning_rate': 0.0001821360185868216, 'epoch': 0.22}
{'loss': 0.8477, 'learning_rate': 0.00018210045422523745, 'epoch': 0.22}
{'loss': 0.3069, 'learning_rate': 0.00018206485797678295, 'epoch': 0.22}
{'loss': 0.8359, 'learning_rate': 0.00018202922985528322, 'epoch': 0.22}
{'loss': 0.8799, 'learning_rate': 0.00018199356987457586, 'epoch': 0.22}
{'loss': 0.9066, 'learning_rate': 0.00018195787804851078, 'epoch': 0.22}
{'loss': 0.842, 'learning_rate': 0.00018192215439095024, 'epoch': 0.22}
{'loss': 0.8557, 'learning_rate': 0.00018188639891576893, 'epoch': 0.22}
{'loss': 0.7959, 'learning_rate': 0.00018185061163685385, 'epoch': 0.22}
{'loss': 0.8486, 'learning_rate': 0.0001818147925681044, 'epoch': 0.22}
{'loss': 0.8292, 'learning_rate': 0.00018177894172343226, 'epoch': 0.22}
{'loss': 0.7898, 'learning_rate': 0.00018174305911676148, 'epoch': 0.22}
{'loss': 0.8516, 'learning_rate': 0.00018170714476202848, 'epoch': 0.22}
{'loss': 0.345, 'learning_rate': 0.00018167119867318197, 'epoch': 0.22}
{'loss': 0.8438, 'learning_rate': 0.00018163522086418299, 'epoch': 0.22}
{'loss': 0.897, 'learning_rate': 0.00018159921134900487, 'epoch': 0.22}
{'loss': 0.9199, 'learning_rate': 0.00018156317014163338, 'epoch': 0.22}
{'loss': 0.8147, 'learning_rate': 0.0001815270972560664, 'epoch': 0.22}
{'loss': 0.8525, 'learning_rate': 0.00018149099270631434, 'epoch': 0.22}
{'loss': 0.3319, 'learning_rate': 0.00018145485650639974, 'epoch': 0.22}
{'loss': 0.8425, 'learning_rate': 0.00018141868867035745, 'epoch': 0.22}
{'loss': 0.9229, 'learning_rate': 0.00018138248921223467, 'epoch': 0.22}
{'loss': 0.3119, 'learning_rate': 0.00018134625814609083, 'epoch': 0.22}
{'loss': 0.8212, 'learning_rate': 0.00018130999548599767, 'epoch': 0.22}
{'loss': 0.8241, 'learning_rate': 0.00018127370124603926, 'epoch': 0.22}
{'loss': 0.8669, 'learning_rate': 0.00018123737544031176, 'epoch': 0.22}
{'loss': 0.8821, 'learning_rate': 0.00018120101808292372, 'epoch': 0.22}
{'loss': 0.9036, 'learning_rate': 0.000181164629187996, 'epoch': 0.22}
{'loss': 0.8496, 'learning_rate': 0.00018112820876966152, 'epoch': 0.22}
{'loss': 0.8308, 'learning_rate': 0.00018109175684206559, 'epoch': 0.22}
{'loss': 0.8506, 'learning_rate': 0.00018105527341936574, 'epoch': 0.22}
{'loss': 0.3233, 'learning_rate': 0.0001810187585157317, 'epoch': 0.22}
{'loss': 0.3165, 'learning_rate': 0.00018098221214534542, 'epoch': 0.22}
{'loss': 0.8038, 'learning_rate': 0.00018094563432240107, 'epoch': 0.22}
{'loss': 0.8289, 'learning_rate': 0.0001809090250611051, 'epoch': 0.22}
{'loss': 0.8838, 'learning_rate': 0.00018087238437567614, 'epoch': 0.22}
{'loss': 0.8188, 'learning_rate': 0.00018083571228034496, 'epoch': 0.22}
{'loss': 0.7534, 'learning_rate': 0.0001807990087893546, 'epoch': 0.22}
{'loss': 0.9214, 'learning_rate': 0.00018076227391696027, 'epoch': 0.22}
{'loss': 0.3206, 'learning_rate': 0.0001807255076774294, 'epoch': 0.22}
{'loss': 0.8206, 'learning_rate': 0.00018068871008504152, 'epoch': 0.23}
{'loss': 0.8118, 'learning_rate': 0.00018065188115408843, 'epoch': 0.23}
{'loss': 0.8665, 'learning_rate': 0.00018061502089887407, 'epoch': 0.23}
{'loss': 0.8601, 'learning_rate': 0.0001805781293337145, 'epoch': 0.23}
{'loss': 0.9189, 'learning_rate': 0.00018054120647293797, 'epoch': 0.23}
{'loss': 0.3519, 'learning_rate': 0.00018050425233088495, 'epoch': 0.23}
{'loss': 0.8667, 'learning_rate': 0.000180467266921908, 'epoch': 0.23}
{'loss': 0.9299, 'learning_rate': 0.00018043025026037176, 'epoch': 0.23}
{'loss': 0.8269, 'learning_rate': 0.00018039320236065313, 'epoch': 0.23}
{'loss': 0.8875, 'learning_rate': 0.0001803561232371411, 'epoch': 0.23}
{'loss': 0.8711, 'learning_rate': 0.0001803190129042367, 'epoch': 0.23}
{'loss': 0.8835, 'learning_rate': 0.00018028187137635325, 'epoch': 0.23}
{'loss': 0.864, 'learning_rate': 0.000180244698667916, 'epoch': 0.23}
{'loss': 0.8018, 'learning_rate': 0.00018020749479336247, 'epoch': 0.23}
{'loss': 0.3405, 'learning_rate': 0.00018017025976714218, 'epoch': 0.23}
{'loss': 0.806, 'learning_rate': 0.00018013299360371686, 'epoch': 0.23}
{'loss': 0.8059, 'learning_rate': 0.00018009569631756013, 'epoch': 0.23}
{'loss': 0.8499, 'learning_rate': 0.0001800583679231579, 'epoch': 0.23}
{'loss': 0.8701, 'learning_rate': 0.0001800210084350081, 'epoch': 0.23}
{'loss': 0.9001, 'learning_rate': 0.0001799836178676207, 'epoch': 0.23}
{'loss': 0.9463, 'learning_rate': 0.00017994619623551777, 'epoch': 0.23}
{'loss': 0.8248, 'learning_rate': 0.00017990874355323344, 'epoch': 0.23}
{'loss': 0.8018, 'learning_rate': 0.00017987125983531392, 'epoch': 0.23}
{'loss': 0.9182, 'learning_rate': 0.00017983374509631742, 'epoch': 0.23}
{'loss': 0.8729, 'learning_rate': 0.00017979619935081426, 'epoch': 0.23}
{'loss': 0.8823, 'learning_rate': 0.0001797586226133867, 'epoch': 0.23}
{'loss': 0.8611, 'learning_rate': 0.00017972101489862924, 'epoch': 0.23}
{'loss': 0.8586, 'learning_rate': 0.00017968337622114824, 'epoch': 0.23}
{'loss': 0.8905, 'learning_rate': 0.00017964570659556207, 'epoch': 0.23}
{'loss': 0.7843, 'learning_rate': 0.0001796080060365012, 'epoch': 0.23}
{'loss': 0.868, 'learning_rate': 0.00017957027455860813, 'epoch': 0.23}
{'loss': 0.8931, 'learning_rate': 0.0001795325121765373, 'epoch': 0.23}
{'loss': 0.32, 'learning_rate': 0.00017949471890495522, 'epoch': 0.23}
{'loss': 0.8855, 'learning_rate': 0.0001794568947585403, 'epoch': 0.23}
{'loss': 0.8167, 'learning_rate': 0.00017941903975198306, 'epoch': 0.23}
{'loss': 0.9314, 'learning_rate': 0.00017938115389998593, 'epoch': 0.23}
{'loss': 0.8479, 'learning_rate': 0.00017934323721726332, 'epoch': 0.23}
{'loss': 0.9094, 'learning_rate': 0.00017930528971854163, 'epoch': 0.23}
{'loss': 0.8009, 'learning_rate': 0.00017926731141855928, 'epoch': 0.23}
{'loss': 0.8655, 'learning_rate': 0.00017922930233206657, 'epoch': 0.23}
{'loss': 0.8452, 'learning_rate': 0.00017919126247382575, 'epoch': 0.23}
{'loss': 0.8479, 'learning_rate': 0.00017915319185861118, 'epoch': 0.23}
{'loss': 0.8076, 'learning_rate': 0.0001791150905012089, 'epoch': 0.23}
{'loss': 0.7861, 'learning_rate': 0.00017907695841641713, 'epoch': 0.23}
{'loss': 0.8369, 'learning_rate': 0.00017903879561904598, 'epoch': 0.23}
{'loss': 0.9246, 'learning_rate': 0.0001790006021239173, 'epoch': 0.23}
{'loss': 0.3112, 'learning_rate': 0.00017896237794586513, 'epoch': 0.23}
{'loss': 0.793, 'learning_rate': 0.0001789241230997352, 'epoch': 0.23}
{'loss': 0.8745, 'learning_rate': 0.00017888583760038534, 'epoch': 0.23}
{'loss': 0.8586, 'learning_rate': 0.00017884752146268512, 'epoch': 0.23}
{'loss': 0.8462, 'learning_rate': 0.00017880917470151615, 'epoch': 0.23}
{'loss': 0.8906, 'learning_rate': 0.00017877079733177184, 'epoch': 0.23}
{'loss': 0.8201, 'learning_rate': 0.0001787323893683575, 'epoch': 0.24}
{'loss': 0.8572, 'learning_rate': 0.0001786939508261904, 'epoch': 0.24}
{'loss': 0.8376, 'learning_rate': 0.00017865548172019957, 'epoch': 0.24}
{'loss': 0.3552, 'learning_rate': 0.00017861698206532596, 'epoch': 0.24}
{'loss': 0.7803, 'learning_rate': 0.00017857845187652246, 'epoch': 0.24}
{'loss': 0.307, 'learning_rate': 0.00017853989116875375, 'epoch': 0.24}
{'loss': 0.8625, 'learning_rate': 0.00017850129995699624, 'epoch': 0.24}
{'loss': 0.8696, 'learning_rate': 0.00017846267825623845, 'epoch': 0.24}
{'loss': 0.8826, 'learning_rate': 0.00017842402608148053, 'epoch': 0.24}
{'loss': 0.9185, 'learning_rate': 0.00017838534344773453, 'epoch': 0.24}
{'loss': 0.8577, 'learning_rate': 0.00017834663037002443, 'epoch': 0.24}
{'loss': 0.9106, 'learning_rate': 0.00017830788686338584, 'epoch': 0.24}
{'loss': 0.8176, 'learning_rate': 0.00017826911294286637, 'epoch': 0.24}
{'loss': 0.9216, 'learning_rate': 0.0001782303086235253, 'epoch': 0.24}
{'loss': 0.8611, 'learning_rate': 0.0001781914739204338, 'epoch': 0.24}
{'loss': 0.8735, 'learning_rate': 0.00017815260884867484, 'epoch': 0.24}
{'loss': 0.7974, 'learning_rate': 0.00017811371342334317, 'epoch': 0.24}
{'loss': 0.8567, 'learning_rate': 0.0001780747876595453, 'epoch': 0.24}
{'loss': 0.8445, 'learning_rate': 0.00017803583157239956, 'epoch': 0.24}
{'loss': 0.8381, 'learning_rate': 0.00017799684517703603, 'epoch': 0.24}
{'loss': 0.3416, 'learning_rate': 0.0001779578284885966, 'epoch': 0.24}
{'loss': 0.8374, 'learning_rate': 0.00017791878152223492, 'epoch': 0.24}
{'loss': 0.886, 'learning_rate': 0.00017787970429311632, 'epoch': 0.24}
{'loss': 0.8025, 'learning_rate': 0.00017784059681641797, 'epoch': 0.24}
{'loss': 0.8914, 'learning_rate': 0.0001778014591073288, 'epoch': 0.24}
{'loss': 0.8308, 'learning_rate': 0.00017776229118104942, 'epoch': 0.24}
{'loss': 0.8245, 'learning_rate': 0.0001777230930527922, 'epoch': 0.24}
{'loss': 0.8589, 'learning_rate': 0.00017768386473778123, 'epoch': 0.24}
{'loss': 0.8652, 'learning_rate': 0.00017764460625125235, 'epoch': 0.24}
{'loss': 0.8164, 'learning_rate': 0.0001776053176084531, 'epoch': 0.24}
{'loss': 0.8853, 'learning_rate': 0.00017756599882464273, 'epoch': 0.24}
{'loss': 0.8604, 'learning_rate': 0.00017752664991509224, 'epoch': 0.24}
{'loss': 0.9038, 'learning_rate': 0.0001774872708950842, 'epoch': 0.24}
{'loss': 0.306, 'learning_rate': 0.0001774478617799131, 'epoch': 0.24}
{'loss': 0.8696, 'learning_rate': 0.00017740842258488488, 'epoch': 0.24}
{'loss': 0.929, 'learning_rate': 0.0001773689533253173, 'epoch': 0.24}
{'loss': 0.876, 'learning_rate': 0.00017732945401653977, 'epoch': 0.24}
{'loss': 0.8232, 'learning_rate': 0.0001772899246738934, 'epoch': 0.24}
{'loss': 0.8416, 'learning_rate': 0.00017725036531273088, 'epoch': 0.24}
{'loss': 0.8623, 'learning_rate': 0.00017721077594841662, 'epoch': 0.24}
{'loss': 0.8494, 'learning_rate': 0.00017717115659632667, 'epoch': 0.24}
{'loss': 0.885, 'learning_rate': 0.00017713150727184876, 'epoch': 0.24}
{'loss': 0.8564, 'learning_rate': 0.00017709182799038222, 'epoch': 0.24}
{'loss': 0.8469, 'learning_rate': 0.000177052118767338, 'epoch': 0.24}
{'loss': 0.8298, 'learning_rate': 0.00017701237961813875, 'epoch': 0.24}
{'loss': 0.8811, 'learning_rate': 0.00017697261055821863, 'epoch': 0.24}
{'loss': 0.8289, 'learning_rate': 0.00017693281160302356, 'epoch': 0.24}
{'loss': 0.8135, 'learning_rate': 0.00017689298276801093, 'epoch': 0.24}
{'loss': 0.8345, 'learning_rate': 0.00017685312406864984, 'epoch': 0.24}
{'loss': 0.368, 'learning_rate': 0.00017681323552042096, 'epoch': 0.24}
{'loss': 0.8318, 'learning_rate': 0.00017677331713881648, 'epoch': 0.24}
{'loss': 0.7627, 'learning_rate': 0.0001767333689393403, 'epoch': 0.24}
{'loss': 0.8901, 'learning_rate': 0.00017669339093750785, 'epoch': 0.25}
{'loss': 0.8635, 'learning_rate': 0.00017665338314884607, 'epoch': 0.25}
{'loss': 0.8752, 'learning_rate': 0.00017661334558889357, 'epoch': 0.25}
{'loss': 0.8999, 'learning_rate': 0.00017657327827320045, 'epoch': 0.25}
{'loss': 0.8422, 'learning_rate': 0.0001765331812173284, 'epoch': 0.25}
{'loss': 0.8564, 'learning_rate': 0.00017649305443685068, 'epoch': 0.25}
{'loss': 0.7786, 'learning_rate': 0.000176452897947352, 'epoch': 0.25}
{'loss': 0.8748, 'learning_rate': 0.00017641271176442876, 'epoch': 0.25}
{'loss': 0.8762, 'learning_rate': 0.00017637249590368877, 'epoch': 0.25}
{'loss': 0.8574, 'learning_rate': 0.00017633225038075142, 'epoch': 0.25}
{'loss': 0.8523, 'learning_rate': 0.00017629197521124755, 'epoch': 0.25}
{'loss': 0.8774, 'learning_rate': 0.00017625167041081965, 'epoch': 0.25}
{'loss': 0.8259, 'learning_rate': 0.00017621133599512163, 'epoch': 0.25}
{'loss': 0.8513, 'learning_rate': 0.00017617097197981889, 'epoch': 0.25}
{'loss': 0.876, 'learning_rate': 0.0001761305783805883, 'epoch': 0.25}
{'loss': 0.3197, 'learning_rate': 0.00017609015521311835, 'epoch': 0.25}
{'loss': 0.8718, 'learning_rate': 0.0001760497024931089, 'epoch': 0.25}
{'loss': 0.8284, 'learning_rate': 0.00017600922023627136, 'epoch': 0.25}
{'loss': 0.8189, 'learning_rate': 0.0001759687084583285, 'epoch': 0.25}
WARNING: tokenization mismatch: 1 vs. 737. (ignored)
{'loss': 0.7563, 'learning_rate': 0.0001759281671750147, 'epoch': 0.25}
{'loss': 0.3513, 'learning_rate': 0.00017588759640207563, 'epoch': 0.25}
{'loss': 0.8604, 'learning_rate': 0.00017584699615526858, 'epoch': 0.25}
{'loss': 0.8945, 'learning_rate': 0.00017580636645036225, 'epoch': 0.25}
{'loss': 0.8657, 'learning_rate': 0.0001757657073031367, 'epoch': 0.25}
{'loss': 0.3365, 'learning_rate': 0.00017572501872938342, 'epoch': 0.25}
{'loss': 0.8757, 'learning_rate': 0.0001756843007449055, 'epoch': 0.25}
{'loss': 0.8845, 'learning_rate': 0.00017564355336551727, 'epoch': 0.25}
{'loss': 0.3603, 'learning_rate': 0.00017560277660704453, 'epoch': 0.25}
{'loss': 0.835, 'learning_rate': 0.00017556197048532456, 'epoch': 0.25}
{'loss': 0.8336, 'learning_rate': 0.00017552113501620594, 'epoch': 0.25}
{'loss': 0.9253, 'learning_rate': 0.00017548027021554874, 'epoch': 0.25}
{'loss': 0.7922, 'learning_rate': 0.0001754393760992243, 'epoch': 0.25}
{'loss': 0.8513, 'learning_rate': 0.00017539845268311547, 'epoch': 0.25}
{'loss': 0.8171, 'learning_rate': 0.00017535749998311647, 'epoch': 0.25}
{'loss': 0.3428, 'learning_rate': 0.0001753165180151328, 'epoch': 0.25}
{'loss': 0.9119, 'learning_rate': 0.0001752755067950814, 'epoch': 0.25}
{'loss': 0.8372, 'learning_rate': 0.0001752344663388906, 'epoch': 0.25}
{'loss': 0.8673, 'learning_rate': 0.00017519339666249997, 'epoch': 0.25}
{'loss': 0.842, 'learning_rate': 0.00017515229778186054, 'epoch': 0.25}
{'loss': 0.8337, 'learning_rate': 0.0001751111697129346, 'epoch': 0.25}
{'loss': 0.8773, 'learning_rate': 0.00017507001247169587, 'epoch': 0.25}
{'loss': 0.8059, 'learning_rate': 0.00017502882607412933, 'epoch': 0.25}
{'loss': 0.8315, 'learning_rate': 0.00017498761053623128, 'epoch': 0.25}
{'loss': 0.8346, 'learning_rate': 0.0001749463658740094, 'epoch': 0.25}
{'loss': 0.8914, 'learning_rate': 0.00017490509210348263, 'epoch': 0.25}
{'loss': 0.7635, 'learning_rate': 0.0001748637892406812, 'epoch': 0.25}
{'loss': 0.8027, 'learning_rate': 0.0001748224573016467, 'epoch': 0.25}
{'loss': 0.8984, 'learning_rate': 0.00017478109630243195, 'epoch': 0.25}
{'loss': 0.8572, 'learning_rate': 0.0001747397062591011, 'epoch': 0.25}
{'loss': 0.8948, 'learning_rate': 0.00017469828718772958, 'epoch': 0.25}
{'loss': 0.3201, 'learning_rate': 0.00017465683910440405, 'epoch': 0.25}
{'loss': 0.8503, 'learning_rate': 0.00017461536202522247, 'epoch': 0.25}
{'loss': 0.8302, 'learning_rate': 0.0001745738559662941, 'epoch': 0.26}
{'loss': 0.834, 'learning_rate': 0.00017453232094373936, 'epoch': 0.26}
{'loss': 0.8059, 'learning_rate': 0.00017449075697369005, 'epoch': 0.26}
{'loss': 0.8171, 'learning_rate': 0.00017444916407228904, 'epoch': 0.26}
{'loss': 0.8447, 'learning_rate': 0.0001744075422556906, 'epoch': 0.26}
{'loss': 0.8284, 'learning_rate': 0.00017436589154006012, 'epoch': 0.26}
{'loss': 0.8838, 'learning_rate': 0.00017432421194157432, 'epoch': 0.26}
{'loss': 0.8735, 'learning_rate': 0.000174282503476421, 'epoch': 0.26}
{'loss': 0.8596, 'learning_rate': 0.00017424076616079934, 'epoch': 0.26}
{'loss': 0.8801, 'learning_rate': 0.00017419900001091953, 'epoch': 0.26}
{'loss': 0.8887, 'learning_rate': 0.00017415720504300314, 'epoch': 0.26}
{'loss': 0.8618, 'learning_rate': 0.00017411538127328283, 'epoch': 0.26}
{'loss': 0.851, 'learning_rate': 0.00017407352871800244, 'epoch': 0.26}
{'loss': 0.8223, 'learning_rate': 0.00017403164739341706, 'epoch': 0.26}
{'loss': 0.8335, 'learning_rate': 0.0001739897373157929, 'epoch': 0.26}
{'loss': 0.8579, 'learning_rate': 0.00017394779850140734, 'epoch': 0.26}
{'loss': 0.8782, 'learning_rate': 0.00017390583096654896, 'epoch': 0.26}
{'loss': 0.8577, 'learning_rate': 0.00017386383472751743, 'epoch': 0.26}
{'loss': 0.8567, 'learning_rate': 0.00017382180980062364, 'epoch': 0.26}
{'loss': 0.7993, 'learning_rate': 0.00017377975620218953, 'epoch': 0.26}
{'loss': 0.8331, 'learning_rate': 0.00017373767394854834, 'epoch': 0.26}
{'loss': 0.8455, 'learning_rate': 0.00017369556305604422, 'epoch': 0.26}
{'loss': 0.8772, 'learning_rate': 0.00017365342354103264, 'epoch': 0.26}
{'loss': 0.863, 'learning_rate': 0.00017361125541988002, 'epoch': 0.26}
{'loss': 0.9036, 'learning_rate': 0.00017356905870896406, 'epoch': 0.26}
{'loss': 0.8308, 'learning_rate': 0.0001735268334246734, 'epoch': 0.26}
{'loss': 0.3265, 'learning_rate': 0.0001734845795834079, 'epoch': 0.26}
{'loss': 0.8408, 'learning_rate': 0.00017344229720157845, 'epoch': 0.26}
{'loss': 0.8413, 'learning_rate': 0.00017339998629560706, 'epoch': 0.26}
{'loss': 0.8184, 'learning_rate': 0.00017335764688192678, 'epoch': 0.26}
{'loss': 0.9373, 'learning_rate': 0.0001733152789769817, 'epoch': 0.26}
{'loss': 0.8131, 'learning_rate': 0.00017327288259722713, 'epoch': 0.26}
{'loss': 0.8665, 'learning_rate': 0.00017323045775912926, 'epoch': 0.26}
{'loss': 0.8804, 'learning_rate': 0.00017318800447916542, 'epoch': 0.26}
{'loss': 0.8552, 'learning_rate': 0.00017314552277382402, 'epoch': 0.26}
{'loss': 0.8494, 'learning_rate': 0.00017310301265960446, 'epoch': 0.26}
{'loss': 0.8589, 'learning_rate': 0.00017306047415301706, 'epoch': 0.26}
{'loss': 0.8779, 'learning_rate': 0.00017301790727058345, 'epoch': 0.26}
{'loss': 0.9272, 'learning_rate': 0.000172975312028836, 'epoch': 0.26}
{'loss': 0.8472, 'learning_rate': 0.00017293268844431828, 'epoch': 0.26}
{'loss': 0.876, 'learning_rate': 0.00017289003653358473, 'epoch': 0.26}
{'loss': 0.7675, 'learning_rate': 0.00017284735631320093, 'epoch': 0.26}
{'loss': 0.8447, 'learning_rate': 0.00017280464779974335, 'epoch': 0.26}
{'loss': 0.7449, 'learning_rate': 0.0001727619110097995, 'epoch': 0.26}
{'loss': 0.8423, 'learning_rate': 0.00017271914595996782, 'epoch': 0.26}
{'loss': 0.8926, 'learning_rate': 0.00017267635266685782, 'epoch': 0.26}
{'loss': 0.8525, 'learning_rate': 0.0001726335311470899, 'epoch': 0.26}
{'loss': 0.842, 'learning_rate': 0.00017259068141729542, 'epoch': 0.26}
{'loss': 0.345, 'learning_rate': 0.00017254780349411675, 'epoch': 0.26}
{'loss': 0.8257, 'learning_rate': 0.00017250489739420718, 'epoch': 0.26}
{'loss': 0.3284, 'learning_rate': 0.00017246196313423093, 'epoch': 0.26}
{'loss': 0.874, 'learning_rate': 0.00017241900073086317, 'epoch': 0.26}
{'loss': 0.8635, 'learning_rate': 0.00017237601020079002, 'epoch': 0.27}
{'loss': 0.8787, 'learning_rate': 0.0001723329915607085, 'epoch': 0.27}
{'loss': 0.8311, 'learning_rate': 0.00017228994482732651, 'epoch': 0.27}
{'loss': 0.887, 'learning_rate': 0.000172246870017363, 'epoch': 0.27}
{'loss': 0.8237, 'learning_rate': 0.00017220376714754764, 'epoch': 0.27}
{'loss': 0.8962, 'learning_rate': 0.00017216063623462112, 'epoch': 0.27}
{'loss': 0.8755, 'learning_rate': 0.00017211747729533502, 'epoch': 0.27}
{'loss': 0.9038, 'learning_rate': 0.00017207429034645175, 'epoch': 0.27}
{'loss': 0.8894, 'learning_rate': 0.0001720310754047446, 'epoch': 0.27}
{'loss': 0.842, 'learning_rate': 0.00017198783248699779, 'epoch': 0.27}
{'loss': 0.8728, 'learning_rate': 0.00017194456161000635, 'epoch': 0.27}
{'loss': 0.8477, 'learning_rate': 0.0001719012627905762, 'epoch': 0.27}
{'loss': 0.3503, 'learning_rate': 0.00017185793604552408, 'epoch': 0.27}
{'loss': 0.7781, 'learning_rate': 0.00017181458139167764, 'epoch': 0.27}
{'loss': 0.8342, 'learning_rate': 0.00017177119884587535, 'epoch': 0.27}
{'loss': 0.897, 'learning_rate': 0.0001717277884249664, 'epoch': 0.27}
{'loss': 0.339, 'learning_rate': 0.00017168435014581094, 'epoch': 0.27}
{'loss': 0.8679, 'learning_rate': 0.0001716408840252799, 'epoch': 0.27}
{'loss': 0.843, 'learning_rate': 0.00017159739008025505, 'epoch': 0.27}
{'loss': 0.3315, 'learning_rate': 0.0001715538683276289, 'epoch': 0.27}
{'loss': 0.8628, 'learning_rate': 0.0001715103187843048, 'epoch': 0.27}
{'loss': 0.9233, 'learning_rate': 0.0001714667414671969, 'epoch': 0.27}
{'loss': 0.8923, 'learning_rate': 0.0001714231363932301, 'epoch': 0.27}
{'loss': 0.8987, 'learning_rate': 0.00017137950357934016, 'epoch': 0.27}
{'loss': 0.8289, 'learning_rate': 0.00017133584304247353, 'epoch': 0.27}
{'loss': 0.821, 'learning_rate': 0.00017129215479958745, 'epoch': 0.27}
{'loss': 0.844, 'learning_rate': 0.00017124843886765, 'epoch': 0.27}
{'loss': 0.8514, 'learning_rate': 0.00017120469526363982, 'epoch': 0.27}
{'loss': 0.8843, 'learning_rate': 0.00017116092400454652, 'epoch': 0.27}
{'loss': 0.8435, 'learning_rate': 0.00017111712510737035, 'epoch': 0.27}
{'loss': 0.8422, 'learning_rate': 0.00017107329858912225, 'epoch': 0.27}
{'loss': 0.8521, 'learning_rate': 0.00017102944446682395, 'epoch': 0.27}
{'loss': 0.8848, 'learning_rate': 0.00017098556275750787, 'epoch': 0.27}
{'loss': 0.8285, 'learning_rate': 0.00017094165347821723, 'epoch': 0.27}
{'loss': 0.7598, 'learning_rate': 0.00017089771664600582, 'epoch': 0.27}
{'loss': 0.8982, 'learning_rate': 0.00017085375227793818, 'epoch': 0.27}
{'loss': 0.3595, 'learning_rate': 0.00017080976039108965, 'epoch': 0.27}
{'loss': 0.8276, 'learning_rate': 0.00017076574100254613, 'epoch': 0.27}
{'loss': 0.8403, 'learning_rate': 0.00017072169412940422, 'epoch': 0.27}
{'loss': 0.8721, 'learning_rate': 0.00017067761978877121, 'epoch': 0.27}
{'loss': 0.8374, 'learning_rate': 0.00017063351799776513, 'epoch': 0.27}
{'loss': 0.8459, 'learning_rate': 0.00017058938877351456, 'epoch': 0.27}
{'loss': 0.8379, 'learning_rate': 0.00017054523213315878, 'epoch': 0.27}
{'loss': 0.3016, 'learning_rate': 0.00017050104809384774, 'epoch': 0.27}
{'loss': 0.8201, 'learning_rate': 0.000170456836672742, 'epoch': 0.27}
{'loss': 0.8333, 'learning_rate': 0.00017041259788701281, 'epoch': 0.27}
{'loss': 0.8586, 'learning_rate': 0.0001703683317538419, 'epoch': 0.27}
{'loss': 0.3333, 'learning_rate': 0.00017032403829042182, 'epoch': 0.27}
{'loss': 0.3511, 'learning_rate': 0.00017027971751395563, 'epoch': 0.27}
{'loss': 0.84, 'learning_rate': 0.00017023536944165698, 'epoch': 0.27}
{'loss': 0.8923, 'learning_rate': 0.00017019099409075014, 'epoch': 0.27}
{'loss': 0.8503, 'learning_rate': 0.00017014659147847002, 'epoch': 0.27}
{'loss': 0.8674, 'learning_rate': 0.00017010216162206208, 'epoch': 0.28}
{'loss': 0.8831, 'learning_rate': 0.00017005770453878234, 'epoch': 0.28}
{'loss': 0.8767, 'learning_rate': 0.00017001322024589742, 'epoch': 0.28}
{'loss': 0.8728, 'learning_rate': 0.00016996870876068452, 'epoch': 0.28}
{'loss': 0.8486, 'learning_rate': 0.00016992417010043142, 'epoch': 0.28}
{'loss': 0.8325, 'learning_rate': 0.00016987960428243638, 'epoch': 0.28}
{'loss': 0.8743, 'learning_rate': 0.00016983501132400824, 'epoch': 0.28}
{'loss': 0.8495, 'learning_rate': 0.00016979039124246642, 'epoch': 0.28}
{'loss': 0.3187, 'learning_rate': 0.00016974574405514083, 'epoch': 0.28}
{'loss': 0.8896, 'learning_rate': 0.00016970106977937192, 'epoch': 0.28}
{'loss': 0.3056, 'learning_rate': 0.0001696563684325107, 'epoch': 0.28}
{'loss': 0.8232, 'learning_rate': 0.00016961164003191862, 'epoch': 0.28}
{'loss': 0.8484, 'learning_rate': 0.00016956688459496765, 'epoch': 0.28}
{'loss': 0.8523, 'learning_rate': 0.00016952210213904038, 'epoch': 0.28}
{'loss': 0.8713, 'learning_rate': 0.0001694772926815297, 'epoch': 0.28}
{'loss': 0.9106, 'learning_rate': 0.00016943245623983916, 'epoch': 0.28}
{'loss': 0.9519, 'learning_rate': 0.00016938759283138268, 'epoch': 0.28}
{'loss': 0.8677, 'learning_rate': 0.00016934270247358468, 'epoch': 0.28}
{'loss': 0.8816, 'learning_rate': 0.00016929778518388007, 'epoch': 0.28}
{'loss': 0.8781, 'learning_rate': 0.00016925284097971425, 'epoch': 0.28}
{'loss': 0.9138, 'learning_rate': 0.00016920786987854294, 'epoch': 0.28}
{'loss': 0.8379, 'learning_rate': 0.00016916287189783252, 'epoch': 0.28}
{'loss': 0.8931, 'learning_rate': 0.00016911784705505957, 'epoch': 0.28}
{'loss': 0.9207, 'learning_rate': 0.0001690727953677113, 'epoch': 0.28}
{'loss': 0.8208, 'learning_rate': 0.00016902771685328523, 'epoch': 0.28}
{'loss': 0.8533, 'learning_rate': 0.00016898261152928931, 'epoch': 0.28}
{'loss': 0.8784, 'learning_rate': 0.00016893747941324196, 'epoch': 0.28}
{'loss': 0.8579, 'learning_rate': 0.000168892320522672, 'epoch': 0.28}
{'loss': 0.8606, 'learning_rate': 0.00016884713487511858, 'epoch': 0.28}
{'loss': 0.9338, 'learning_rate': 0.0001688019224881313, 'epoch': 0.28}
{'loss': 0.8596, 'learning_rate': 0.00016875668337927013, 'epoch': 0.28}
{'loss': 0.8735, 'learning_rate': 0.00016871141756610544, 'epoch': 0.28}
{'loss': 0.833, 'learning_rate': 0.0001686661250662179, 'epoch': 0.28}
{'loss': 0.8503, 'learning_rate': 0.0001686208058971986, 'epoch': 0.28}
{'loss': 0.8167, 'learning_rate': 0.00016857546007664905, 'epoch': 0.28}
{'loss': 0.814, 'learning_rate': 0.000168530087622181, 'epoch': 0.28}
{'loss': 0.8982, 'learning_rate': 0.0001684846885514166, 'epoch': 0.28}
{'loss': 0.3493, 'learning_rate': 0.00016843926288198828, 'epoch': 0.28}
{'loss': 0.8513, 'learning_rate': 0.0001683938106315389, 'epoch': 0.28}
{'loss': 0.3385, 'learning_rate': 0.0001683483318177216, 'epoch': 0.28}
{'loss': 0.8577, 'learning_rate': 0.00016830282645819976, 'epoch': 0.28}
{'loss': 0.8792, 'learning_rate': 0.0001682572945706472, 'epoch': 0.28}
{'loss': 0.8516, 'learning_rate': 0.00016821173617274793, 'epoch': 0.28}
{'loss': 0.8916, 'learning_rate': 0.00016816615128219635, 'epoch': 0.28}
{'loss': 0.8159, 'learning_rate': 0.0001681205399166971, 'epoch': 0.28}
{'loss': 0.8728, 'learning_rate': 0.00016807490209396506, 'epoch': 0.28}
{'loss': 0.845, 'learning_rate': 0.00016802923783172552, 'epoch': 0.28}
{'loss': 0.8521, 'learning_rate': 0.0001679835471477139, 'epoch': 0.28}
{'loss': 0.8372, 'learning_rate': 0.00016793783005967592, 'epoch': 0.28}
{'loss': 0.3383, 'learning_rate': 0.0001678920865853676, 'epoch': 0.28}
{'loss': 0.8218, 'learning_rate': 0.00016784631674255518, 'epoch': 0.28}
{'loss': 0.8638, 'learning_rate': 0.00016780052054901512, 'epoch': 0.28}
{'loss': 0.8643, 'learning_rate': 0.00016775469802253414, 'epoch': 0.29}
{'loss': 0.814, 'learning_rate': 0.0001677088491809092, 'epoch': 0.29}
{'loss': 0.8003, 'learning_rate': 0.00016766297404194745, 'epoch': 0.29}
{'loss': 0.8296, 'learning_rate': 0.00016761707262346625, 'epoch': 0.29}
{'loss': 0.849, 'learning_rate': 0.0001675711449432932, 'epoch': 0.29}
{'loss': 0.8549, 'learning_rate': 0.00016752519101926607, 'epoch': 0.29}
{'loss': 0.8704, 'learning_rate': 0.00016747921086923283, 'epoch': 0.29}
{'loss': 0.9133, 'learning_rate': 0.00016743320451105168, 'epoch': 0.29}
{'loss': 0.8547, 'learning_rate': 0.00016738717196259094, 'epoch': 0.29}
{'loss': 0.8293, 'learning_rate': 0.0001673411132417291, 'epoch': 0.29}
{'loss': 0.886, 'learning_rate': 0.0001672950283663548, 'epoch': 0.29}
{'loss': 0.8499, 'learning_rate': 0.00016724891735436697, 'epoch': 0.29}
{'loss': 0.8545, 'learning_rate': 0.00016720278022367452, 'epoch': 0.29}
{'loss': 0.8508, 'learning_rate': 0.00016715661699219663, 'epoch': 0.29}
{'loss': 0.8102, 'learning_rate': 0.00016711042767786257, 'epoch': 0.29}
{'loss': 0.8337, 'learning_rate': 0.00016706421229861166, 'epoch': 0.29}
{'loss': 0.3173, 'learning_rate': 0.00016701797087239354, 'epoch': 0.29}
{'loss': 0.8688, 'learning_rate': 0.0001669717034171677, 'epoch': 0.29}
{'loss': 0.8408, 'learning_rate': 0.000166925409950904, 'epoch': 0.29}
{'loss': 0.801, 'learning_rate': 0.00016687909049158228, 'epoch': 0.29}
{'loss': 0.8894, 'learning_rate': 0.00016683274505719246, 'epoch': 0.29}
{'loss': 0.9236, 'learning_rate': 0.00016678637366573454, 'epoch': 0.29}
{'loss': 0.9124, 'learning_rate': 0.0001667399763352187, 'epoch': 0.29}
{'loss': 0.8728, 'learning_rate': 0.0001666935530836651, 'epoch': 0.29}
{'loss': 0.8608, 'learning_rate': 0.00016664710392910395, 'epoch': 0.29}
{'loss': 0.8774, 'learning_rate': 0.00016660062888957563, 'epoch': 0.29}
{'loss': 0.8325, 'learning_rate': 0.00016655412798313051, 'epoch': 0.29}
{'loss': 0.8076, 'learning_rate': 0.00016650760122782895, 'epoch': 0.29}
{'loss': 0.8234, 'learning_rate': 0.00016646104864174145, 'epoch': 0.29}
{'loss': 0.8562, 'learning_rate': 0.0001664144702429485, 'epoch': 0.29}
{'loss': 0.7981, 'learning_rate': 0.0001663678660495406, 'epoch': 0.29}
{'loss': 0.7959, 'learning_rate': 0.0001663212360796183, 'epoch': 0.29}
{'loss': 0.8782, 'learning_rate': 0.0001662745803512921, 'epoch': 0.29}
{'loss': 0.8716, 'learning_rate': 0.00016622789888268258, 'epoch': 0.29}
{'loss': 0.7656, 'learning_rate': 0.00016618119169192026, 'epoch': 0.29}
{'loss': 0.7465, 'learning_rate': 0.00016613445879714572, 'epoch': 0.29}
{'loss': 0.8254, 'learning_rate': 0.00016608770021650943, 'epoch': 0.29}
{'loss': 0.8601, 'learning_rate': 0.00016604091596817192, 'epoch': 0.29}
{'loss': 0.877, 'learning_rate': 0.00016599410607030365, 'epoch': 0.29}
{'loss': 0.8801, 'learning_rate': 0.00016594727054108498, 'epoch': 0.29}
{'loss': 0.8777, 'learning_rate': 0.0001659004093987064, 'epoch': 0.29}
{'loss': 0.8167, 'learning_rate': 0.00016585352266136814, 'epoch': 0.29}
{'loss': 0.8433, 'learning_rate': 0.00016580661034728053, 'epoch': 0.29}
{'loss': 0.8091, 'learning_rate': 0.00016575967247466376, 'epoch': 0.29}
{'loss': 0.792, 'learning_rate': 0.0001657127090617479, 'epoch': 0.29}
{'loss': 0.8125, 'learning_rate': 0.0001656657201267731, 'epoch': 0.29}
{'loss': 0.8555, 'learning_rate': 0.00016561870568798924, 'epoch': 0.29}
{'loss': 0.8435, 'learning_rate': 0.00016557166576365622, 'epoch': 0.29}
{'loss': 0.8984, 'learning_rate': 0.00016552460037204384, 'epoch': 0.29}
{'loss': 0.7708, 'learning_rate': 0.00016547750953143167, 'epoch': 0.29}
{'loss': 0.8489, 'learning_rate': 0.00016543039326010928, 'epoch': 0.29}
{'loss': 0.8574, 'learning_rate': 0.00016538325157637614, 'epoch': 0.29}
{'loss': 0.334, 'learning_rate': 0.00016533608449854147, 'epoch': 0.3}
{'loss': 0.8507, 'learning_rate': 0.00016528889204492448, 'epoch': 0.3}
{'loss': 0.8383, 'learning_rate': 0.00016524167423385413, 'epoch': 0.3}
{'loss': 0.8899, 'learning_rate': 0.00016519443108366927, 'epoch': 0.3}
{'loss': 0.8787, 'learning_rate': 0.00016514716261271866, 'epoch': 0.3}
{'loss': 0.7924, 'learning_rate': 0.00016509986883936074, 'epoch': 0.3}
{'loss': 0.3544, 'learning_rate': 0.00016505254978196388, 'epoch': 0.3}
{'loss': 0.8533, 'learning_rate': 0.00016500520545890634, 'epoch': 0.3}
{'loss': 0.8591, 'learning_rate': 0.00016495783588857605, 'epoch': 0.3}
{'loss': 0.3079, 'learning_rate': 0.0001649104410893708, 'epoch': 0.3}
{'loss': 0.7863, 'learning_rate': 0.0001648630210796982, 'epoch': 0.3}
{'loss': 0.9043, 'learning_rate': 0.0001648155758779756, 'epoch': 0.3}
{'loss': 0.8503, 'learning_rate': 0.0001647681055026302, 'epoch': 0.3}
{'loss': 0.8394, 'learning_rate': 0.000164720609972099, 'epoch': 0.3}
{'loss': 0.8525, 'learning_rate': 0.00016467308930482864, 'epoch': 0.3}
{'loss': 0.8594, 'learning_rate': 0.00016462554351927557, 'epoch': 0.3}
{'loss': 0.8075, 'learning_rate': 0.00016457797263390612, 'epoch': 0.3}
{'loss': 0.8982, 'learning_rate': 0.00016453037666719624, 'epoch': 0.3}
{'loss': 0.8635, 'learning_rate': 0.00016448275563763162, 'epoch': 0.3}
{'loss': 0.7979, 'learning_rate': 0.0001644351095637078, 'epoch': 0.3}
{'loss': 0.9021, 'learning_rate': 0.00016438743846392985, 'epoch': 0.3}
{'loss': 0.8618, 'learning_rate': 0.00016433974235681274, 'epoch': 0.3}
{'loss': 0.8069, 'learning_rate': 0.0001642920212608811, 'epoch': 0.3}
{'loss': 0.8323, 'learning_rate': 0.00016424427519466924, 'epoch': 0.3}
{'loss': 0.8684, 'learning_rate': 0.00016419650417672118, 'epoch': 0.3}
{'loss': 0.8127, 'learning_rate': 0.00016414870822559064, 'epoch': 0.3}
{'loss': 0.8354, 'learning_rate': 0.00016410088735984102, 'epoch': 0.3}
{'loss': 0.8513, 'learning_rate': 0.00016405304159804534, 'epoch': 0.3}
{'loss': 0.8643, 'learning_rate': 0.00016400517095878643, 'epoch': 0.3}
{'loss': 0.8499, 'learning_rate': 0.00016395727546065665, 'epoch': 0.3}
{'loss': 0.8799, 'learning_rate': 0.00016390935512225805, 'epoch': 0.3}
{'loss': 0.8496, 'learning_rate': 0.00016386140996220232, 'epoch': 0.3}
{'loss': 0.8713, 'learning_rate': 0.00016381343999911086, 'epoch': 0.3}
{'loss': 0.8796, 'learning_rate': 0.00016376544525161465, 'epoch': 0.3}
{'loss': 0.8567, 'learning_rate': 0.00016371742573835426, 'epoch': 0.3}
{'loss': 0.8094, 'learning_rate': 0.0001636693814779799, 'epoch': 0.3}
{'loss': 0.8896, 'learning_rate': 0.00016362131248915144, 'epoch': 0.3}
{'loss': 0.8306, 'learning_rate': 0.00016357321879053834, 'epoch': 0.3}
{'loss': 0.8547, 'learning_rate': 0.00016352510040081963, 'epoch': 0.3}
{'loss': 0.8223, 'learning_rate': 0.00016347695733868388, 'epoch': 0.3}
{'loss': 0.8335, 'learning_rate': 0.00016342878962282938, 'epoch': 0.3}
{'loss': 0.9014, 'learning_rate': 0.00016338059727196387, 'epoch': 0.3}
{'loss': 0.8975, 'learning_rate': 0.0001633323803048047, 'epoch': 0.3}
{'loss': 0.8022, 'learning_rate': 0.0001632841387400788, 'epoch': 0.3}
{'loss': 0.7964, 'learning_rate': 0.00016323587259652267, 'epoch': 0.3}
{'loss': 0.8042, 'learning_rate': 0.00016318758189288226, 'epoch': 0.3}
{'loss': 0.8167, 'learning_rate': 0.00016313926664791314, 'epoch': 0.3}
{'loss': 0.8474, 'learning_rate': 0.00016309092688038046, 'epoch': 0.3}
{'loss': 0.8945, 'learning_rate': 0.00016304256260905872, 'epoch': 0.3}
{'loss': 0.8457, 'learning_rate': 0.00016299417385273214, 'epoch': 0.3}
{'loss': 0.3471, 'learning_rate': 0.00016294576063019428, 'epoch': 0.3}
{'loss': 0.8472, 'learning_rate': 0.00016289732296024834, 'epoch': 0.3}
{'loss': 0.8821, 'learning_rate': 0.00016284886086170698, 'epoch': 0.31}
{'loss': 0.3506, 'learning_rate': 0.00016280037435339217, 'epoch': 0.31}
{'loss': 0.8516, 'learning_rate': 0.00016275186345413568, 'epoch': 0.31}
{'loss': 0.3267, 'learning_rate': 0.00016270332818277847, 'epoch': 0.31}
{'loss': 0.8169, 'learning_rate': 0.00016265476855817116, 'epoch': 0.31}
{'loss': 0.8127, 'learning_rate': 0.00016260618459917368, 'epoch': 0.31}
{'loss': 0.8582, 'learning_rate': 0.00016255757632465553, 'epoch': 0.31}
{'loss': 0.8633, 'learning_rate': 0.00016250894375349558, 'epoch': 0.31}
{'loss': 0.8813, 'learning_rate': 0.00016246028690458216, 'epoch': 0.31}
{'loss': 0.8, 'learning_rate': 0.00016241160579681308, 'epoch': 0.31}
{'loss': 0.8606, 'learning_rate': 0.0001623629004490954, 'epoch': 0.31}
{'loss': 0.8955, 'learning_rate': 0.00016231417088034586, 'epoch': 0.31}
{'loss': 0.3175, 'learning_rate': 0.0001622654171094904, 'epoch': 0.31}
{'loss': 0.8174, 'learning_rate': 0.00016221663915546436, 'epoch': 0.31}
{'loss': 0.8528, 'learning_rate': 0.00016216783703721266, 'epoch': 0.31}
{'loss': 0.8716, 'learning_rate': 0.00016211901077368935, 'epoch': 0.31}
{'loss': 0.8247, 'learning_rate': 0.0001620701603838581, 'epoch': 0.31}
{'loss': 0.8533, 'learning_rate': 0.00016202128588669177, 'epoch': 0.31}
{'loss': 0.8479, 'learning_rate': 0.00016197238730117271, 'epoch': 0.31}
{'loss': 0.8484, 'learning_rate': 0.00016192346464629246, 'epoch': 0.31}
{'loss': 0.8206, 'learning_rate': 0.00016187451794105214, 'epoch': 0.31}
{'loss': 0.8699, 'learning_rate': 0.00016182554720446202, 'epoch': 0.31}
{'loss': 0.7952, 'learning_rate': 0.00016177655245554177, 'epoch': 0.31}
{'loss': 0.8655, 'learning_rate': 0.00016172753371332038, 'epoch': 0.31}
{'loss': 0.832, 'learning_rate': 0.00016167849099683624, 'epoch': 0.31}
{'loss': 0.8684, 'learning_rate': 0.00016162942432513687, 'epoch': 0.31}
{'loss': 0.8538, 'learning_rate': 0.00016158033371727924, 'epoch': 0.31}
{'loss': 0.8337, 'learning_rate': 0.00016153121919232962, 'epoch': 0.31}
{'loss': 0.8201, 'learning_rate': 0.00016148208076936348, 'epoch': 0.31}
{'loss': 0.8679, 'learning_rate': 0.0001614329184674656, 'epoch': 0.31}
{'loss': 0.8657, 'learning_rate': 0.00016138373230573013, 'epoch': 0.31}
{'loss': 0.804, 'learning_rate': 0.00016133452230326033, 'epoch': 0.31}
{'loss': 0.8872, 'learning_rate': 0.00016128528847916883, 'epoch': 0.31}
{'loss': 0.874, 'learning_rate': 0.00016123603085257745, 'epoch': 0.31}
{'loss': 0.8264, 'learning_rate': 0.00016118674944261732, 'epoch': 0.31}
{'loss': 0.77, 'learning_rate': 0.0001611374442684288, 'epoch': 0.31}
{'loss': 0.8333, 'learning_rate': 0.00016108811534916136, 'epoch': 0.31}
{'loss': 0.8984, 'learning_rate': 0.00016103876270397386, 'epoch': 0.31}
{'loss': 0.8367, 'learning_rate': 0.0001609893863520343, 'epoch': 0.31}
{'loss': 0.8215, 'learning_rate': 0.0001609399863125198, 'epoch': 0.31}
{'loss': 0.8665, 'learning_rate': 0.00016089056260461688, 'epoch': 0.31}
{'loss': 0.876, 'learning_rate': 0.00016084111524752105, 'epoch': 0.31}
{'loss': 0.8716, 'learning_rate': 0.00016079164426043718, 'epoch': 0.31}
{'loss': 0.7881, 'learning_rate': 0.0001607421496625791, 'epoch': 0.31}
{'loss': 0.8723, 'learning_rate': 0.00016069263147317013, 'epoch': 0.31}
{'loss': 0.7852, 'learning_rate': 0.00016064308971144238, 'epoch': 0.31}
{'loss': 0.8401, 'learning_rate': 0.00016059352439663739, 'epoch': 0.31}
{'loss': 0.832, 'learning_rate': 0.00016054393554800576, 'epoch': 0.31}
{'loss': 0.8279, 'learning_rate': 0.0001604943231848072, 'epoch': 0.31}
{'loss': 0.8784, 'learning_rate': 0.00016044468732631057, 'epoch': 0.31}
{'loss': 0.8434, 'learning_rate': 0.00016039502799179394, 'epoch': 0.31}
{'loss': 0.8279, 'learning_rate': 0.00016034534520054433, 'epoch': 0.31}
{'loss': 0.7654, 'learning_rate': 0.000160295638971858, 'epoch': 0.32}
{'loss': 0.8865, 'learning_rate': 0.0001602459093250403, 'epoch': 0.32}
{'loss': 0.8464, 'learning_rate': 0.0001601961562794056, 'epoch': 0.32}
{'loss': 0.3138, 'learning_rate': 0.0001601463798542775, 'epoch': 0.32}
{'loss': 0.8628, 'learning_rate': 0.00016009658006898848, 'epoch': 0.32}
{'loss': 0.8418, 'learning_rate': 0.00016004675694288026, 'epoch': 0.32}
{'loss': 0.8801, 'learning_rate': 0.0001599969104953036, 'epoch': 0.32}
{'loss': 0.8918, 'learning_rate': 0.0001599470407456182, 'epoch': 0.32}
{'loss': 0.886, 'learning_rate': 0.00015989714771319299, 'epoch': 0.32}
{'loss': 0.8656, 'learning_rate': 0.00015984723141740576, 'epoch': 0.32}
{'loss': 0.8064, 'learning_rate': 0.0001597972918776435, 'epoch': 0.32}
{'loss': 0.8647, 'learning_rate': 0.00015974732911330208, 'epoch': 0.32}
{'loss': 0.7935, 'learning_rate': 0.00015969734314378654, 'epoch': 0.32}
{'loss': 0.8159, 'learning_rate': 0.00015964733398851077, 'epoch': 0.32}
{'loss': 0.8518, 'learning_rate': 0.00015959730166689783, 'epoch': 0.32}
{'loss': 0.8279, 'learning_rate': 0.00015954724619837967, 'epoch': 0.32}
{'loss': 0.8176, 'learning_rate': 0.00015949716760239722, 'epoch': 0.32}
{'loss': 0.8325, 'learning_rate': 0.00015944706589840046, 'epoch': 0.32}
{'loss': 0.8066, 'learning_rate': 0.00015939694110584832, 'epoch': 0.32}
{'loss': 0.9028, 'learning_rate': 0.00015934679324420872, 'epoch': 0.32}
{'loss': 0.8567, 'learning_rate': 0.00015929662233295843, 'epoch': 0.32}
{'loss': 0.864, 'learning_rate': 0.00015924642839158332, 'epoch': 0.32}
{'loss': 0.8621, 'learning_rate': 0.0001591962114395781, 'epoch': 0.32}
{'loss': 0.808, 'learning_rate': 0.00015914597149644652, 'epoch': 0.32}
{'loss': 0.8433, 'learning_rate': 0.00015909570858170112, 'epoch': 0.32}
{'loss': 0.886, 'learning_rate': 0.00015904542271486346, 'epoch': 0.32}
{'loss': 0.8235, 'learning_rate': 0.00015899511391546402, 'epoch': 0.32}
{'loss': 0.8098, 'learning_rate': 0.00015894478220304214, 'epoch': 0.32}
{'loss': 0.3356, 'learning_rate': 0.00015889442759714603, 'epoch': 0.32}
{'loss': 0.812, 'learning_rate': 0.00015884405011733292, 'epoch': 0.32}
{'loss': 0.876, 'learning_rate': 0.0001587936497831688, 'epoch': 0.32}
{'loss': 0.8138, 'learning_rate': 0.00015874322661422856, 'epoch': 0.32}
{'loss': 0.8918, 'learning_rate': 0.000158692780630096, 'epoch': 0.32}
{'loss': 0.8765, 'learning_rate': 0.0001586423118503638, 'epoch': 0.32}
{'loss': 0.9053, 'learning_rate': 0.0001585918202946334, 'epoch': 0.32}
{'loss': 0.8618, 'learning_rate': 0.00015854130598251512, 'epoch': 0.32}
{'loss': 0.8625, 'learning_rate': 0.0001584907689336282, 'epoch': 0.32}
{'loss': 0.8745, 'learning_rate': 0.0001584402091676006, 'epoch': 0.32}
{'loss': 0.8518, 'learning_rate': 0.00015838962670406916, 'epoch': 0.32}
{'loss': 0.8513, 'learning_rate': 0.00015833902156267956, 'epoch': 0.32}
{'loss': 0.8611, 'learning_rate': 0.00015828839376308618, 'epoch': 0.32}
{'loss': 0.8335, 'learning_rate': 0.00015823774332495235, 'epoch': 0.32}
{'loss': 0.8302, 'learning_rate': 0.0001581870702679501, 'epoch': 0.32}
{'loss': 0.885, 'learning_rate': 0.0001581363746117602, 'epoch': 0.32}
{'loss': 0.3513, 'learning_rate': 0.00015808565637607237, 'epoch': 0.32}
{'loss': 0.8372, 'learning_rate': 0.00015803491558058488, 'epoch': 0.32}
{'loss': 0.8236, 'learning_rate': 0.00015798415224500492, 'epoch': 0.32}
{'loss': 0.8936, 'learning_rate': 0.00015793336638904838, 'epoch': 0.32}
{'loss': 0.8469, 'learning_rate': 0.0001578825580324399, 'epoch': 0.32}
{'loss': 0.8096, 'learning_rate': 0.0001578317271949129, 'epoch': 0.32}
{'loss': 0.8209, 'learning_rate': 0.00015778087389620938, 'epoch': 0.32}
{'loss': 0.3328, 'learning_rate': 0.00015772999815608028, 'epoch': 0.32}
{'loss': 0.7959, 'learning_rate': 0.00015767909999428513, 'epoch': 0.33}
{'loss': 0.8723, 'learning_rate': 0.00015762817943059217, 'epoch': 0.33}
{'loss': 0.8335, 'learning_rate': 0.00015757723648477837, 'epoch': 0.33}
{'loss': 0.8257, 'learning_rate': 0.0001575262711766294, 'epoch': 0.33}
{'loss': 0.8298, 'learning_rate': 0.00015747528352593956, 'epoch': 0.33}
{'loss': 0.8126, 'learning_rate': 0.0001574242735525119, 'epoch': 0.33}
{'loss': 0.8262, 'learning_rate': 0.00015737324127615806, 'epoch': 0.33}
{'loss': 0.8652, 'learning_rate': 0.00015732218671669844, 'epoch': 0.33}
{'loss': 0.8655, 'learning_rate': 0.00015727110989396202, 'epoch': 0.33}
{'loss': 0.8716, 'learning_rate': 0.00015722001082778646, 'epoch': 0.33}
{'loss': 0.8091, 'learning_rate': 0.00015716888953801804, 'epoch': 0.33}
{'loss': 0.8413, 'learning_rate': 0.00015711774604451167, 'epoch': 0.33}
{'loss': 0.31, 'learning_rate': 0.0001570665803671309, 'epoch': 0.33}
{'loss': 0.3319, 'learning_rate': 0.00015701539252574792, 'epoch': 0.33}
{'loss': 0.8616, 'learning_rate': 0.00015696418254024344, 'epoch': 0.33}
{'loss': 0.8086, 'learning_rate': 0.00015691295043050688, 'epoch': 0.33}
{'loss': 0.9165, 'learning_rate': 0.0001568616962164362, 'epoch': 0.33}
{'loss': 0.8135, 'learning_rate': 0.0001568104199179379, 'epoch': 0.33}
{'loss': 0.8291, 'learning_rate': 0.00015675912155492712, 'epoch': 0.33}
{'loss': 0.9016, 'learning_rate': 0.00015670780114732756, 'epoch': 0.33}
{'loss': 0.8425, 'learning_rate': 0.00015665645871507151, 'epoch': 0.33}
{'loss': 0.8267, 'learning_rate': 0.00015660509427809974, 'epoch': 0.33}
{'loss': 0.8552, 'learning_rate': 0.0001565537078563616, 'epoch': 0.33}
{'loss': 0.8032, 'learning_rate': 0.000156502299469815, 'epoch': 0.33}
{'loss': 0.8193, 'learning_rate': 0.00015645086913842636, 'epoch': 0.33}
{'loss': 0.8521, 'learning_rate': 0.00015639941688217065, 'epoch': 0.33}
{'loss': 0.7858, 'learning_rate': 0.00015634794272103127, 'epoch': 0.33}
{'loss': 0.8586, 'learning_rate': 0.0001562964466750003, 'epoch': 0.33}
{'loss': 0.9045, 'learning_rate': 0.0001562449287640781, 'epoch': 0.33}
{'loss': 0.8501, 'learning_rate': 0.00015619338900827367, 'epoch': 0.33}
{'loss': 0.8669, 'learning_rate': 0.00015614182742760448, 'epoch': 0.33}
{'loss': 0.884, 'learning_rate': 0.00015609024404209643, 'epoch': 0.33}
{'loss': 0.7767, 'learning_rate': 0.00015603863887178393, 'epoch': 0.33}
{'loss': 0.8481, 'learning_rate': 0.00015598701193670982, 'epoch': 0.33}
{'loss': 0.8564, 'learning_rate': 0.0001559353632569254, 'epoch': 0.33}
{'loss': 0.8469, 'learning_rate': 0.00015588369285249047, 'epoch': 0.33}
{'loss': 0.7516, 'learning_rate': 0.00015583200074347316, 'epoch': 0.33}
{'loss': 0.7729, 'learning_rate': 0.0001557802869499501, 'epoch': 0.33}
{'loss': 0.8271, 'learning_rate': 0.00015572855149200638, 'epoch': 0.33}
{'loss': 0.8645, 'learning_rate': 0.0001556767943897354, 'epoch': 0.33}
{'loss': 0.8916, 'learning_rate': 0.00015562501566323907, 'epoch': 0.33}
{'loss': 0.8904, 'learning_rate': 0.0001555732153326276, 'epoch': 0.33}
{'loss': 0.7664, 'learning_rate': 0.00015552139341801967, 'epoch': 0.33}
{'loss': 0.7937, 'learning_rate': 0.00015546954993954227, 'epoch': 0.33}
{'loss': 0.811, 'learning_rate': 0.00015541768491733092, 'epoch': 0.33}
{'loss': 0.8809, 'learning_rate': 0.00015536579837152926, 'epoch': 0.33}
{'loss': 0.8604, 'learning_rate': 0.00015531389032228955, 'epoch': 0.33}
{'loss': 0.8591, 'learning_rate': 0.00015526196078977217, 'epoch': 0.33}
{'loss': 0.8396, 'learning_rate': 0.00015521000979414602, 'epoch': 0.33}
{'loss': 0.8513, 'learning_rate': 0.00015515803735558826, 'epoch': 0.33}
{'loss': 0.8818, 'learning_rate': 0.00015510604349428436, 'epoch': 0.33}
{'loss': 0.8655, 'learning_rate': 0.00015505402823042818, 'epoch': 0.33}
{'loss': 0.803, 'learning_rate': 0.00015500199158422178, 'epoch': 0.34}
{'loss': 0.8552, 'learning_rate': 0.0001549499335758757, 'epoch': 0.34}
{'loss': 0.7642, 'learning_rate': 0.00015489785422560857, 'epoch': 0.34}
{'loss': 0.8416, 'learning_rate': 0.00015484575355364743, 'epoch': 0.34}
{'loss': 0.7739, 'learning_rate': 0.00015479363158022764, 'epoch': 0.34}
{'loss': 0.793, 'learning_rate': 0.0001547414883255927, 'epoch': 0.34}
{'loss': 0.8892, 'learning_rate': 0.0001546893238099945, 'epoch': 0.34}
{'loss': 0.8022, 'learning_rate': 0.0001546371380536931, 'epoch': 0.34}
{'loss': 0.8296, 'learning_rate': 0.00015458493107695686, 'epoch': 0.34}
{'loss': 0.8474, 'learning_rate': 0.00015453270290006238, 'epoch': 0.34}
{'loss': 0.8538, 'learning_rate': 0.00015448045354329447, 'epoch': 0.34}
{'loss': 0.8258, 'learning_rate': 0.00015442818302694618, 'epoch': 0.34}
{'loss': 0.3411, 'learning_rate': 0.0001543758913713188, 'epoch': 0.34}
{'loss': 0.3308, 'learning_rate': 0.00015432357859672177, 'epoch': 0.34}
{'loss': 0.8472, 'learning_rate': 0.00015427124472347278, 'epoch': 0.34}
{'loss': 0.8628, 'learning_rate': 0.0001542188897718977, 'epoch': 0.34}
{'loss': 0.8269, 'learning_rate': 0.00015416651376233063, 'epoch': 0.34}
{'loss': 0.3129, 'learning_rate': 0.00015411411671511377, 'epoch': 0.34}
{'loss': 0.8198, 'learning_rate': 0.00015406169865059749, 'epoch': 0.34}
{'loss': 0.797, 'learning_rate': 0.00015400925958914042, 'epoch': 0.34}
{'loss': 0.8098, 'learning_rate': 0.00015395679955110925, 'epoch': 0.34}
{'loss': 0.8511, 'learning_rate': 0.00015390431855687896, 'epoch': 0.34}
{'loss': 0.8164, 'learning_rate': 0.00015385181662683244, 'epoch': 0.34}
{'loss': 0.8149, 'learning_rate': 0.00015379929378136087, 'epoch': 0.34}
{'loss': 0.8101, 'learning_rate': 0.00015374675004086355, 'epoch': 0.34}
{'loss': 0.7805, 'learning_rate': 0.00015369418542574782, 'epoch': 0.34}
{'loss': 0.8882, 'learning_rate': 0.00015364159995642917, 'epoch': 0.34}
{'loss': 0.8723, 'learning_rate': 0.00015358899365333124, 'epoch': 0.34}
{'loss': 0.8569, 'learning_rate': 0.00015353636653688563, 'epoch': 0.34}
{'loss': 0.8955, 'learning_rate': 0.0001534837186275322, 'epoch': 0.34}
{'loss': 0.8232, 'learning_rate': 0.00015343104994571875, 'epoch': 0.34}
{'loss': 0.8438, 'learning_rate': 0.0001533783605119012, 'epoch': 0.34}
{'loss': 0.8545, 'learning_rate': 0.00015332565034654344, 'epoch': 0.34}
{'loss': 0.8318, 'learning_rate': 0.00015327291947011762, 'epoch': 0.34}
{'loss': 0.7644, 'learning_rate': 0.00015322016790310372, 'epoch': 0.34}
{'loss': 0.8007, 'learning_rate': 0.00015316739566598986, 'epoch': 0.34}
{'loss': 0.7903, 'learning_rate': 0.00015311460277927217, 'epoch': 0.34}
{'loss': 0.8707, 'learning_rate': 0.0001530617892634548, 'epoch': 0.34}
{'loss': 0.8228, 'learning_rate': 0.0001530089551390499, 'epoch': 0.34}
{'loss': 0.3294, 'learning_rate': 0.0001529561004265777, 'epoch': 0.34}
{'loss': 0.8135, 'learning_rate': 0.00015290322514656626, 'epoch': 0.34}
{'loss': 0.8212, 'learning_rate': 0.00015285032931955177, 'epoch': 0.34}
{'loss': 0.7839, 'learning_rate': 0.0001527974129660784, 'epoch': 0.34}
{'loss': 0.8396, 'learning_rate': 0.0001527444761066982, 'epoch': 0.34}
{'loss': 0.8586, 'learning_rate': 0.00015269151876197125, 'epoch': 0.34}
{'loss': 0.865, 'learning_rate': 0.00015263854095246557, 'epoch': 0.34}
{'loss': 0.8577, 'learning_rate': 0.00015258554269875717, 'epoch': 0.34}
{'loss': 0.7764, 'learning_rate': 0.00015253252402142988, 'epoch': 0.34}
{'loss': 0.8523, 'learning_rate': 0.00015247948494107565, 'epoch': 0.34}
{'loss': 0.8508, 'learning_rate': 0.00015242642547829417, 'epoch': 0.34}
{'loss': 0.7651, 'learning_rate': 0.0001523733456536931, 'epoch': 0.34}
{'loss': 0.8569, 'learning_rate': 0.00015232024548788813, 'epoch': 0.34}
{'loss': 0.8126, 'learning_rate': 0.00015226712500150268, 'epoch': 0.35}
{'loss': 0.917, 'learning_rate': 0.00015221398421516816, 'epoch': 0.35}
{'loss': 0.8542, 'learning_rate': 0.0001521608231495238, 'epoch': 0.35}
{'loss': 0.7793, 'learning_rate': 0.0001521076418252168, 'epoch': 0.35}
{'loss': 0.7373, 'learning_rate': 0.00015205444026290216, 'epoch': 0.35}
{'loss': 0.8297, 'learning_rate': 0.00015200121848324275, 'epoch': 0.35}
{'loss': 0.8813, 'learning_rate': 0.00015194797650690926, 'epoch': 0.35}
{'loss': 0.8496, 'learning_rate': 0.0001518947143545803, 'epoch': 0.35}
{'loss': 0.8308, 'learning_rate': 0.0001518414320469423, 'epoch': 0.35}
{'loss': 0.8525, 'learning_rate': 0.00015178812960468945, 'epoch': 0.35}
{'loss': 0.8748, 'learning_rate': 0.0001517348070485238, 'epoch': 0.35}
{'loss': 0.8232, 'learning_rate': 0.00015168146439915525, 'epoch': 0.35}
{'loss': 0.8315, 'learning_rate': 0.00015162810167730143, 'epoch': 0.35}
{'loss': 0.8154, 'learning_rate': 0.00015157471890368785, 'epoch': 0.35}
{'loss': 0.8174, 'learning_rate': 0.0001515213160990477, 'epoch': 0.35}
{'loss': 0.9077, 'learning_rate': 0.00015146789328412212, 'epoch': 0.35}
{'loss': 0.3585, 'learning_rate': 0.00015141445047965984, 'epoch': 0.35}
{'loss': 0.8342, 'learning_rate': 0.0001513609877064174, 'epoch': 0.35}
{'loss': 0.3678, 'learning_rate': 0.0001513075049851592, 'epoch': 0.35}
{'loss': 0.798, 'learning_rate': 0.0001512540023366573, 'epoch': 0.35}
{'loss': 0.8596, 'learning_rate': 0.00015120047978169144, 'epoch': 0.35}
{'loss': 0.8052, 'learning_rate': 0.00015114693734104927, 'epoch': 0.35}
{'loss': 0.8972, 'learning_rate': 0.00015109337503552595, 'epoch': 0.35}
{'loss': 0.7979, 'learning_rate': 0.00015103979288592453, 'epoch': 0.35}
{'loss': 0.771, 'learning_rate': 0.0001509861909130557, 'epoch': 0.35}
{'loss': 0.8608, 'learning_rate': 0.00015093256913773786, 'epoch': 0.35}
{'loss': 0.832, 'learning_rate': 0.000150878927580797, 'epoch': 0.35}
{'loss': 0.3401, 'learning_rate': 0.00015082526626306695, 'epoch': 0.35}
{'loss': 0.8036, 'learning_rate': 0.0001507715852053892, 'epoch': 0.35}
{'loss': 0.8389, 'learning_rate': 0.00015071788442861276, 'epoch': 0.35}
{'loss': 0.8701, 'learning_rate': 0.00015066416395359444, 'epoch': 0.35}
{'loss': 0.793, 'learning_rate': 0.00015061042380119864, 'epoch': 0.35}
{'loss': 0.8926, 'learning_rate': 0.0001505566639922974, 'epoch': 0.35}
{'loss': 0.3337, 'learning_rate': 0.00015050288454777046, 'epoch': 0.35}
{'loss': 0.8933, 'learning_rate': 0.0001504490854885051, 'epoch': 0.35}
{'loss': 0.897, 'learning_rate': 0.00015039526683539625, 'epoch': 0.35}
{'loss': 0.9109, 'learning_rate': 0.0001503414286093465, 'epoch': 0.35}
{'loss': 0.8921, 'learning_rate': 0.00015028757083126592, 'epoch': 0.35}
{'loss': 0.8242, 'learning_rate': 0.00015023369352207229, 'epoch': 0.35}
{'loss': 0.8376, 'learning_rate': 0.00015017979670269095, 'epoch': 0.35}
{'loss': 0.8074, 'learning_rate': 0.0001501258803940548, 'epoch': 0.35}
{'loss': 0.8552, 'learning_rate': 0.0001500719446171043, 'epoch': 0.35}
{'loss': 0.8503, 'learning_rate': 0.00015001798939278752, 'epoch': 0.35}
{'loss': 0.3303, 'learning_rate': 0.00014996401474205997, 'epoch': 0.35}
{'loss': 0.844, 'learning_rate': 0.00014991002068588484, 'epoch': 0.35}
{'loss': 0.8386, 'learning_rate': 0.0001498560072452328, 'epoch': 0.35}
{'loss': 0.7809, 'learning_rate': 0.00014980197444108205, 'epoch': 0.35}
{'loss': 0.8325, 'learning_rate': 0.00014974792229441826, 'epoch': 0.35}
{'loss': 0.843, 'learning_rate': 0.00014969385082623472, 'epoch': 0.35}
{'loss': 0.9034, 'learning_rate': 0.00014963976005753215, 'epoch': 0.35}
{'loss': 0.9128, 'learning_rate': 0.00014958565000931876, 'epoch': 0.35}
{'loss': 0.7826, 'learning_rate': 0.00014953152070261026, 'epoch': 0.35}
{'loss': 0.8413, 'learning_rate': 0.0001494773721584299, 'epoch': 0.36}
{'loss': 0.8733, 'learning_rate': 0.0001494232043978083, 'epoch': 0.36}
{'loss': 0.8867, 'learning_rate': 0.00014936901744178367, 'epoch': 0.36}
{'loss': 0.8733, 'learning_rate': 0.00014931481131140147, 'epoch': 0.36}
{'loss': 0.3278, 'learning_rate': 0.00014926058602771484, 'epoch': 0.36}
{'loss': 0.7864, 'learning_rate': 0.00014920634161178425, 'epoch': 0.36}
{'loss': 0.7721, 'learning_rate': 0.00014915207808467756, 'epoch': 0.36}
{'loss': 0.783, 'learning_rate': 0.0001490977954674701, 'epoch': 0.36}
{'loss': 0.8394, 'learning_rate': 0.00014904349378124467, 'epoch': 0.36}
{'loss': 0.8384, 'learning_rate': 0.0001489891730470914, 'epoch': 0.36}
{'loss': 0.8933, 'learning_rate': 0.00014893483328610777, 'epoch': 0.36}
{'loss': 0.8442, 'learning_rate': 0.0001488804745193988, 'epoch': 0.36}
{'loss': 0.8052, 'learning_rate': 0.00014882609676807675, 'epoch': 0.36}
{'loss': 0.8749, 'learning_rate': 0.00014877170005326136, 'epoch': 0.36}
{'loss': 0.8508, 'learning_rate': 0.00014871728439607966, 'epoch': 0.36}
{'loss': 0.8711, 'learning_rate': 0.00014866284981766606, 'epoch': 0.36}
{'loss': 0.8427, 'learning_rate': 0.00014860839633916236, 'epoch': 0.36}
{'loss': 0.8733, 'learning_rate': 0.0001485539239817176, 'epoch': 0.36}
{'loss': 0.7944, 'learning_rate': 0.0001484994327664883, 'epoch': 0.36}
{'loss': 0.8479, 'learning_rate': 0.0001484449227146381, 'epoch': 0.36}
{'loss': 0.8198, 'learning_rate': 0.0001483903938473382, 'epoch': 0.36}
{'loss': 0.8738, 'learning_rate': 0.00014833584618576695, 'epoch': 0.36}
{'loss': 0.8735, 'learning_rate': 0.00014828127975111, 'epoch': 0.36}
{'loss': 0.8726, 'learning_rate': 0.0001482266945645603, 'epoch': 0.36}
{'loss': 0.8362, 'learning_rate': 0.00014817209064731817, 'epoch': 0.36}
{'loss': 0.8525, 'learning_rate': 0.00014811746802059113, 'epoch': 0.36}
{'loss': 0.8855, 'learning_rate': 0.000148062826705594, 'epoch': 0.36}
{'loss': 0.8384, 'learning_rate': 0.00014800816672354877, 'epoch': 0.36}
{'loss': 0.8757, 'learning_rate': 0.00014795348809568476, 'epoch': 0.36}
{'loss': 0.8311, 'learning_rate': 0.00014789879084323857, 'epoch': 0.36}
{'loss': 0.8806, 'learning_rate': 0.00014784407498745394, 'epoch': 0.36}
